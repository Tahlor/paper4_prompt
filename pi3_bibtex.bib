@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLn_truncate_to_n_names = "1",
  CTLuse_forced_etal       = "yes",
}

@misc{1950CensusHome,
  title = {1950 {{Census}} - {{Home}} {\textbar} 1950 {{Census}}},
  urldate = {2023-08-23},
  howpublished = {https://1950census.archives.gov/}
}

@misc{240218039ResLoRA,
  title = {[2402.18039] {{ResLoRA}}: {{Identity Residual Mapping}} in {{Low-Rank Adaption}}},
  urldate = {2024-03-01},
  howpublished = {https://arxiv.org/abs/2402.18039}
}

@article{abdalImage2StyleGANHowEmbed2019,
  title = {{{Image2StyleGAN}}: {{How}} to {{Embed Images Into}} the {{StyleGAN Latent Space}}?},
  author = {Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  year = {2019},
  month = apr,
  eprint = {1904.03189},
  urldate = {2020-02-28},
  abstract = {We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHQ dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\UF2UXHUS\full-text.pdf}
}

@techreport{Abdelaziz2016,
  title = {A Large Vocabulary System for {{Arabic}} Online Handwriting Recognition},
  author = {Abdelaziz, Ibrahim and Abdou, Sherif and {Al-Barhamtoshy}, Hassanin},
  year = {2016},
  journal = {Pattern Analysis and Applications},
  volume = {19},
  number = {4},
  pages = {1129--1141},
  issn = {14337541},
  doi = {10.1007/s10044-015-0526-7},
  urldate = {2020-07-20},
  abstract = {The success of using Hidden Markov Models (HMMs) for speech recognition application has motivated the adoption of these models for handwriting recognition especially the online handwriting that has large similarity with the speech signal as a sequential process. Some languages such as Arabic, Farsi and Urdo include large number of delayed strokes that are written above or below most letters and usually written delayed in time. These delayed strokes represent a modeling challenge for the conventional left-right HMM that is commonly used for Automatic Speech Recognition (ASR) systems. In this paper, we introduce a new approach for handling delayed strokes in Arabic online handwriting recognition using HMMs. We also show that several modeling approaches such as context based tri-grapheme models, speaker adaptive training and discriminative training that are currently used in most state-of-the-art ASR systems can provide similar performance improvement for Hand Writing Recognition (HWR) systems. Finally, we show that using a multi-pass decoder that use the computationally less expensive models in the early passes can provide an Arabic large vocabulary HWR system with practical decoding time. We evaluated the performance of our proposed Arabic HWR system using two databases of small and large lexicons. For the small lexicon data set, our system achieved competing results compared to the best reported state-of-the-art Arabic HWR systems. For the large lexicon, our system achieved promising results (accuracy and time) for a vocabulary size of 64k words with the possibility of adapting the models for specific writers to get even better results.},
  keywords = {Adaptive training,Advanced modeling,Arabic,Hidden Markov models,Large vocabulary,Online handwriting recognition},
  file = {C:\Users\tarchibald\Zotero\storage\4T4ZI2CB\full-text.pdf}
}

@misc{abdellatifLMVRPALargeModel2024,
  title = {{{LMV-RPA}}: {{Large Model Voting-based Robotic Process Automation}}},
  shorttitle = {{{LMV-RPA}}},
  author = {Abdellatif, Osama and Ayman, Ahmed and Hamdi, Ali},
  year = {2024},
  month = dec,
  number = {arXiv:2412.17965},
  eprint = {2412.17965},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17965},
  urldate = {2025-03-14},
  abstract = {Automating high-volume unstructured data processing is essential for operational efficiency. Optical Character Recognition (OCR) is critical but often struggles with accuracy and efficiency in complex layouts and ambiguous text. These challenges are especially pronounced in large-scale tasks requiring both speed and precision. This paper introduces LMV-RPA, a Large Model Voting-based Robotic Process Automation system to enhance OCR workflows. LMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR, Easy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and Gemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs into structured JSON formats, improving accuracy, particularly in complex layouts. The multi-phase pipeline processes text extracted by OCR engines through LLMs, combining results to ensure the most accurate outputs. LMV-RPA achieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94 percent, while reducing processing time by 80 percent. Benchmark evaluations confirm its scalability and demonstrate that LMV-RPA offers a faster, more reliable, and efficient solution for automating large-scale document processing tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,Computer Science - Software Engineering},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\KNMI4JXE\\Abdellatif et al. - 2024 - LMV-RPA Large Model Voting-based Robotic Process .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\YSKIYAL4\\2412.html}
}

@inproceedings{afzalCuttingErrorHalf2017,
  title = {Cutting the {{Error}} by {{Half}}: {{Investigation}} of {{Very Deep CNN}} and {{Advanced Training Strategies}} for {{Document Image Classification}}},
  shorttitle = {Cutting the {{Error}} by {{Half}}},
  booktitle = {2017 14th {{IAPR International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Afzal, Muhammad Zeshan and K{\"o}lsch, Andreas and Ahmed, Sheraz and Liwicki, Marcus},
  year = {2017},
  month = nov,
  volume = {01},
  pages = {883--888},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2017.149},
  abstract = {We present an exhaustive investigation of recent Deep Learning architectures, algorithms, and strategies for the task of document image classification to finally reduce the error by more than half. Existing approaches, such as the DeepDoc-Classifier, apply standard Convolutional Network architectures with transfer learning from the object recognition domain. The contribution of the paper is threefold: First, it investigates recently introduced very deep neural network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from real images). Second, it proposes transfer learning from a huge set of document images, i.e. 400; 000 documents. Third, it analyzes the impact of the amount of training data (document images) and other parameters to the classification abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP dataset. We achieve an accuracy of 91:13\% for the Tobacco-3482 dataset while earlier approaches reach only 77:6\%. Thus, a relative error reduction of more than 60\% is achieved. For the large dataset RVL-CDIP, an accuracy of 90:97\% is achieved, corresponding to a relative error reduction of 11:5\%.},
  keywords = {Convolution,Convolutional Neural Network,Deep CNN,Document Image Classification,Feature extraction,Layout,Machine learning,Neural networks,Task analysis,Training,Transfer Learning},
  file = {C:\Users\tarchibald\Zotero\storage\QRBAFG6C\Afzal et al. - 2017 - Cutting the Error by Half Investigation of Very D.pdf}
}

@article{Aksan2018,
  title = {{{DeepWriting}}: {{Making Digital Ink Editable}} via {{Deep Generative Modeling}}},
  author = {Aksan, Emre and Pece, Fabrizio and Hilliges, Otmar},
  year = {2018},
  month = jan,
  eprint = {1801.08379},
  urldate = {2019-09-24},
  abstract = {Digital ink promises to combine the flexibility and aesthetics of handwriting and the ability to process, search and edit digital text. Character recognition converts handwritten text into a digital representation, albeit at the cost of losing personalized appearance due to the technical difficulties of separating the interwoven components of content and style. In this paper, we propose a novel generative neural network architecture that is capable of disentangling style from content and thus making digital ink editable. Our model can synthesize arbitrary text, while giving users control over the visual appearance (style). For example, allowing for style transfer without changing the content, editing of digital ink at the word level and other application scenarios such as spell-checking and correction of handwritten text. We furthermore contribute a new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\DRAWFMPP\full-text.pdf}
}

@article{al_doc_categorization,
  title = {Analyzing the Potential of Active Learning for Document Image Classification},
  author = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  year = {2023},
  month = apr,
  journal = {International Journal on Document Analysis and Recognition},
  pages = {1--23},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {14332825},
  doi = {10.1007/S10032-023-00429-8/TABLES/7},
  urldate = {2023-08-15},
  abstract = {Deep learning has been extensively researched in the field of document analysis and has shown excellent performance across a wide range of document-related tasks. As a result, a great deal of emphasis is now being placed on its practical deployment and integration into modern industrial document processing pipelines. It is well known, however, that deep learning models are data-hungry and often require huge volumes of annotated data in order to achieve competitive performances. And since data annotation is a costly and labor-intensive process, it remains one of the major hurdles to their practical deployment. This study investigates the possibility of using active learning to reduce the costs of data annotation in the context of document image classification, which is one of the core components of modern document processing pipelines. The results of this study demonstrate that by utilizing active learning (AL), deep document classification models can achieve competitive performances to the models trained on fully annotated datasets and, in some cases, even surpass them by annotating only 15--40\% of the total training dataset. Furthermore, this study demonstrates that modern AL strategies significantly outperform random querying, and in many cases achieve comparable performance to the models trained on fully annotated datasets even in the presence of practical deployment issues such as data imbalance, and annotation noise, and thus, offer tremendous benefits in real-world deployment of deep document classification models. The code to reproduce our experiments is publicly available at https://github.com/saifullah3396/doc\_al.},
  keywords = {Active learning,Deep active learning,Document analysis,Document image classification},
  file = {C:\Users\tarchibald\Zotero\storage\YBPQBRYG\full-text.pdf}
}

@techreport{Al-Rfou2019,
  title = {Character-{{Level Language Modeling}} with {{Deeper Self-Attention}}},
  author = {{Al-Rfou}, Rami and Choe, Dokook and Guo, Mandy and Jones, Llion},
  year = {2019},
  eprint = {1808.04444v2},
  urldate = {2019-09-18},
  abstract = {LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\Z5C5RVR3\full-text.pdf}
}

@article{alkendiAdvancementsChallengesHandwritten2024,
  title = {Advancements and {{Challenges}} in {{Handwritten Text Recognition}}: {{A Comprehensive Survey}}},
  shorttitle = {Advancements and {{Challenges}} in {{Handwritten Text Recognition}}},
  author = {AlKendi, Wissam and Gechter, Franck and Heyberger, Laurent and Guyeux, Christophe},
  year = {2024},
  month = jan,
  journal = {Journal of Imaging},
  volume = {10},
  number = {1},
  pages = {18},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2313-433X},
  doi = {10.3390/jimaging10010018},
  urldate = {2025-05-31},
  abstract = {Handwritten Text Recognition (HTR) is essential for digitizing historical documents in different kinds of archives. In this study, we introduce a hybrid form archive written in French: the Belfort civil registers of births. The digitization of these historical documents is challenging due to their unique characteristics such as writing style variations, overlapped characters and words, and marginal annotations. The objective of this survey paper is to summarize research on handwritten text documents and provide research directions toward effectively transcribing this French dataset. To achieve this goal, we presented a brief survey of several modern and historical HTR offline systems of different international languages, and the top state-of-the-art contributions reported of the French language specifically. The survey classifies the HTR systems based on techniques employed, datasets used, publication years, and the level of recognition. Furthermore, an analysis of the systems' accuracies is presented, highlighting the best-performing approach. We have also showcased the performance of some HTR commercial systems. In addition, this paper presents a summarization of the HTR datasets that publicly available, especially those identified as benchmark datasets in the International Conference on Document Analysis and Recognition (ICDAR) and the International Conference on Frontiers in Handwriting Recognition (ICFHR) competitions. This paper, therefore, presents updated state-of-the-art research in HTR and highlights new directions in the research field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Belfort civil registers of births,handwritten text recognition (HTR),HTR datasets,ICDAR,ICFHR,machine learning},
  file = {C:\Users\tarchibald\Zotero\storage\RQBH9LNG\AlKendi et al. - 2024 - Advancements and Challenges in Handwritten Text Re.pdf}
}

@article{alonso2019adversarial,
  title = {Adversarial {{Generation}} of {{Handwritten Text Images Conditioned}} on {{Sequences}}},
  author = {Alonso, Eloi A and Moysset, Bastien and Messina, Ronaldo},
  year = {2019},
  journal = {arXiv preprint arXiv:1903.00277},
  eprint = {1903.00277},
  urldate = {2019-10-02},
  abstract = {State-of-the-art offline handwriting text recognition systems tend to use neural networks and therefore require a large amount of annotated data to be trained. In order to partially satisfy this requirement, we propose a system based on Generative Adversarial Networks (GAN) to produce synthetic images of handwritten words. We use bidirectional LSTM recurrent layers to get an embedding of the word to be rendered, and we feed it to the generator network. We also modify the standard GAN by adding an auxiliary network for text recognition. The system is then trained with a balanced combination of an adversarial loss and a CTC loss. Together, these extensions to GAN enable to control the textual content of the generated word images. We obtain realistic images on both French and Arabic datasets, and we show that integrating these synthetic images into the existing training data of a text recognition system can slightly enhance its performance.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\BKG5UHE2\Alonso, Moysset - Unknown - Adversarial Generation of Handwritten Text Images Conditioned on Sequences.pdf}
}

@article{amrheinSupervisedOCRError2018,
  title = {Supervised {{OCR Error Detection}} and {{Correction Using Statistical}} and {{Neural Machine Translation Methods}}},
  author = {Amrhein, Chantal and Clematide, Simon},
  year = {2018},
  publisher = {Gesellschaft f{\"u}r Sprachtechnologie und Computerlinguistik (GSCL)},
  doi = {10.5167/UZH-162394},
  urldate = {2022-12-08},
  abstract = {For indexing the content of digitized historical texts, optical character recognition (OCR) errors are a hampering problem. To explore the effectivity of new strategies for OCR post-correction, this article focuses on methods of character-based machine translation, specifically neural machine translation and statistical machine translation. Using the ICDAR 2017 data set on OCR post-correction for English and French, we experiment with different strategies for error detection and error correction. We analyze how OCR post-correction with NMT can profit from using additional information and show that SMT and NMT can benefit from each other for these tasks. An ensemble of our models reached best performance in ICDAR's 2017 error correction subtask and performed competitively in error detection. However, our experimental results also suggest that tuning supervised learning for OCR post-correction of texts from different sources, text types (periodicals and monographs), time periods and languages is a difficult task: the data on which the MT systems are trained have a large influence on which methods and features work best. Conclusive and generally applicable insights are hard to achieve.},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\9G3MWCXN\Amrhein and Clematide - 2018 - Supervised OCR Error Detection and Correction Usin.pdf}
}

@techreport{AndriyBurkov,
  title = {The {{Hundred-Page Machine Learning}}},
  author = {Andriy Burkov, Book},
  file = {C:\Users\tarchibald\Zotero\storage\FKUN3XC9\Andriy Burkov - The Hundred-Page Machine Learning Book (2019, Andriy Burkov).pdf}
}

@inproceedings{antonacopoulosICDAR2009Page2009,
  title = {{{ICDAR}} 2009 {{Page Segmentation Competition}}},
  booktitle = {2009 10th {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Antonacopoulos, Apostolos and Pletschacher, Stefan and Bridson, David and Papadopoulos, Christos},
  year = {2009},
  month = jul,
  pages = {1370--1374},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2009.275},
  abstract = {This paper presents an objective comparative evaluation of layout analysis methods in realistic circumstances. It describes the Page Segmentation competition (modus operandi, dataset and evaluation methodology) held in the context of ICDAR2009 and presents the results of the evaluation of four submitted methods. Two state-of-the art methods are also compared as well as the three methods from the ICDA2007 Page Segmentation competition. The results indicate that although methods continue to mature, there is still a considerable need to develop robust methods that deal with everyday documents.},
  keywords = {Art,Image analysis,Image enhancement,Image recognition,Image segmentation,Pattern analysis,Pattern recognition,Pixel,Robustness,Text analysis},
  file = {C:\Users\tarchibald\Zotero\storage\FBLF2T8F\Antonacopoulos et al. - 2009 - ICDAR 2009 Page Segmentation Competition.pdf}
}

@article{Antoniou2017,
  title = {Data {{Augmentation Generative Adversarial Networks}}},
  author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  year = {2017},
  month = nov,
  eprint = {1711.04340},
  urldate = {2019-09-24},
  abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\C3949SRG\full-text.pdf}
}

@inproceedings{Antoniou2018,
  title = {Augmenting Image Classifiers Using Data Augmentation Generative Adversarial Networks},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
  year = {2018},
  volume = {11141 LNCS},
  pages = {594--603},
  publisher = {Springer Verlag},
  issn = {16113349},
  doi = {10.1007/978-3-030-01424-7_58},
  urldate = {2019-09-24},
  abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively, but standard data augmentation produces only limited plausible alternative data. Given the potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, uses data from a source domain and learns to take a data item and augment it by generating other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes. We demonstrate that a Data Augmentation Generative Adversarial Network (DA-GAN) augments classifiers well on Omniglot, EMNIST and VGG-Face.},
  isbn = {978-3-030-01423-0},
  file = {C:\Users\tarchibald\Zotero\storage\L9XUTIDB\full-text.pdf}
}

@misc{anvariSurveyDeepLearning2022,
  title = {A {{Survey}} on {{Deep}} Learning Based {{Document Image Enhancement}}},
  author = {Anvari, Zahra and Athitsos, Vassilis},
  year = {2022},
  month = jan,
  number = {arXiv:2112.02719},
  eprint = {2112.02719},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.02719},
  urldate = {2023-08-16},
  abstract = {Digitized documents such as scientific articles, tax forms, invoices, contract papers, historic texts are widely used nowadays. These document images could be degraded or damaged due to various reasons including poor lighting conditions, shadow, distortions like noise and blur, aging, ink stain, bleed-through, watermark, stamp, etc. Document image enhancement plays a crucial role as a pre-processing step in many automated document analysis and recognition tasks such as character recognition. With recent advances in deep learning, many methods are proposed to enhance the quality of these document images. In this paper, we review deep learning-based methods, datasets, and metrics for six main document image enhancement tasks, including binarization, debluring, denoising, defading, watermark removal, and shadow removal. We summarize the recent works for each task and discuss their features, challenges, and limitations. We introduce multiple document image enhancement tasks that have received little to no attention, including over and under exposure correction, super resolution, and bleed-through removal. We identify several promising research directions and opportunities for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\TEGB82ER\\Anvari and Athitsos - 2022 - A Survey on Deep learning based Document Image Enh.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\MQC5PHWJ\\2112.html}
}

@article{appalarajuDocFormerEndtoEndTransformer2021,
  title = {{{DocFormer}}: {{End-to-End Transformer}} for {{Document Understanding}}},
  author = {Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R.},
  year = {2021},
  month = jun,
  eprint = {2106.11539},
  pages = {973--983},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {15505499},
  doi = {10.48550/arxiv.2106.11539},
  urldate = {2022-06-04},
  abstract = {We present DocFormer -- a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).},
  archiveprefix = {arXiv},
  isbn = {9781665428125}
}

@techreport{Archibald2019,
  title = {Scan, {{Attend}}, {{Recover}}: {{A Deep Encoder-Decoder Network}} to {{Recover Handwritten Strokes}} from {{Images Handwritten Strokes}} from {{Images}}},
  author = {Archibald, Taylor},
  year = {2019},
  abstract = {Scan, Attend, Recover: A Deep Encoder-Decoder Network to Recover Stroke order and velocity are helpful features in the fields of signature verification, handwriting recognition, and handwriting synthesis. We use a deep neural network (DNN) encoder-decoder architecture with attention to infer temporal stroke information from static images of handwritten text. We believe this is the first DNN approach to stroke recovery that accommodates variable width inputs. We further propose several loss functions that are novel for this task. Finally, we demonstrate that our recovered temporal stroke information can improve both handwriting recognition and handwriting synthesis. Figure 1: Comparison of online and offline handwriting data.},
  file = {C:\Users\tarchibald\Zotero\storage\LKADWVVI\-1916376105.pdf}
}

@misc{archibaldDELINE8KSyntheticData2024,
  title = {{{DELINE8K}}: {{A Synthetic Data Pipeline}} for the {{Semantic Segmentation}} of {{Historical Documents}}},
  shorttitle = {{{DELINE8K}}},
  author = {Archibald, Taylor and Martinez, Tony},
  year = {2024},
  month = apr,
  number = {arXiv:2404.19259},
  eprint = {2404.19259},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.19259},
  urldate = {2024-05-09},
  abstract = {Document semantic segmentation is a promising avenue that can facilitate document analysis tasks, including optical character recognition (OCR), form classification, and document editing. Although several synthetic datasets have been developed to distinguish handwriting from printed text, they fall short in class variety and document diversity. We demonstrate the limitations of training on existing datasets when solving the National Archives Form Semantic Segmentation dataset (NAFSS), a dataset which we introduce. To address these limitations, we propose the most comprehensive document semantic segmentation synthesis pipeline to date, incorporating preprinted text, handwriting, and document backgrounds from over 10 sources to create the Document Element Layer INtegration Ensemble 8K, or DELINE8K dataset. Our customized dataset exhibits superior performance on the NAFSS benchmark, demonstrating it as a promising tool in further research. The DELINE8K dataset is available at https://github.com/Tahlor/deline8k.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\tarchibald\Zotero\storage\UPVVTRPH\Archibald and Martinez - 2024 - DELINE8K A Synthetic Data Pipeline for the Semant.html}
}

@inproceedings{ardizzoneAnalyzingInverseProblems2019,
  title = {Analyzing Inverse Problems with Invertible Neural Networks},
  booktitle = {7th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2019},
  author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and {Maier-Hein}, Lena and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2019},
  eprint = {1808.04730},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2020-07-23},
  abstract = {For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task - so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\KUSWRLPG\full-text.pdf}
}

@inproceedings{assranSelfSupervisedLearningImages2023,
  title = {Self-{{Supervised Learning From Images With}} a {{Joint-Embedding Predictive Architecture}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  year = {2023},
  pages = {15619--15629},
  urldate = {2024-05-22},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\CSWTRXYZ\Assran et al. - 2023 - Self-Supervised Learning From Images With a Joint-.pdf}
}

@article{baevskiData2vecGeneralFramework,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  urldate = {2022-10-07},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
  file = {C:\Users\tarchibald\Zotero\storage\IZUGEMLA\full-text.pdf}
}

@article{baevskiData2vecGeneralFramework2022,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  year = {2022},
  month = feb,
  eprint = {2202.03555},
  doi = {10.48550/arxiv.2202.03555},
  urldate = {2022-04-06},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\74BYNM52\full-text.pdf}
}

@misc{banerjeeSwinDocSegmenterEndtoEndUnified2023,
  title = {{{SwinDocSegmenter}}: {{An End-to-End Unified Domain Adaptive Transformer}} for {{Document Instance Segmentation}}},
  shorttitle = {{{SwinDocSegmenter}}},
  author = {Banerjee, Ayan and Biswas, Sanket and Llad{\'o}s, Josep and Pal, Umapada},
  year = {2023},
  month = may,
  number = {arXiv:2305.04609},
  eprint = {2305.04609},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04609},
  urldate = {2023-08-16},
  abstract = {Instance-level segmentation of documents consists in assigning a class-aware and instance-aware label to each pixel of the image. It is a key step in document parsing for their understanding. In this paper, we present a unified transformer encoder-decoder architecture for en-to-end instance segmentation of complex layouts in document images. The method adapts a contrastive training with a mixed query selection for anchor initialization in the decoder. Later on, it performs a dot product between the obtained query embeddings and the pixel embedding map (coming from the encoder) for semantic reasoning. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone achieves better segmentation performance than the existing state-of-the-art approaches with the average precision of {\textbackslash}textbf\{93.72\}, {\textbackslash}textbf\{54.39\}, {\textbackslash}textbf\{84.65\} and {\textbackslash}textbf\{98.04\} respectively under one billion parameters. The code is made publicly available at: {\textbackslash}href\{https://github.com/ayanban011/SwinDocSegmenter\}\{github.com/ayanban011/SwinDocSegmenter\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\QMWMSEYT\\Banerjee et al. - 2023 - SwinDocSegmenter An End-to-End Unified Domain Ada.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\QI5KZW5X\\2305.html}
}

@article{baoBEiTBERTPreTraining2021,
  title = {{{BEiT}}: {{BERT Pre-Training}} of {{Image Transformers}}},
  author = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  year = {2021},
  month = jun,
  eprint = {2106.08254},
  urldate = {2023-04-10},
  abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\UU5MQVKI\full-text.pdf}
}

@article{barrattNoteInceptionScore2018,
  title = {A {{Note}} on the {{Inception Score}}},
  author = {Barratt, Shane and Sharma, Rishi},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.01973 [cs, stat]},
  eprint = {1801.01973},
  primaryclass = {cs, stat},
  urldate = {2018-11-20},
  abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@techreport{Barron,
  title = {A {{General}} and {{Adaptive Robust Loss Function}}},
  author = {Barron, Jonathan T and Research, Google},
  urldate = {2019-10-01},
  abstract = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbon-nier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter , our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation , without requiring any manual parameter tuning. Many problems in statistics and optimization require ro-bustness-that a model be less influenced by outliers than by inliers [17, 19]. This idea is common in parameter estimation and learning tasks, where a robust loss (say, absolute error) may be preferred over a non-robust loss (say, squared error) due to its reduced sensitivity to large errors. Researchers have developed various robust penalties with particular properties, many of which are summarized well in [3, 39]. In gradient descent or M-estimation [16] these losses are often interchangeable, so researchers may experiment with different losses when designing a system. This flexibility in shaping a loss function may be useful because of non-Gaussian noise, or simply because the loss that is minimized during learning or parameter estimation is different from how the resulting learned model or estimated parameters will be evaluated. For example, one might train a neural network by minimizing the difference between the network's output and a set of images, but evaluate that network in terms of how well it hallucinates random images. In this paper we present a single loss function that is a superset of many common robust loss functions. A single continuous-valued parameter in our general loss function can be set such that it is equal to several traditional losses, and can be adjusted to model a wider family of functions. This allows us to generalize algorithms built around a fixed robust loss with a new "robustness" hyperparameter that can be tuned or annealed to improve performance. Though new hyperparameters may be valuable to a practitioner , they complicate experimentation by requiring manual tuning or time-consuming cross-validation. However, by viewing our general loss function as the negative log-likelihood of a probability distribution, and by treating the robustness of that distribution as a latent variable, we show that maximizing the likelihood of that distribution allows gradient-based optimization frameworks to automatically determine how robust the loss should be without any manual parameter tuning. This "adaptive" form of our loss is particularly effective in models with multivariate output spaces (say, image generation or depth estimation) as we can introduce independent robustness variables for each dimension in the output and thereby allow the model to independently adapt the robustness of its loss in each dimension. The rest of the paper is as follows: In Section 1 we define our general loss function, relate it to existing losses, and enumerate some of its useful properties. In Section 2 we use our loss to construct a probability distribution , which requires deriving a partition function and a sampling procedure. Section 3 discusses four representative experiments: In Sections 3.1 and 3.2 we take two Figure 1. Our general loss function (left) and its gradient (right) for different values of its shape parameter {$\alpha$}. Several values of {$\alpha$} reproduce existing loss functions: L2 loss ({$\alpha$} = 2), Charbonnier loss ({$\alpha$} = 1), Cauchy loss ({$\alpha$} = 0), Geman-McClure loss ({$\alpha$} = -2), and Welsch loss ({$\alpha$} = -{$\infty$}).},
  file = {C:\Users\tarchibald\Zotero\storage\JE25ELTD\full-text.pdf}
}

@inproceedings{barron2019general,
  title = {A General and Adaptive Robust Loss Function},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Barron, Jonathan T.},
  year = {2019},
  month = jan,
  eprint = {1701.03077},
  pages = {4331--4339},
  urldate = {2019-11-11},
  abstract = {We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\HWL42IVW\Barron - 2017 - A General and Adaptive Robust Loss Function.pdf}
}

@techreport{bartunovAssessingScalabilityBiologicallyMotivated,
  title = {Assessing the {{Scalability}} of {{Biologically-Motivated Deep Learning Algorithms}} and {{Architectures}}},
  author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A and Marris, Luke and Hinton, Geoffrey E and Brain, Google and Lillicrap, Timothy P},
  urldate = {2020-12-24},
  abstract = {The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets, explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and examine performance in both fully-and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.},
  file = {C:\Users\tarchibald\Zotero\storage\ZK66GXK3\full-text.pdf}
}

@article{bartzSynthesisStyleSemantic2021,
  title = {Synthesis in {{Style}}: {{Semantic Segmentation}} of {{Historical Documents}} Using {{Synthetic Data}}},
  author = {Bartz, Christian and Raetz, Hendrik and Otholt, Jona and Meinel, Christoph and Yang, Haojin},
  year = {2021},
  month = jul,
  journal = {Proceedings - International Conference on Pattern Recognition},
  volume = {2022-August},
  eprint = {2107.06777v3},
  pages = {3878--3884},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ICPR56361.2022.9956471},
  urldate = {2023-05-17},
  abstract = {One of the most pressing problems in the automated analysis of historical documents is the availability of annotated training data. The problem is that labeling samples is a time-consuming task because it requires human expertise and thus, cannot be automated well. In this work, we propose a novel method to construct synthetic labeled datasets for historical documents where no annotations are available. We train a StyleGAN model to synthesize document images that capture the core features of the original documents. While originally, the StyleGAN architecture was not intended to produce labels, it indirectly learns the underlying semantics to generate realistic images. Using our approach, we can extract the semantic information from the intermediate feature maps and use it to generate ground truth labels. To investigate if our synthetic dataset can be used to segment the text in historical documents, we use it to train multiple supervised segmentation models and evaluate their performance. We also train these models on another dataset created by a state-of-the-art synthesis approach to show that the models trained on our dataset achieve better results while requiring even less human annotation effort.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\YCHZNWS7\full-text.pdf}
}

@article{bearLearningPhysicalGraph2020,
  title = {Learning {{Physical Graph Representations}} from {{Visual Scenes}}},
  author = {Bear, Daniel M. and Fan, Chaofei and Mrowca, Damian and Li, Yunzhu and Alter, Seth and Nayebi, Aran and Schwartz, Jeremy and {Fei-Fei}, Li and Wu, Jiajun and Tenenbaum, Joshua B. and Yamins, Daniel L.K.},
  year = {2020},
  month = jun,
  journal = {Advances in Neural Information Processing Systems},
  volume = {2020-December},
  eprint = {2006.12373},
  publisher = {Neural information processing systems foundation},
  issn = {10495258},
  doi = {10.48550/arxiv.2006.12373},
  urldate = {2022-03-18},
  abstract = {Convolutional Neural Networks (CNNs) have proved exceptional at learning representations for visual object categorization. However, CNNs do not explicitly encode objects, parts, and their physical properties, which has limited CNNs' success on tasks that require structured understanding of visual scenes. To overcome these limitations, we introduce the idea of Physical Scene Graphs (PSGs), which represent scenes as hierarchical graphs, with nodes in the hierarchy corresponding intuitively to object parts at different scales, and edges to physical connections between parts. Bound to each node is a vector of latent attributes that intuitively represent object properties such as surface shape and texture. We also describe PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet augments standard CNNs by including: recurrent feedback connections to combine low and high-level image information; graph pooling and vectorization operations that convert spatially-uniform feature maps into object-centric graph structures; and perceptual grouping principles to encourage the identification of meaningful scene elements. We show that PSGNet outperforms alternative self-supervised scene representation algorithms at scene segmentation tasks, especially on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. We present a series of ablation studies illustrating the importance of each component of the PSGNet architecture, analyses showing that learned latent attributes capture intuitive scene properties, and illustrate the use of PSGs for compositional scene inference.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\M2VK7PS6\full-text.pdf}
}

@techreport{Bemdt1994,
  title = {Using {{Dynamic Time Warping}} to {{FindPatterns}} in {{Time Series}}},
  author = {Bemdt, Donald J and Clifford, James},
  year = {1994},
  urldate = {2019-10-17},
  abstract = {Knowledge discovery in databases presents many interesting challenges within the onte{\textasciitilde}t of providing computer tools for exploring large data archives. Electronic data .repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detect{\pounds}ug patterns in such data streams or time series is an important knowledge discovery task. This paper describes some pr{\textasciitilde}{\textbar}{\textasciitilde}m;{\textasciitilde},ry experiments with a dynamic prograrnm{\textasciitilde},{\textasciitilde}g approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field.},
  keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat-tern analysis,time series},
  file = {C:\Users\tarchibald\Zotero\storage\X872JXXG\full-text.pdf}
}

@article{bertolamiHiddenMarkovModelbased2008,
  title = {Hidden {{Markov}} Model-Based Ensemble Methods for Offline Handwritten Text Line Recognition},
  author = {Bertolami, Roman and Bunke, Horst},
  year = {2008},
  month = nov,
  journal = {Pattern Recognition},
  volume = {41},
  number = {11},
  pages = {3452--3460},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2008.04.003},
  urldate = {2025-06-01},
  abstract = {This paper investigates various ensemble methods for offline handwritten text line recognition. To obtain ensembles of recognisers, we implement bagging, random feature subspace, and language model variation methods. For the combination, the word sequences returned by the individual ensemble members are first aligned. Then a confidence-based voting strategy determines the final word sequence. A number of confidence measures based on normalised likelihoods and alternative candidates are evaluated. Experiments show that the proposed ensemble methods can improve the recognition accuracy over an optimised single reference recogniser.},
  keywords = {Confidence measures,Ensemble methods,Offline handwritten text line recognition},
  file = {C:\Users\tarchibald\Zotero\storage\S596SABV\S0031320308001349.html}
}

@inproceedings{beyerWhenNearestNeighbor1999,
  title = {When {{Is}} ``{{Nearest Neighbor}}'' {{Meaningful}}?},
  booktitle = {Database {{Theory}} --- {{ICDT}}'99},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  editor = {Beeri, Catriel and Buneman, Peter},
  year = {1999},
  pages = {217--235},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-49257-7_15},
  abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10--15 dimensions.},
  isbn = {978-3-540-49257-3},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\AGDKZ94I\Beyer et al. - 1999 - When Is Nearest Neighbor Meaningful.pdf}
}

@article{bezmaternykhUNetbinHackingDocument2019,
  title = {U-{{Net-bin}}: Hacking the Document Image Binarization Contest},
  shorttitle = {U-{{Net-bin}}},
  author = {Bezmaternykh, Pavel and Ilin, D.A. and Nikolaev, Dmitry},
  year = {2019},
  month = oct,
  journal = {Computer Optics},
  volume = {43},
  pages = {825--832},
  doi = {10.18287/2412-6179-2019-43-5-825-832},
  abstract = {Image binarization is still a challenging task in a variety of applications. In particular, Document Image Binarization Contest (DIBCO) is organized regularly to track the state-of-the-art techniques for the historical document binarization. In this work we present a binarization method that was ranked first in the DIBCO`17 contest. It is a convolutional neural network (CNN) based method which uses U-Net architecture, originally designed for biomedical image segmentation. We describe our approach to training data preparation and contest ground truth examination and provide multiple insights on its construction (so called hacking). It led to more accurate historical document binarization problem statement with respect to the challenges one could face in the open access datasets. A docker container with the final network along with all the supplementary data we used in the training process has been published on Github.},
  file = {C:\Users\tarchibald\Zotero\storage\65QMXHAE\Bezmaternykh et al. - 2019 - U-Net-bin hacking the document image binarization.pdf}
}

@inproceedings{Bhunia2019,
  title = {Improving {{Document Binarization Via Adversarial Noise-Texture Augmentation}}},
  author = {Bhunia, Ankan Kumar and Bhunia, Ayan Kumar and Sain, Aneeshan and Roy, Partha Pratim},
  year = {2019},
  month = aug,
  pages = {2721--2725},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/icip.2019.8803348},
  urldate = {2019-09-24},
  abstract = {Binarization of degraded document images is an elementary step in most of the problems in document image analysis domain. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. At last, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. Also, it is noteworthy that our model can learn from unpaired data. Experimental results suggest that the proposed method achieves superior performance over widely used DIBCO datasets.},
  file = {C:\Users\tarchibald\Zotero\storage\8PAQ63PG\full-text.pdf}
}

@article{bhunia2019indic,
  title = {Indic {{Handwritten Script Identification}} Using {{Offline-Online}} Multi-Modal {{Deep Network}}},
  author = {Bhunia, Ankan Kumar Ayan Kumar Ankan Kumar Ayan Kumar and Mukherjee, Subham and Sain, Aneeshan and Bhunia, Ankan Kumar Ayan Kumar Ankan Kumar Ayan Kumar and Roy, Partha Pratim and Pal, Umapada and Kumar Bhunia, Ayan Ankan and Mukherjee, Subham and Sain, Aneeshan and Kumar Bhunia, Ayan Ankan and Roy, Partha Pratim and Pal, Umapada},
  year = {2019},
  journal = {Information Fusion},
  eprint = {1802.08568v2},
  publisher = {Elsevier},
  urldate = {2019-10-08},
  abstract = {In this paper, we propose a novel approach of word-level Indic script identification using only character-level data in training stage. Our method uses a multi-modal deep network which takes both offline and online modality of the data as input in order to explore the information from both the modalities jointly for script identification task. We take handwritten data in either modality as input and the opposite modality is generated through intermodality conversion. Thereafter, we feed this offline-online modality pair to our network. Hence, along with the advantage of utilizing information from both the modalities, the proposed framework can work for both offline and online script identification which alleviates the need for designing two separate script identification modules for individual modality. We also propose a novel conditional multi-modal fusion scheme to combine the information from offline and online modality which takes into account the original modality of the data being fed to our network and thus it combines adaptively. An exhaustive experimental study has been done on a data set including English(Roman) and 6 other official Indic scripts. Our proposed scheme outperforms traditional classifiers along with handcrafted features and deep learning based methods. Experiment results show that using only character level training data can achieve competitive performance against traditional training using word level data.},
  archiveprefix = {arXiv},
  keywords = {Deep Neural,Handwritten Script Identification},
  file = {C:\Users\tarchibald\Zotero\storage\54DFXXTQ\Kumar Bhunia et al. - 2019 - Indic Handwritten Script Identification using Offline-Online multi-modal Deep Network.pdf}
}

@inproceedings{bhuniaHandwritingTransformers2021,
  title = {Handwriting {{Transformers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Bhunia, Ankan Kumar and Khan, Salman and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2021},
  pages = {1086--1094},
  urldate = {2024-02-19},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\MZEW6738\Bhunia et al. - 2021 - Handwriting Transformers.pdf}
}

@article{bhuniaImprovingDocumentBinarization2019,
  title = {Improving {{Document Binarization Via Adversarial Noise-Texture Augmentation}}},
  author = {Bhunia, Ayan Kumar and Bhunia, Ankan Kumar and Sain, Aneeshan and Roy, Partha Pratim},
  year = {2019},
  month = sep,
  journal = {Proceedings - International Conference on Image Processing, ICIP},
  volume = {2019-September},
  eprint = {1810.11120},
  pages = {2721--2725},
  publisher = {IEEE Computer Society},
  issn = {15224880},
  doi = {10.1109/ICIP.2019.8803348},
  urldate = {2023-08-15},
  abstract = {Binarization of degraded document images is an elementary step in most problems involving document image analysis. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. Finally, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. The most significant contribution of our framework is that it does not require any paired data unlike other Deep Learning-based methods [1], [2], [3]. Such a novel approach has never been implemented earlier thus making it the very first of its kind in Document Image Analysis community. Experimental results suggest that the proposed method1 achieves superior performance over widely used DIBCO datasets.},
  archiveprefix = {arXiv},
  isbn = {9781538662496},
  keywords = {Adversarial Learning,Augmentation,Document image binarization,Style transfer,Unpaired data}
}

@inproceedings{bhuniaImprovingDocumentBinarization2019a,
  title = {Improving {{Document Binarization Via Adversarial Noise-Texture Augmentation}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Bhunia, Ankan Kumar and Bhunia, Ayan Kumar and Sain, Aneeshan and Roy, Partha Pratim},
  year = {2019},
  month = sep,
  pages = {2721--2725},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2019.8803348},
  abstract = {Binarization of degraded document images is an elementary step in most problems involving document image analysis. The paper re-visits the binarization problem by introducing an adversarial learning approach. We construct a Texture Augmentation Network that transfers the texture element of a degraded reference document image to a clean binary image. In this way, the network creates multiple versions of the same textual content with various noisy textures, thus enlarging the available document binarization datasets. Finally, the newly generated images are passed through a Binarization network to get back the clean version. By jointly training the two networks we can increase the adversarial robustness of our system. The most significant contribution of our framework is that it does not require any paired data unlike other Deep Learning-based methods [1], [2], [3]. Such a novel approach has never been implemented earlier thus making it the very first of its kind in Document Image Analysis community. Experimental results suggest that the proposed method achieves superior performance over widely used DIBCO datasets.},
  keywords = {Adversarial Learning,Augmentation,Decoding,Degradation,Document image binarization,Integrated circuits,Noise measurement,Style transfer,Text analysis,Training,Training data,Unpaired data},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\QLDD65NM\\Bhunia et al. - 2019 - Improving Document Binarization Via Adversarial No.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\J94QMULS\\8803348.html}
}

@misc{biswasDocBinFormerTwoLevelTransformer2023,
  title = {{{DocBinFormer}}: {{A Two-Level Transformer Network}} for {{Effective Document Image Binarization}}},
  shorttitle = {{{DocBinFormer}}},
  author = {Biswas, Risab and Roy, Swalpa Kumar and Wang, Ning and Pal, Umapada and Huang, Guang-Bin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03568},
  eprint = {2312.03568},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.03568},
  urldate = {2023-12-09},
  abstract = {In real life, various degradation scenarios exist that might damage document images, making it harder to recognize and analyze them, thus binarization is a fundamental and crucial step for achieving the most optimal performance in any document analysis task. We propose DocBinFormer (Document Binarization Transformer), a novel two-level vision transformer (TL-ViT) architecture based on vision transformers for effective document image binarization. The presented architecture employs a two-level transformer encoder to effectively capture both global and local feature representation from the input images. These complimentary bi-level features are exploited for efficient document image binarization, resulting in improved results for system-generated as well as handwritten document images in a comprehensive approach. With the absence of convolutional layers, the transformer encoder uses the pixel patches and sub-patches along with their positional information to operate directly on them, while the decoder generates a clean (binarized) output image from the latent representation of the patches. Instead of using a simple vision transformer block to extract information from the image patches, the proposed architecture uses two transformer blocks for greater coverage of the extracted feature space on a global and local scale. The encoded feature representation is used by the decoder block to generate the corresponding binarized output. Extensive experiments on a variety of DIBCO and H-DIBCO benchmarks show that the proposed model outperforms state-of-the-art techniques on four metrics. The source code will be made available at https://github.com/RisabBiswas/DocBinFormer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\tarchibald\Zotero\storage\NVHQ9NUU\Biswas et al. - 2023 - DocBinFormer A Two-Level Transformer Network for .pdf}
}

@article{biswasRobustnessStructuredData2024,
  title = {Robustness of {{Structured Data Extraction}} from {{In-plane Rotated Documents}} Using {{Multi-Modal Large Language Models}} ({{LLM}})},
  author = {Biswas, Anjanava and Talukdar, Wrick},
  year = {2024},
  month = mar,
  urldate = {2025-07-07},
  abstract = {Multi-modal large language models (LLMs) have shown remarkable performance in various natural language processing tasks, including data extraction from documents. However, the accuracy of these models can be significantly affected by document in-plane rotation, also known as skew, a common issue in real-world scenarios for scanned documents. This study investigates the impact of document skew on the data extraction accuracy of three state-of-the-art multi-modal LLMs: Anthropic Claude V3 Sonnet, GPT-4-Turbo, and Llava:v1.6. We focus on extracting specific entities from synthetically generated sample documents with varying degrees of skewness. The results demonstrate that document skew adversely affects the data extraction accuracy of all the tested LLMs, with the severity of the impact varying across models. We identify the safe in-plane rotation angles (SIPRA) for each model and investigate the effects of skew on model hallucinations. Furthermore, we explore existing skew detection and correction mechanisms and discuss their potential limitations. We propose alternative approaches, including developing new multi-modal architectures that are inherently more robust to document skew and incorporating skewing techniques during the pre-training phase of the models. Additionally, we highlight the need for more comprehensive testing on a wider range of document quality and conditions to fully understand the challenges and opportunities associated with using multi-modal LLMs for information extraction in real-world scenarios.},
  langid = {american},
  keywords = {computer vision,data extraction,document skew,document understanding,multi-modal large language models,OCR,skew correction}
}

@misc{biswasRobustnessStructuredData2024preprint,
  title = {Robustness of {{Structured Data Extraction}} from {{In-plane Rotated Documents}} Using {{Multi-Modal Large Language Models}} ({{LLM}})},
  author = {Biswas, Anjanava and Talukdar, Wrick},
  year = {2024},
  month = jun,
  number = {arXiv:2406.10295},
  eprint = {2406.10295},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10295},
  urldate = {2025-05-31},
  abstract = {Multi-modal large language models (LLMs) have shown remarkable performance in various natural language processing tasks, including data extraction from documents. However, the accuracy of these models can be significantly affected by document in-plane rotation, also known as skew, a common issue in real-world scenarios for scanned documents. This study investigates the impact of document skew on the data extraction accuracy of three state-of-the-art multi-modal LLMs: Anthropic Claude V3 Sonnet, GPT-4-Turbo, and Llava:v1.6. We focus on extracting specific entities from synthetically generated sample documents with varying degrees of skewness. The results demonstrate that document skew adversely affects the data extraction accuracy of all the tested LLMs, with the severity of the impact varying across models. We identify the safe in-plane rotation angles (SIPRA) for each model and investigate the effects of skew on model hallucinations. Furthermore, we explore existing skew detection and correction mechanisms and discuss their potential limitations. We propose alternative approaches, including developing new multi-modal architectures that are inherently more robust to document skew and incorporating skewing techniques during the pre-training phase of the models. Additionally, we highlight the need for more comprehensive testing on a wider range of document quality and conditions to fully understand the challenges and opportunities associated with using multi-modal LLMs for information extraction in real-world scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {C:\Users\tarchibald\Zotero\storage\9MZNLZ53\Biswas and Talukdar - 2024 - Robustness of Structured Data Extraction from In-p.pdf}
}

@misc{blalockMultiplyingMatricesMultiplying2021,
  title = {Multiplying {{Matrices Without Multiplying}}},
  author = {Blalock, Davis and Guttag, John},
  year = {2021},
  month = jun,
  number = {arXiv:2106.10860},
  eprint = {2106.10860},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-03-01},
  abstract = {Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs \$100{\textbackslash}times\$ faster than exact matrix products and \$10{\textbackslash}times\$ faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling\$-\$the core operations of our method\$-\$could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Performance,Statistics - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\9Z4J9ADC\\Blalock and Guttag - 2021 - Multiplying Matrices Without Multiplying.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\TNMX5T2V\\Blalock and Guttag - 2021 - Multiplying Matrices Without Multiplying.html}
}

@misc{blecherNougatNeuralOptical2023,
  title = {Nougat: {{Neural Optical Understanding}} for {{Academic Documents}}},
  shorttitle = {Nougat},
  author = {Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  year = {2023},
  month = aug,
  number = {arXiv:2308.13418},
  eprint = {2308.13418},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.13418},
  urldate = {2024-04-17},
  abstract = {Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\P6W3WM27\\Blecher et al. - 2023 - Nougat Neural Optical Understanding for Academic .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\FMV3LY7E\\2308.html}
}

@techreport{Bluche,
  title = {Scan, {{Attend}} and {{Read}}: {{End-to-End Handwritten Paragraph Recognition}} with {{MDLSTM Attention}}},
  author = {Bluche, Th{\'e}odore and Louradour, J{\'e}r{\^o}me and Messina, Ronaldo},
  eprint = {1604.03286v3},
  urldate = {2019-09-17},
  abstract = {We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the implementation of covert and overt attention with a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was critical in previous approaches. To the best of our knowledge this is the first successful attempt of end-to-end multi-line handwriting recognition. We carried out experiments on the well-known IAM Database. The results are encouraging and bring hope to perform full paragraph transcription in the near future.},
  archiveprefix = {arXiv},
  isbn = {1604.03286v3},
  file = {C:\Users\tarchibald\Zotero\storage\35H6BJI2\full-text.pdf}
}

@inproceedings{Bluche2018,
  title = {Gated {{Convolutional Recurrent Neural Networks}} for {{Multilingual Handwriting Recognition}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Bluche, Theodore and Messina, Ronaldo},
  year = {2018},
  month = jan,
  volume = {1},
  pages = {646--651},
  publisher = {IEEE Computer Society},
  issn = {15205363},
  doi = {10.1109/ICDAR.2017.111},
  urldate = {2019-08-28},
  abstract = {{\copyright} 2017 IEEE. In this paper, we propose a new neural network architecture for state-of-The-art handwriting recognition, alternative to multi-dimensional long short-Term memory (MD-LSTM) recurrent neural networks. The model is based on a convolutional encoder of the input images, and a bidirectional LSTM decoder predicting character sequences. In this paradigm, we aim at producing generic, multilingual and reusable features with the convolutional encoder, leveraging more data for transfer learning. The architecture is also motivated by the need for a fast training on GPUs, and the requirement of a fast decoding on CPUs. The main contribution of this paper lies in the convolutional gates in the encoder, enabling hierarchical context-sensitive feature extraction. The experiments on a large benchmark including seven languages show a consistent and significant improvement of the proposed approach over our previous production systems. We also report state-of-The-art results on line and paragraph level recognition on the IAM and Rimes databases.},
  isbn = {978-1-5386-3586-5},
  file = {C:\Users\tarchibald\Zotero\storage\YM6EUIEY\full-text.pdf}
}

@book{blumComplexityRealComputation1998,
  title = {Complexity and {{Real Computation}}},
  author = {Blum, Lenore and Cucker, Felipe and Shub, Michael and Smale, Steve},
  year = {1998},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  isbn = {978-0-387-98281-6}
}

@inproceedings{boilletMultipleDocumentDatasets2021,
  title = {Multiple {{Document Datasets Pre-training Improves Text Line Detection With Deep Neural Networks}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Boillet, M{\'e}lodie and Kermorvant, Christopher and Paquet, Thierry},
  year = {2021},
  month = jan,
  eprint = {2012.14163},
  primaryclass = {cs},
  pages = {2134--2141},
  doi = {10.1109/ICPR48806.2021.9412447},
  urldate = {2023-08-16},
  abstract = {In this paper, we introduce a fully convolutional network for the document layout analysis task. While state-of-the-art methods are using models pre-trained on natural scene images, our method Doc-UFCN relies on a U-shaped model trained from scratch for detecting objects from historical documents. We consider the line segmentation task and more generally the layout analysis problem as a pixel-wise classification task then our model outputs a pixel-labeling of the input images. We show that Doc-UFCN outperforms state-of-the-art methods on various datasets and also demonstrate that the pre-trained parts on natural scene images are not required to reach good results. In addition, we show that pre-training on multiple document datasets can improve the performances. We evaluate the models using various metrics to have a fair and complete comparison between the methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,segmentation},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\IN82N64H\\Boillet et al. - 2021 - Multiple Document Datasets Pre-training Improves T.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\5K3PT8ZY\\2012.html}
}

@misc{borszukovszkiKnowWhatYou2025,
  title = {Know {{What You}} Do {{Not Know}}: {{Verbalized Uncertainty Estimation Robustness}} on {{Corrupted Images}} in {{Vision-Language Models}}},
  shorttitle = {Know {{What You}} Do {{Not Know}}},
  author = {Borszukovszki, Mirko and de Jong, Ivo Pascal and {Valdenegro-Toro}, Matias},
  year = {2025},
  month = apr,
  number = {arXiv:2504.03440},
  eprint = {2504.03440},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.03440},
  urldate = {2025-06-04},
  abstract = {To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers' uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\8EC6KMSY\\Borszukovszki et al. - 2025 - Know What You do Not Know Verbalized Uncertainty .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\RCEEI76I\\2504.html}
}

@inproceedings{bostromBytePairEncoding2020,
  title = {Byte {{Pair Encoding}} Is {{Suboptimal}} for {{Language Model Pretraining}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Bostrom, Kaj and Durrett, Greg},
  editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {4617--4624},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.414},
  urldate = {2025-03-16},
  abstract = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE`s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.},
  file = {C:\Users\tarchibald\Zotero\storage\WMPQPRVD\Bostrom and Durrett - 2020 - Byte Pair Encoding is Suboptimal for Language Mode.pdf}
}

@article{boudotComparingDifficultyFactorization,
  title = {Comparing the {{Difficulty}} of {{Factorization}} and {{Discrete Logarithm}}: A 240-Digit {{Experiment}}},
  author = {Boudot, Fabrice and Gaudry, Pierrick and Guillevic, Aurore and Heninger, Nadia and Thom{\'e}, Emmanuel and Zimmermann, Paul},
  doi = {10.1007/978-3-030-56880-1_3},
  urldate = {2021-05-24},
  abstract = {We report on two new records: the factorization of RSA-240, a 795-bit number, and a discrete logarithm computation over a 795-bit prime field. Previous records were the factorization of RSA-768 in 2009 and a 768-bit discrete logarithm computation in 2016. Our two computations at the 795-bit level were done using the same hardware and software, and show that computing a discrete logarithm is not much harder than a factorization of the same size. Moreover, thanks to algorithmic variants and well-chosen parameters, our computations were significantly less expensive than anticipated based on previous records. The last page of this paper also reports on the factorization of RSA-250.}
}

@techreport{briggsIntroductionGeneralNumber1998,
  title = {An {{Introduction}} to the {{General Number Field Sieve}}},
  author = {Briggs, Matthew E},
  year = {1998},
  urldate = {2021-05-28},
  abstract = {(ABSTRACT) With the proliferation of computers into homes and businesses and the explosive growth rate of the Internet, the ability to conduct secure electronic communications and transactions has become an issue of vital concern. One of the most prominent systems for securing electronic information, known as RSA, relies upon the fact that it is computationally difficult to factor a "large" integer into its component prime integers. If an efficient algorithm is developed that can factor any arbitrarily large integer in a "reasonable" amount of time, the security value of the RSA system would be nullified. The General Number Field Sieve algorithm is the fastest known method for factoring large integers. Research and development of this algorithm within the past five years has facilitated factorizations of integers that were once speculated to require thousands of years of supercomputer time to accomplish. While this method has many unexplored features that merit further research, the complexity of the algorithm prevents almost anyone but an expert from investigating its behavior. We address this concern by first pulling together much of the background information necessary to understand the concepts that are central in the General Number Field Sieve. These concepts are woven together into a cohesive presentation that details each theory while clearly describing how a particular theory fits into the algorithm. Formal proofs from existing literature are recast and illuminated to clarify their inner-workings and the role they play in the whole process. We also present a complete, detailed example of a factorization achieved with the General Number Field Sieve in order to concretize the concepts that are outlined.},
  keywords = {Algebraic Number Theory,Cryptography,Factoring,Number Field Sieve},
  file = {C:\Users\tarchibald\Zotero\storage\MCUVPW3A\full-text.pdf}
}

@techreport{Bromley,
  title = {Signature {{Verification}} Using a "{{Siamese}}" {{Time Delay Neural Network}}},
  author = {Bromley, Jane and Guyon, Isabelle and Lecun, Yann and Sickinger, Eduard and Shah, Roopak and Bell, At\&t and Holmdel, Laboratories},
  urldate = {2019-10-14},
  abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector {\textasciitilde}ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.},
  file = {C:\Users\tarchibald\Zotero\storage\6IRCECYG\full-text.pdf}
}

@inproceedings{bromley1994signature,
  title = {Signature Verification Using a" Siamese" Time Delay Neural Network},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bromley, Jane and Guyon, Isabelle and Lecun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak and Sickinger, Eduard and Shah, Roopak and Bell, At\&t and Holmdel, Laboratories and S{\"a}ckinger, Eduard and Shah, Roopak},
  year = {1994},
  pages = {737--744},
  urldate = {2019-10-14},
  abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector {\textasciitilde}ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.},
  file = {C:\Users\tarchibald\Zotero\storage\VFFZWYLM\Bromley et al. - Unknown - Signature Verification using a Siamese Time Delay Neural Network.pdf}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = may,
  journal = {arXiv},
  eprint = {2005.14165},
  publisher = {arXiv},
  urldate = {2021-01-28},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\DSIEHVLJ\full-text.pdf}
}

@article{buslaevAlbumentationsFastFlexible2020,
  title = {Albumentations: {{Fast}} and {{Flexible Image Augmentations}}},
  shorttitle = {Albumentations},
  author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
  year = {2020},
  month = feb,
  journal = {Information},
  volume = {11},
  number = {2},
  pages = {125},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info11020125},
  urldate = {2024-02-19},
  abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {computer vision,data augmentation,deep learning},
  file = {C:\Users\tarchibald\Zotero\storage\HKRLRT98\Buslaev et al. - 2020 - Albumentations Fast and Flexible Image Augmentati.pdf}
}

@article{Carbune2019,
  title = {Fast {{Multi-language LSTM-based Online Handwriting Recognition}}},
  author = {Carbune, Victor and Gonnet, Pedro and Deselaers, Thomas and Rowley, Henry A. and Daryin, Alexander and Calvo, Marcos and Wang, Li-Lun and Keysers, Daniel and Feuz, Sandro and Gervais, Philippe},
  year = {2019},
  month = feb,
  journal = {arXiv preprint arXiv:1902.10525},
  eprint = {1902.10525},
  urldate = {2019-10-05},
  abstract = {We describe an online handwriting system that is able to support 102 languages using a deep neural network architecture. This new system has completely replaced our previous Segment-and-Decode-based system and reduced the error rate by 20\%-40\% relative for most languages. Further, we report new state-of-the-art results on IAM-OnDB for both the open and closed dataset setting. The system combines methods from sequence recognition with a new input encoding using B{\textbackslash}'ezier curves. This leads to up to 10x faster recognition times compared to our previous system. Through a series of experiments we determine the optimal configuration of our models and report the results of our setup on a number of additional public datasets.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\R5HH33S2\Carbune et al. - 2019 - Fast Multi-language LSTM-based Online Handwriting Recognition.pdf}
}

@article{carbuneFastMultilanguageLSTMbased2019,
  title = {Fast {{Multi-language LSTM-based Online Handwriting Recognition}}},
  author = {Carbune, Victor and Gonnet, Pedro and Deselaers, Thomas and Rowley, Henry A. and Daryin, Alexander and Calvo, Marcos and Wang, Li-Lun and Keysers, Daniel and Feuz, Sandro and Gervais, Philippe},
  year = {2019},
  month = feb,
  eprint = {1902.10525},
  urldate = {2019-11-11},
  abstract = {We describe an online handwriting system that is able to support 102 languages using a deep neural network architecture. This new system has completely replaced our previous Segment-and-Decode-based system and reduced the error rate by 20\%-40\% relative for most languages. Further, we report new state-of-the-art results on IAM-OnDB for both the open and closed dataset setting. The system combines methods from sequence recognition with a new input encoding using B{\textbackslash}'ezier curves. This leads to up to 10x faster recognition times compared to our previous system. Through a series of experiments we determine the optimal configuration of our models and report the results of our setup on a number of additional public datasets.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\9QABD2MY\full-text.pdf}
}

@misc{caronUnsupervisedLearningVisual2021,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}} ({{SwAV}})},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = {2021},
  month = jan,
  number = {arXiv:2006.09882},
  eprint = {2006.09882},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.09882},
  urldate = {2023-08-24},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\BS8QVQLU\\Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\6R4VVI5P\\2006.html}
}

@article{carreiraHierarchicalPerceiver2022,
  title = {Hierarchical {{Perceiver}}},
  author = {Carreira, Joao and Koppula, Skanda and Zoran, Daniel and Recasens, Adria and Ionescu, Catalin and Henaff, Olivier and Shelhamer, Evan and Arandjelovic, Relja and Botvinick, Matt and Vinyals, Oriol and Simonyan, Karen and Zisserman, Andrew and Jaegle, Andrew},
  year = {2022},
  month = feb,
  eprint = {2202.10890},
  doi = {10.48550/arxiv.2202.10890},
  urldate = {2022-03-18},
  abstract = {General perception systems such as Perceivers can process arbitrary modalities in any combination and are able to handle up to a few hundred thousand inputs. They achieve this generality by exclusively using global attention operations. This however hinders them from scaling up to the inputs sizes required to process raw high-resolution images or video. In this paper, we show that some degree of locality can be introduced back into these models, greatly improving their efficiency while preserving their generality. To scale them further, we introduce a self-supervised approach that enables learning dense low-dimensional positional embeddings for very large signals. We call the resulting model a Hierarchical Perceiver (HiP). HiP retains the ability to process arbitrary modalities, but now at higher-resolution and without any specialized preprocessing, improving over flat Perceivers in both efficiency and accuracy on the ImageNet, Audioset and PASCAL VOC datasets.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\4WXPVHJ4\full-text.pdf}
}

@techreport{caseBeginnerGuideGeneral,
  title = {A {{Beginner}}'s {{Guide To The General Number Field Sieve}}},
  author = {Case, Michael},
  urldate = {2021-05-24},
  abstract = {RSA is a very popular public key cryptosystem. This algorithm is known to be secure, but this fact relies on the difficulty of factoring large numbers. Because of the popularity of the algorithm, much research has gone into this problem of factoring a large number.}
}

@misc{Chan2019,
  title = {No {{Title}}},
  author = {Chan, Chungkwong},
  year = {2019},
  month = may,
  urldate = {2019-10-05},
  abstract = {Offline handwritten mathematical expression recognition is often considered much harder than its online counterpart due to the absence of temporal information and the presence of background noise. In order to take advantage of the more developed techniques on online recognition and save resources, an oversegmentation approach is proposed to recover strokes from a textual bitmap image automatically. The proposed algorithm first break down the skeleton of a binarized image into junctions and segments, then segments are merged to form strokes, finally the ordering is determined by recursive projection and topological sort. Given a state-of-art online handwritten mathematical expression recognition system, the proposed procedure correctly recognized 58.22\%, 65.65\% and 65.05\% of the offline formulas rendered from CROHME 2014, 2016 and 2019 respectively. Therefore, the effectiveness of stroke extraction to offline recognition is justified.},
  keywords = {68T45,68U10,Handwritten mathematical expression recognition,Offline formula recognition,Op-tical character recognition,Stroke extraction,Stroke order normalization 2010 MSC 68T10},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\8H44CGVI\\Chan - 2019 - Stroke extraction for offline handwritten mathematical expression recognition(2).pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\Z8W3QBP6\\Chan - 2019 - Stroke extraction for offline handwritten mathematical expression recognition.pdf}
}

@inproceedings{Chang2018,
  title = {Generating {{Handwritten Chinese Characters Using CycleGAN}}},
  booktitle = {Proceedings - 2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}}, {{WACV}} 2018},
  author = {Chang, Bo and Zhang, Qiong and Pan, Shenyi and Meng, Lili},
  year = {2018},
  month = may,
  volume = {2018-Janua},
  pages = {199--207},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/WACV.2018.00028},
  abstract = {Handwriting of Chinese has long been an important skill in East Asia. However, automatic generation of handwritten Chinese characters poses a great challenge due to the large number of characters. Various machine learning techniques have been used to recognize Chinese characters, but few works have studied the handwritten Chinese character generation problem, especially with unpaired training data. In this work, we formulate the Chinese handwritten character generation as a problem that learns a mapping from an existing printed font to a personalized handwritten style. We further propose DenseNet CycleGAN to generate Chinese handwritten characters. Our method is applied not only to commonly used Chinese characters but also to calligraphy work with aesthetic values. Furthermore, we propose content accuracy and style discrepancy as the evaluation metrics to assess the quality of the handwritten characters generated. We then use our proposed metrics to evaluate the generated characters from CASIA dataset as well as our newly introduced Lanting calligraphy dataset.},
  isbn = {978-1-5386-4886-5}
}

@techreport{chanStrokeExtractionOffline2019,
  title = {Stroke Extraction for Offline Handwritten Mathematical Expression Recognition *},
  author = {Chan, Chungkwong},
  year = {2019},
  eprint = {1905.06749v1},
  urldate = {2020-01-15},
  abstract = {Offline handwritten mathematical expression recognition is often considered much harder than its online counterpart due to the absence of temporal information and the presence of background noise. In order to take advantage of the more developed techniques on online recognition and save resources, an oversegmentation approach is proposed to recover strokes from a textual bitmap image automatically. The proposed algorithm first break down the skeleton of a binarized image into junctions and segments, then segments are merged to form strokes, finally the ordering is determined by recursive projection and topological sort. Given a state-of-art online handwritten mathematical expression recognition system, the proposed procedure correctly recognized 58.22\%, 65.65\% and 65.05\% of the offline formulas rendered from CROHME 2014, 2016 and 2019 respectively. Therefore, the effectiveness of stroke extraction to offline recognition is justified.},
  archiveprefix = {arXiv},
  keywords = {68T45,68U10,Handwritten mathematical expression recognition,Offline formula recognition,Op-tical character recognition,Stroke extraction,Stroke order normalization 2010 MSC 68T10},
  file = {C:\Users\tarchibald\Zotero\storage\DB3N3QCX\full-text.pdf}
}

@article{Chao2017,
  title = {A Robot Calligraphy System: {{From}} Simple to Complex Writing by Human Gestures},
  author = {Chao, Fei and Huang, Yuxuan and Zhang, Xin and Shang, Changjing and Yang, Longzhi and Zhou, Changle and Hu, Huosheng and Lin, Chih Min},
  year = {2017},
  month = mar,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {59},
  pages = {1--14},
  publisher = {Elsevier Ltd},
  issn = {09521976},
  doi = {10.1016/j.engappai.2016.12.006},
  urldate = {2019-10-16},
  abstract = {Robotic writing is a very challenging task and involves complicated kinematic control algorithms and image processing work. This paper, alternatively, proposes a robot calligraphy system that firstly applies human arm gestures to establish a font database of Chinese character elementary strokes and English letters, then uses the created database and human gestures to write Chinese characters and English words. A three-dimensional motion sensing input device is deployed to capture the human arm trajectories, which are used to build the font database and to train a classifier ensemble. 26 types of human gesture are used for writing English letters, and 5 types of gesture are used to generate 5 elementary strokes for writing Chinese characters. By using the font database, the robot calligraphy system acquires a basic writing ability to write simple strokes and letters. Then, the robot can develop to write complex Chinese characters and English words by following human body movements. The classifier ensemble, which is used to identify each gesture, is implemented through using feature selection techniques and the harmony search algorithm, thereby achieving better classification performance. The experimental evaluations are carried out to demonstrate the feasibility and performance of the proposed method. By following the motion trajectories of the human right arm, the end-effector of the robot can successfully write the English words or Chinese characters that correspond to the arm trajectories.},
  keywords = {Classifier ensemble,Human gesture recognition,Human-robot interaction,Robotic calligraphy,Robotic writing},
  file = {C:\Users\tarchibald\Zotero\storage\J2QNTQXH\full-text.pdf}
}

@article{Chen2021,
  title = {Pix2seq: {{A Language Modeling Framework}} for {{Object Detection}}},
  author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
  year = {2021},
  month = sep,
  eprint = {2109.10852},
  doi = {10.48550/arxiv.2109.10852},
  urldate = {2022-03-23},
  abstract = {This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.},
  archiveprefix = {arXiv}
}

@article{chenConvolutionalNeuralNetworks2017,
  title = {Convolutional {{Neural Networks}} for {{Page Segmentation}} of {{Historical Document Images}}},
  author = {Chen, Kai and Seuret, Mathias and Hennebert, Jean and Ingold, Rolf},
  year = {2017},
  month = jul,
  journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
  volume = {1},
  pages = {965--970},
  publisher = {IEEE Computer Society},
  issn = {15205363},
  doi = {10.1109/ICDAR.2017.161},
  urldate = {2023-08-09},
  abstract = {This paper presents a page segmentation method for handwritten historical document images based on a Convolutional Neural Network (CNN). We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on hand-crafted features carefully tuned considering prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.},
  isbn = {9781538635865},
  keywords = {Convolutional neural network,Deep learning,Historical document images,Layout analysis,Page segmentation},
  file = {C:\Users\tarchibald\Zotero\storage\3S77UPCR\full-text.pdf}
}

@article{chenDiscriminativeCrossModalTransfer2020,
  title = {Discriminative {{Cross-Modal Transfer Learning}} and {{Densely Cross-Level Feedback Fusion}} for {{RGB-D Salient Object Detection}}},
  author = {Chen, Hao and Li, Youfu and Su, Dan},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Cybernetics},
  volume = {50},
  number = {11},
  pages = {4808--4820},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21682275},
  doi = {10.1109/TCYB.2019.2934986},
  urldate = {2021-02-27},
  abstract = {This article addresses two key issues in RGB-D salient object detection based on the convolutional neural network (CNN). 1) How to bridge the gap between the 'data-hungry' nature of CNNs and the insufficient labeled training data in the depth modality? 2) How to take full advantages of the complementary information among two modalities. To solve the first problem, we model the depth-induced saliency detection as a CNN-based cross-modal transfer learning problem. Instead of directly adopting the RGB CNN as initialization, we additionally train a modality classification network (MCNet) to encourage discriminative modality-specific representations in minimizing the modality classification loss. To solve the second problem, we propose a densely cross-level feedback topology, in which the cross-modal complements are combined in each level and then densely fed back to all shallower layers for sufficient cross-level interactions. Compared to traditional two-stream frameworks, the proposed one can better explore, select, and fuse cross-modal cross-level complements. Experiments show the significant and consistent improvements of the proposed CNN framework over other state-of-the-art methods.},
  pmid = {31484153},
  keywords = {Convolutional neural networks (CNNs),cross-modal transfer,dense feedback,RGB-D,salient object detection}
}

@inproceedings{chenExploringSimpleSiamese2021,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Xinlei and He, Kaiming},
  year = {2021},
  pages = {15750--15758},
  urldate = {2023-08-24},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\NT8447ZC\Chen and He - 2021 - Exploring Simple Siamese Representation Learning.pdf}
}

@misc{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  number = {arXiv:2002.05709},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.05709},
  urldate = {2023-08-23},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\6PCELRRP\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\RIMT6WQW\\2002.html}
}

@techreport{Choi,
  title = {{{StarGAN}}: {{Unified Generative Adversarial Networks}} for {{Multi-Domain Image-to-Image Translation}}},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  eprint = {1711.09020v3},
  urldate = {2019-09-30},
  abstract = {Figure 1. Multi-domain image-to-image translation results on the CelebA dataset via transferring knowledge learned from the RaFD dataset. The first and sixth columns show input images while the remaining columns are images generated by StarGAN. Note that the images are generated by a single generator network, and facial expression labels such as angry, happy, and fearful are from RaFD, not CelebA. Abstract Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\BYXWV7EV\full-text.pdf}
}

@article{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  journal = {arXiv},
  eprint = {1911.01547},
  publisher = {arXiv},
  urldate = {2021-02-01},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\S6SIPYA6\full-text.pdf}
}

@misc{chuConditionalPositionalEncodings2023,
  title = {Conditional {{Positional Encodings}} for {{Vision Transformers}}},
  author = {Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Shen, Chunhua},
  year = {2023},
  month = feb,
  number = {arXiv:2102.10882},
  eprint = {2102.10882},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.10882},
  urldate = {2023-08-23},
  abstract = {We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved performance. We implement CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings and delivers outperforming results. Our code is available at https://github.com/Meituan-AutoML/CPVT .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\U9HRIPC7\\Chu et al. - 2023 - Conditional Positional Encodings for Vision Transf.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\ZRBSCPS9\\2102.html}
}

@inproceedings{cohenEMNISTExtendingMNIST2017,
  title = {{{EMNIST}}: {{Extending MNIST}} to Handwritten Letters},
  shorttitle = {{{EMNIST}}},
  booktitle = {2017 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and {van Schaik}, Andr{\'e}},
  year = {2017},
  month = may,
  pages = {2921--2926},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2017.7966217},
  urldate = {2024-02-19},
  abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, the relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a dataset that constitutes a more challenging classification task involving letters and digits, and one that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results using an online ELM algorithm are presented along with a validation of the conversion process through the comparison of the classification results on NIST digits and the MNIST digits.},
  keywords = {Benchmark testing,Databases,NIST,Training},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\ZM6MGIYS\\Cohen et al. - 2017 - EMNIST Extending MNIST to handwritten letters.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\8FDQUC5K\\7966217.html}
}

@misc{cohenGroupEquivariantConvolutional2016,
  title = {Group {{Equivariant Convolutional Networks}}},
  author = {Cohen, Taco S. and Welling, Max},
  year = {2016},
  month = jun,
  number = {arXiv:1602.07576},
  eprint = {1602.07576},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.07576},
  urldate = {2023-08-25},
  abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\C23XLT2F\\Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\C9NIMIXW\\1602.html}
}

@misc{ComputervisionrecipesContribDocument_cleanup,
  title = {Computervision-Recipes/Contrib/Document\_cleanup/light\_weight\_document\_cleanup\_{{ICDAR2021}} at Master {$\cdot$} Microsoft/Computervision-Recipes},
  journal = {GitHub},
  urldate = {2023-09-11},
  abstract = {Best Practices, code samples, and documentation for Computer Vision. - microsoft/computervision-recipes},
  howpublished = {https://github.com/microsoft/computervision-recipes/tree/master/contrib/document\_cleanup/light\_weight\_document\_cleanup\_ICDAR2021},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\HPZ5I92G\light_weight_document_cleanup_ICDAR2021.html}
}

@article{Coquenet2022,
  title = {End-to-End {{Handwritten Paragraph Text Recognition Using}} a {{Vertical Attention Network}}},
  author = {Coquenet, Denis and Chatelain, Clement Cl{\'e}ment and Paquet, Thierry},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2012.03868},
  publisher = {IEEE Computer Society},
  issn = {19393539},
  doi = {10.1109/TPAMI.2022.3144899},
  urldate = {2022-03-24},
  abstract = {Unconstrained handwritten text recognition remains challenging for computer vision systems. Paragraph text recognition is traditionally achieved by two models: the first one for line segmentation and the second one for text line recognition. We propose a unified end-to-end model using hybrid attention to tackle this task. This model is designed to iteratively process a paragraph image line by line. It can be split into three modules. An encoder generates feature maps from the whole paragraph image. Then, an attention module recurrently generates a vertical weighted mask enabling to focus on the current text line features. This way, it performs a kind of implicit line segmentation. For each text line features, a decoder module recognizes the character sequence associated, leading to the recognition of a whole paragraph. We achieve state-of-the-art character error rate at paragraph level on three popular datasets: 1.91\% for RIMES, 4.45\% for IAM and 3.59\% for READ 2016. Our code and trained model weights are available at https://github.com/FactoDeepLearning/VerticalAttentionOCR.},
  archiveprefix = {arXiv},
  keywords = {Character recognition,Encoder-decoder,Fully Convolutional Network,Hidden Markov models,Hybrid attention,Image recognition,Image segmentation,Optical Character Recognition,Optical character recognition software,Paragraph handwriting recognition,Segmentation-free,Seq2Seq model,Task analysis,Text recognition},
  file = {C:\Users\tarchibald\Zotero\storage\MJ65U3LC\Coquenet, Chatelain, Paquet - 2020 - End-to-end Handwritten Paragraph Text Recognition Using a Vertical Attention Network.pdf}
}

@article{coquenetSPANSimplePredict2021,
  title = {{{SPAN}}: A {{Simple Predict}} \& {{Align Network}} for {{Handwritten Paragraph Recognition}}},
  author = {Coquenet, Denis and Chatelain, Cl{\'e}ment and Paquet, Thierry},
  year = {2021},
  month = feb,
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {12823 LNCS},
  eprint = {2102.08742v1},
  pages = {70--84},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-3-030-86334-0_5},
  urldate = {2022-03-24},
  abstract = {Unconstrained handwriting recognition is an essential task in document analysis. It is usually carried out in two steps. First, the document is segmented into text lines. Second, an Optical Character Recognition model is applied on these line images. We propose the Simple Predict \& Align Network: an end-to-end recurrence-free Fully Convolutional Network performing OCR at paragraph level without any prior segmentation stage. The framework is as simple as the one used for the recognition of isolated lines and we achieve competitive results on three popular datasets: RIMES, IAM and READ 2016. The proposed model does not require any dataset adaptation, it can be trained from scratch, without segmentation labels, and it does not require line breaks in the transcription labels. Our code and trained model weights are available at https://github.com/FactoDeepLearning/SPAN.},
  archiveprefix = {arXiv},
  keywords = {Fully convolutional network,Handwritten paragraph recognition,Network,Recurrence-free model},
  file = {C:\Users\tarchibald\Zotero\storage\CNCY5K5D\full-text.pdf}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  year = {1967},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  issn = {1557-9654},
  doi = {10.1109/TIT.1967.1053964},
  urldate = {2024-05-22},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast {\l}eq R {\l}eq R{\textasciicircum}{\textbackslash}ast(2 --MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.}
}

@article{crawfordDatabaseHandwritingSamples2020,
  title = {A Database of Handwriting Samples for Applications in Forensic Statistics},
  author = {Crawford, Amy and Ray, Anyesha and Carriquiry, Alicia},
  year = {2020},
  month = feb,
  journal = {Data in Brief},
  volume = {28},
  pages = {105059},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2019.105059},
  urldate = {2024-02-19},
  abstract = {Handwriting samples were collected from 90 adults for the purpose of developing statistical approaches to the evaluation of handwriting as forensic evidence. Each participant completed three data collection sessions, each at least three weeks apart. At each session, a survey was completed and three writing prompts were each transcribed three times. In total, the repository includes 2430 handwriting sample images as well as demographic and session specific information for all 90 participants. The writing samples were scanned and instructional header text was cropped out to obtain the raw writing data as image files. Survey data are provided in table format. Reliable methods for data management were incorporated through systematic document generation, QR code text embedding, and the development of an application to facilitate data entry and automated file naming and handling. The data presented in this article were collected by researchers at the Center for Statistics and Applications in Forensic Evidence (CSAFE) at Iowa State University.},
  keywords = {Forensics,Handwriting,Image analysis,Pattern evidence,Statistics},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\AUAVS4BU\\Crawford et al. - 2020 - A database of handwriting samples for applications.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\GX3M9VQZ\\S2352340919314155.html}
}

@misc{crosillaBenchmarkingLargeLanguage2025,
  title = {Benchmarking {{Large Language Models}} for {{Handwritten Text Recognition}}},
  author = {Crosilla, Giorgia and Klic, Lukas and Colavizza, Giovanni},
  year = {2025},
  month = mar,
  number = {arXiv:2503.15195},
  eprint = {2503.15195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.15195},
  urldate = {2025-06-01},
  abstract = {Traditional machine learning models for Handwritten Text Recognition (HTR) rely on supervised training, requiring extensive manual annotations, and often produce errors due to the separation between layout and text processing. In contrast, Multimodal Large Language Models (MLLMs) offer a general approach to recognizing diverse handwriting styles without the need for model-specific training. The study benchmarks various proprietary and open-source LLMs against Transkribus models, evaluating their performance on both modern and historical datasets written in English, French, German, and Italian. In addition, emphasis is placed on testing the models' ability to autonomously correct previously generated outputs. Findings indicate that proprietary models, especially Claude 3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs achieve excellent results in recognizing modern handwriting and exhibit a preference for the English language due to their pre-training dataset composition. Comparisons with Transkribus show no consistent advantage for either approach. Moreover, LLMs demonstrate limited ability to autonomously correct errors in zero-shot transcriptions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\CH6QQ8D7\\Crosilla et al. - 2025 - Benchmarking Large Language Models for Handwritten.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\UMQLMM63\\2503.html}
}

@misc{CTCNetworksLanguage,
  title = {{{CTC Networks}} and {{Language Models}}: {{Prefix Beam Search Explained}} {\textbar} by {{Lasse Borgholt}} {\textbar} {{Corti}} {\textbar} {{Medium}}},
  urldate = {2021-02-27},
  howpublished = {https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306}
}

@article{Cuturi2017,
  title = {Soft-{{DTW}}: {{A}} Differentiable Loss Function for Time-Series},
  author = {Cuturi, Marco and Blondel, Mathieu},
  year = {2017},
  month = mar,
  journal = {34th International Conference on Machine Learning, ICML 2017},
  volume = {2},
  eprint = {1703.01541},
  pages = {1483--1505},
  urldate = {2019-10-14},
  abstract = {We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regular-ization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.},
  archiveprefix = {arXiv},
  isbn = {9781510855144},
  file = {C:\Users\tarchibald\Zotero\storage\KNE4JSIJ\Cuturi, Blondel - 2017 - Soft-DTW a Differentiable Loss Function for Time-Series(2).pdf}
}

@article{czarneckiUnderstandingSyntheticGradients2017,
  title = {Understanding {{Synthetic Gradients}} and {{Decoupled Neural Interfaces}}},
  author = {Czarnecki, Wojciech Marian and {\'S}wirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2017},
  month = mar,
  journal = {34th International Conference on Machine Learning, ICML 2017},
  volume = {2},
  eprint = {1703.00522},
  pages = {1506--1521},
  publisher = {International Machine Learning Society (IMLS)},
  urldate = {2020-12-24},
  abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated - resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\FEAF7FWU\full-text.pdf}
}

@techreport{Dai,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  eprint = {1901.02860v3},
  urldate = {2019-09-23},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers , achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on en-wiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch 1 .},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\DJNNHJRH\full-text.pdf}
}

@inproceedings{danicaActivePerceptionDeep2019,
  title = {From Active Perception to Deep Learning},
  booktitle = {Science {{Robotics}}},
  author = {{Danica}},
  year = {2019},
  month = jun,
  volume = {3},
  eprint = {1406.2661},
  pages = {2672--2680},
  publisher = {Neural information processing systems foundation},
  issn = {10495258},
  urldate = {2020-10-28},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\QXU5UYTC\full-text.pdf}
}

@article{dascoliConViTImprovingVision2021,
  title = {{{ConViT}}: {{Improving Vision Transformers}} with {{Soft Convolutional Inductive Biases}}},
  author = {{d'Ascoli}, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  year = {2021},
  month = mar,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2022},
  number = {11},
  eprint = {2103.10697v2},
  publisher = {Institute of Physics},
  doi = {10.1088/1742-5468/ac9830},
  urldate = {2023-04-12},
  abstract = {Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ``soft" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.},
  archiveprefix = {arXiv},
  keywords = {deep learning,machine learning}
}

@techreport{dasDewarpNetSingleImageDocument,
  title = {{{DewarpNet}}: {{Single-Image Document Unwarping With Stacked 3D}} and {{2D Regression Networks}}},
  author = {Das, Sagnik and Ma, Ke and Shu, Zhixin and Samaras, Dimitris and Shilkrot, Roy},
  urldate = {2020-09-10},
  abstract = {Capturing document images with hand-held devices in unstructured environments is a common practice nowadays. However, "casual" photos of documents are usually unsuitable for automatic information extraction, mainly due to physical distortion of the document paper, as well as various camera positions and illumination conditions. In this work, we propose DewarpNet, a deep-learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. Also, we contribute the largest and most comprehensive dataset for document image unwarping to date-Doc3D. This dataset features multiple ground-truth annotations, including 3D shape, surface normals, UV map, albedo image, etc. Training with Doc3D, we demonstrate state-of-the-art performance for DewarpNet with extensive qualitative and quantitative evaluations. Our network also significantly improves OCR performance on captured document images, decreasing character error rate by 42\% on average. Both the code and the dataset are released 1 .},
  file = {C:\Users\tarchibald\Zotero\storage\7VGV3SRI\full-text.pdf}
}

@techreport{dauphinMetaInitInitializingLearning,
  title = {{{MetaInit}}: {{Initializing}} Learning by Learning to Initialize},
  author = {Dauphin, Yann N and Ai, Google and Schoenholz, Samuel S},
  urldate = {2020-01-17},
  abstract = {Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called MetaInit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. MetaInit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. MetaInit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.},
  file = {C:\Users\tarchibald\Zotero\storage\IMWNDHKX\full-text.pdf}
}

@misc{davisDeepVisualTemplateFree2019,
  title = {Deep {{Visual Template-Free Form Parsing}}},
  author = {Davis, Brian and Morse, Bryan and Cohen, Scott and Price, Brian and Tensmeyer, Chris},
  year = {2019},
  month = sep,
  number = {arXiv:1909.02576},
  eprint = {1909.02576},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.02576},
  urldate = {2024-02-12},
  abstract = {Automatic, template-free extraction of information from form images is challenging due to the variety of form layouts. This is even more challenging for historical forms due to noise and degradation. A crucial part of the extraction process is associating input text with pre-printed labels. We present a learned, template-free solution to detecting pre-printed text and input text/handwriting and predicting pair-wise relationships between them. While previous approaches to this problem have been focused on clean images and clear layouts, we show our approach is effective in the domain of noisy, degraded, and varied form images. We introduce a new dataset of historical form images (late 1800s, early 1900s) for training and validating our approach. Our method uses a convolutional network to detect pre-printed text and input text lines. We pool features from the detection network to classify possible relationships in a language-agnostic way. We show that our proposed pairing method outperforms heuristic rules and that visual features are critical to obtaining high accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\PN4HWY85\\Davis et al. - 2019 - Deep Visual Template-Free Form Parsing.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\SEJFQ532\\1909.html}
}

@inproceedings{davisEndtoEndDocumentRecognition2023,
  title = {End-to-{{End Document Recognition}} and~{{Understanding}} with~{{Dessurt}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022 {{Workshops}}: {{Tel Aviv}}, {{Israel}}, {{October}} 23--27, 2022, {{Proceedings}}, {{Part IV}}},
  author = {Davis, Brian and Morse, Bryan and Price, Brian and Tensmeyer, Chris and Wigington, Curtis and Morariu, Vlad},
  year = {2023},
  month = feb,
  pages = {280--296},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-25069-9_19},
  urldate = {2023-08-29},
  abstract = {We introduce Dessurt, a relatively simple document understanding transformer capable of being fine-tuned on a greater variety of document tasks than prior methods. It receives a document image and task string as input and generates arbitrary text autoregressively as output. Because Dessurt is an end-to-end architecture that performs text recognition in addition to document understanding, it does not require an external recognition model as prior methods do. Dessurt is a more flexible model than prior methods and is able to handle a variety of document domains and tasks. We show that this model is effective at 9 different dataset-task combinations.},
  isbn = {978-3-031-25068-2},
  keywords = {Document understanding,End-to-end,Form understanding,Handwriting recognition,OCR},
  file = {C:\Users\tarchibald\Zotero\storage\ITTTF4E3\Davis et al. - 2023 - End-to-End Document Recognition andUnderstanding .pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine}
}

@inproceedings{destefanoEfficientMethodOnline2004,
  title = {An Efficient Method for Online Cursive Handwriting Strokes Reordering},
  booktitle = {International {{Journal}} of {{Pattern Recognition}} and {{Artificial Intelligence}}},
  author = {De Stefano, Claudio and Marcelli, Angelo},
  year = {2004},
  month = nov,
  volume = {18},
  pages = {1157--1171},
  publisher = {World Scientific Publishing Company},
  issn = {02180014},
  doi = {10.1142/S0218001404003691},
  urldate = {2020-04-10},
  abstract = {In the framework of online cursive handwriting recognition, we present an efficient method for reordering the sequence of strokes composing handwriting in two special cases of interest: the horizontal bar of the character "t" and the dot of the character "i". The proposed method exploits shape information for selecting the strokes that most likely correspond to the features of interest, and layout and topological information for locating the strokes representing the body of the characters to which the features belong to. The method does not depend on the specific algorithm used for detecting the elementary strokes in which the electronic ink may be decomposed into. The performance of our method, evaluated on a data set of cursive words produced by 50 different writers, has shown a correct reordering of the sequence in more than 85\% of the cases. Thus, the proposed method allows obtaining a more stable and invariant description of the electronic ink in terms of elementary stroke sequences, and therefore can be helpfully used as a preprocessing step for both segmentation-based and word-based handwriting recognition systems.},
  keywords = {Feature extraction,Handwriting generation models,Online handwriting,Shape analysis,Stroke detection},
  file = {C:\Users\tarchibald\Zotero\storage\KK22F4MZ\full-text.pdf}
}

@article{diazRethinkingTextLine2021,
  title = {Rethinking {{Text Line Recognition Models}}},
  author = {Diaz, Daniel Hernandez and Qin, Siyang and Ingle, Reeve and Fujii, Yasuhisa and Bissacco, Alessandro},
  year = {2021},
  month = apr,
  eprint = {2104.07787},
  urldate = {2021-11-02},
  abstract = {In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\WRIZRJFV\full-text.pdf}
}

@article{dimmickStructuredFormsDatabase2010,
  title = {Structured {{Forms Database}} 2},
  author = {Dimmick, D L and Garris, M D},
  year = {2010},
  month = aug,
  journal = {NIST},
  abstract = {NIST Structured Forms Reference Set of Binary Images (SFRS) Price: No charge},
  langid = {english},
  annotation = {Last Modified: 2019-04-27T09:56-04:00},
  file = {C:\Users\tarchibald\Zotero\storage\ELDRCG2L\Dimmick and Garris - Structured Forms Database 2.pdf}
}

@article{dingImprovingHandwrittenOCR2023,
  title = {Improving {{Handwritten OCR}} with {{Training Samples Generated}} by {{Glyph Conditional Denoising Diffusion Probabilistic Model}}},
  author = {Ding, Haisong and Luan, Bozhi and Gui, Dongnan and Chen, Kai and Huo, Qiang},
  year = {2023},
  month = may,
  eprint = {2305.19543},
  urldate = {2023-08-08},
  abstract = {Constructing a highly accurate handwritten OCR system requires large amounts of representative training data, which is both time-consuming and expensive to collect. To mitigate the issue, we propose a denoising diffusion probabilistic model (DDPM) to generate training samples. This model conditions on a printed glyph image and creates mappings between printed characters and handwritten images, thus enabling the generation of photo-realistic handwritten samples with diverse styles and unseen text contents. However, the text contents in synthetic images are not always consistent with the glyph conditional images, leading to unreliable labels of synthetic samples. To address this issue, we further propose a progressive data filtering strategy to add those samples with a high confidence of correctness to the training set. Experimental results on IAM benchmark task show that OCR model trained with augmented DDPM-synthesized training samples can achieve about 45\% relative word error rate reduction compared with the one trained on real data only.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\XU5GKV5E\full-text.pdf}
}

@article{Dinh2016,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2016},
  month = may,
  journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  eprint = {1605.08803},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2020-07-23},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\4CF593NC\full-text.pdf}
}

@article{dinhRecoveryDrawingOrder2016,
  title = {Recovery of Drawing Order from Multi-Stroke {{English}} Handwritten Images Based on Graph Models and Ambiguous Zone Analysis},
  author = {Dinh, Minh and Yang, Hyung Jeong and Lee, Guee Sang and Kim, Soo Hyung and Do, Luu Ngoc},
  year = {2016},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {64},
  pages = {352--364},
  publisher = {Elsevier Ltd},
  issn = {09574174},
  doi = {10.1016/j.eswa.2016.08.017},
  urldate = {2020-10-16},
  abstract = {Recovery of the drawing order of strokes in a handwritten image can be seen as searching for the smoothest path for each stroke on an undirected graph that is constructed from the skeleton of the handwritten image. However, this requires correcting for separating strokes, and detecting starting points. Moreover, ambiguousness at junction points increases the complexity of finding the smoothest paths. In order to resolve these issues, an effective approach that can simultaneously detect the points to separate strokes and find the optimal path for each stroke is proposed. To reduce the complexity of the problem, the skeleton graph of the handwritten image is used, and touching characters or crossing strokes are separated. Touches or crossings of stroke parts at ambiguous zones are detected and the smoothness values are adjusted to improve the accuracy. The greedy algorithm and Dijkstra'salgorithm with a well-defined function of smoothness are applied in searching the optimal path. The nature of the recovery is increased when the optimal path is split into many strokes by using the curvatures of the edges, the un-smoothness between edges and the appearance of double-traced edges. Finally, pixel sequences of strokes are extracted and ordered by using rules of handwriting. The effectiveness of the proposed method is demonstrated through low error rates of pixel sequence comparison and high accuracy of online recognition.},
  keywords = {Drawing order,Multi-stroke handwriting,Smoothest path,Static handwritten image,Trajectory recovery},
  file = {C:\Users\tarchibald\Zotero\storage\WDKQNA6X\full-text.pdf}
}

@article{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  month = mar,
  eprint = {1703.04933},
  urldate = {2019-12-05},
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\7AWTKHBX\full-text.pdf}
}

@misc{DocILEDocumentInformation2023,
  title = {{{DocILE}}: {{Document Information Localization}} and {{Extraction Benchmark}}},
  shorttitle = {{{DocILE}}},
  year = {2023},
  month = aug,
  urldate = {2023-08-23},
  abstract = {DocILE: Document Information Localization and Extraction Benchmark},
  copyright = {MIT},
  howpublished = {Rossum},
  keywords = {benchmark,document,extraction,information,key,kie,kile,understanding}
}

@techreport{Doersch2016,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = jun,
  eprint = {1606.05908v2},
  urldate = {2019-10-12},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits [1, 2], faces [1, 3, 4], house numbers [5, 6], CIFAR images [6], physical models of scenes [4], segmentation [7], and predicting the future from static images [8]. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  keywords = {neural networks,structured prediction,unsupervised learning,variational autoencoders},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\NL8GR39R\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\TMVXJYK9\\Doersch - 2016 - Tutorial on Variational Autoencoders(2).pdf}
}

@inproceedings{donahueDeCAFDeepConvolutional2014,
  title = {{{DeCAF}}: {{A Deep Convolutional Activation Feature}} for {{Generic Visual Recognition}}},
  shorttitle = {{{DeCAF}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  year = {2014},
  month = jan,
  pages = {647--655},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2023-08-16},
  abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks.  Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks.  We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges.  We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges.  We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\A7KPGIKV\Donahue et al. - 2014 - DeCAF A Deep Convolutional Activation Feature for.pdf}
}

@techreport{dosovitskiyIMAGEWORTH16X16,
  title = {{{AN IMAGE IS WORTH 16X16 WORDS}}: {{TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  eprint = {2010.11929v1},
  urldate = {2021-03-03},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\MVR7GRYA\full-text.pdf}
}

@article{drainGeneratingBugFixesUsing2021,
  title = {Generating {{Bug-Fixes Using Pretrained Transformers}}},
  author = {Drain, Dawn and Wu, Chen and Svyatkovskiy, Alexey and Sundaresan, Neel},
  year = {2021},
  month = apr,
  journal = {MAPS 2021 - Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming, co-located with PLDI 2021},
  eprint = {2104.07896},
  pages = {1--8},
  publisher = {Association for Computing Machinery, Inc},
  doi = {10.1145/3460945.3464951},
  urldate = {2021-10-13},
  abstract = {Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33\% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32\%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75\% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.},
  archiveprefix = {arXiv},
  keywords = {Bug-fixing,Bugpatching,Program repair,Transformers},
  file = {C:\Users\tarchibald\Zotero\storage\R8AVC6M8\full-text.pdf}
}

@inproceedings{drobyUnsupervisedDeepLearning2020,
  title = {Unsupervised {{Deep Learning}} for {{Handwritten Page Segmentation}}},
  booktitle = {2020 17th {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}} ({{ICFHR}})},
  author = {Droby, Ahmad and Barakat, Berat Kurar and Madi, Borak and Alaasam, Reem and {El-Sana}, Jihad},
  year = {2020},
  month = sep,
  pages = {240--245},
  doi = {10.1109/ICFHR2020.2020.00052},
  abstract = {Segmenting handwritten document images into regions with homogeneous patterns is an important pre-processing step for many document images analysis tasks. Hand-labeling data to train a deep learning model for layout analysis requires significant human effort. In this paper, we present an unsupervised deep learning method for page segmentation, which revokes the need for annotated images. A siamese neural network is trained to differentiate between patches using their measurable properties such as number of foreground pixels, and average component height and width. The network is trained that spatially nearby patches are similar. The network's learned features are used for page segmentation, where patches are classified as main and side text based on the extracted features. We tested the method on a dataset of handwritten document images with quite complex layouts. Our experiments show that the proposed unsupervised method is as effective as typical supervised methods.},
  keywords = {Deep learning,deep-learning,documents,Feature extraction,hand-written,historical,Image color analysis,Image segmentation,Layout,layout analysis,page segmentation,segmentation,Siamese network,Task analysis,Training,unsupervised},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\FLZZ79CP\\Droby et al. - 2020 - Unsupervised Deep Learning for Handwritten Page Se.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\I8UHWQMV\\9257732.html}
}

@inproceedings{Dutta2018,
  title = {Improving {{CNN-RNN}} Hybrid Networks for Handwriting Recognition},
  booktitle = {Proceedings of {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}, {{ICFHR}}},
  author = {Dutta, Kartik and Krishnan, Praveen and Mathew, Minesh and Jawahar, C. V.},
  year = {2018},
  month = dec,
  volume = {2018-Augus},
  pages = {80--85},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21676453},
  doi = {10.1109/ICFHR-2018.2018.00023},
  urldate = {2019-09-06},
  abstract = {{\copyright} 2018 IEEE. The success of deep learning based models have centered around recent architectures and the availability of large scale annotated data. In this work, we explore these two factors systematically for improving handwritten recognition for scanned off-line document images. We propose a modified CNN-RNN hybrid architecture with a major focus on effective training using: (i) efficient initialization of network using synthetic data for pretraining, (ii) image normalization for slant correction and (iii) domain specific data transformation and distortion for learning important invariances. We perform a detailed ablation study to analyze the contribution of individual modules and present state of art results for the task of unconstrained line and word recognition on popular datasets such as IAM, RIMES and GW.},
  isbn = {978-1-5386-5875-8},
  keywords = {CNN RNN network,Data augmentation,Handwriting recognition,Image pre-processing},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\5HIDT9ZJ\\Dutta et al. - 2018 - Improving CNN-RNN hybrid networks for handwriting recognition(2).pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\NPCC6UNZ\\Dutta et al. - 2018 - Improving CNN-RNN hybrid networks for handwriting recognition.pdf}
}

@article{eccvSelectionConvConvolutionalNeural,
  title = {{{SelectionConv}} : {{Convolutional Neural Networks}} for {{Non-rectilinear Image Data}}},
  author = {Eccv, Anonymous},
  keywords = {graph convolution,irregular images,spherical images,superpixels,texture maps,transfer learning},
  file = {C:\Users\tarchibald\Zotero\storage\ME2GCRN5\Graph Convolution.pdf}
}

@inproceedings{elbaatiTemporalOrderRecovery2009,
  title = {Temporal Order Recovery of the Scanned Handwriting},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Elbaati, Abdelkarim and Kherallah, Monji and Ennaji, Abdellatif and Alimi, Adel M.},
  year = {2009},
  pages = {1116--1120},
  issn = {15205363},
  doi = {10.1109/ICDAR.2009.266},
  urldate = {2020-07-27},
  abstract = {In this paper, we present a new approach to the temporal order restoration of the off-line handwriting. After the pre-processing steps of the word image, a suitable algorithm makes it possible to segment its skeleton in three types of strokes. After that, we developed a genetic algorithm GA in order to optimize the best trajectory of these segments. The repetition of a segment will be studied in a secondary algorithm so that we do not disturb the GA operations. The techniques used in GA are the selection, crossover and the mutation. The fitness function value depends on right-left direction (direction of the Arab writing), the segments repetition and angular deviation on the crossing of the occlusion stroke. To validate our approach, we tested it on the On/Off LMCA dual Arabic handwriting, the Latin IRONOFF and the off-line IFN/ENIT datasets. {\copyright} 2009 IEEE.},
  isbn = {978-0-7695-3725-2},
  file = {C:\Users\tarchibald\Zotero\storage\5TY87CSH\full-text.pdf}
}

@article{eliasmithAttractorNetwork2007,
  title = {Attractor Network},
  author = {Eliasmith, Chris},
  year = {2007},
  month = oct,
  journal = {Scholarpedia},
  volume = {2},
  number = {10},
  pages = {1380},
  publisher = {Scholarpedia},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.1380},
  urldate = {2021-02-27},
  abstract = {In general, an attractor network is a network of nodes (i.e., neurons in a biological network), often recurrently connected, whose time dynamics settle to a stable pattern. That pattern may be stationary, time-varying (e.g. cyclic), or even stochastic-looking (e.g., chaotic). The particular pattern a network settles to is called its `attractor'. In theoretical neuroscience, different kinds of attractor neural networks have been associated with different functions, such as memory, motor behavior, and classification. Describing networks as attractor networks allows researchers to employ methods of dynamical systems theory to quantitatively analyze their characteristics (e.g. stability, robustness, etc.).    More precisely, an attractor network is a set of N network nodes connected in such a way that their global dynamics becomes stable in a D dimensional space, where usually N{$>>$}D. This assumes that there is no additional external input, and that stability indicates that the network state resides, over time, on some D-manifold (e.g., line, circle, plane, toroid, etc.).    Contents [hide]   1 Kinds of attractor network  1.1 Point attractors  1.2 Line, ring, and plane attractors  1.3 Cyclic attractors  1.4 Chaotic attractors  2 Biological interpretations of attractor networks  2.1 Point attractors  2.2 Line attractors  2.3 Ring attractors  2.4 Plane attractors  2.5 Cyclic attractors  2.6 Chaotic attractors  3 Using attractor networks  4 References  5 External links    -------  Kinds of attractor network  It is important to keep two spaces distinct when discussing attractor networks. These are:    the network state space (which is determined by the set of all possible node states); and  the attractor space (which is a subspace of the network state space that only includes points on an attractor).  These are not always distinguished. The subsequent discussion attempts to be clear about which is being described.}
}

@techreport{EllipticCurvePublic,
  title = {Elliptic {{Curve Public Key Cryptography Why}}?},
  urldate = {2021-05-24},
  abstract = {{$\bullet$} ECC offers greater security for a given key size.},
  file = {C:\Users\tarchibald\Zotero\storage\EHFUF4BG\full-text.pdf}
}

@misc{ExplicitWordError,
  title = {Explicit Word Error Minimization in N-Best List Rescoring},
  journal = {ResearchGate},
  urldate = {2025-06-01},
  abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
  howpublished = {https://www.researchgate.net/publication/221489308\_Explicit\_word\_error\_minimization\_in\_n-best\_list\_rescoring},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\5M8GURQB\221489308_Explicit_word_error_minimization_in_n-best_list_rescoring.html}
}

@misc{FakeW2US,
  title = {Fake {{W-2}} ({{US Tax Form}}) {{Dataset}}},
  urldate = {2023-09-07},
  abstract = {W-2 Data set clean and noise added images (PDF/JPG files)},
  howpublished = {https://www.kaggle.com/datasets/mcvishnu1/fake-w2-us-tax-form-dataset},
  langid = {english}
}

@article{fengDocPediaUnleashingPower2024,
  title = {{{DocPedia}}: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding},
  shorttitle = {{{DocPedia}}},
  author = {Feng, Hao and Liu, Qi and Liu, Hao and Tang, Jingqun and Zhou, Wengang and Li, Houqiang and Huang, Can},
  year = {2024},
  month = dec,
  journal = {Science China Information Sciences},
  volume = {67},
  number = {12},
  pages = {220106},
  issn = {1869-1919},
  doi = {10.1007/s11432-024-4250-y},
  urldate = {2025-04-04},
  abstract = {In this work, we present DocPedia, a novel large multimodal model (LMM) for versatile OCR-free document understanding, capable of parsing images up to 2560 {\texttimes} 2560 resolution. Unlike existing studies that either struggle with high-resolution documents or give up the large language model thus vision or language ability constrained, our DocPedia directly processes visual input in the frequency domain rather than the pixel space. The unique characteristic enables DocPedia to capture a greater amount of visual and textual information using a limited number of visual tokens. To consistently enhance both the perception and comprehension abilities of our DocPedia, we develop a dual-stage training strategy and enrich instructions/annotations of all training tasks covering multiple document types. Extensive quantitative and qualitative experiments are conducted on various publicly available benchmarks and the results confirm the mutual benefits of jointly learning perception and comprehension tasks. The results provide further evidence of the effectiveness and superior performance of our DocPedia over other methods.},
  langid = {english},
  keywords = {document understanding,frequency,high-resolution,large multimodal model,OCR-free},
  file = {C:\Users\tarchibald\Zotero\storage\JEYRGRCU\Feng et al. - 2024 - DocPedia unleashing the power of large multimodal.pdf}
}

@techreport{fengMultiLayeredGradientBoosting2018,
  title = {Multi-{{Layered Gradient Boosting Decision Trees}}},
  author = {Feng, Ji and Yu, Yang and Zhou, Zhi-Hua},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {31},
  pages = {3551--3561},
  urldate = {2020-12-24},
  abstract = {Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability.},
  file = {C:\Users\tarchibald\Zotero\storage\A9MJTUKI\full-text.pdf}
}

@inproceedings{fiscusPostprocessingSystemYield1997,
  title = {A Post-Processing System to Yield Reduced Word Error Rates: {{Recognizer Output Voting Error Reduction}} ({{ROVER}})},
  shorttitle = {A Post-Processing System to Yield Reduced Word Error Rates},
  booktitle = {1997 {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding Proceedings}}},
  author = {Fiscus, J.G.},
  year = {1997},
  month = dec,
  pages = {347--354},
  doi = {10.1109/ASRU.1997.659110},
  urldate = {2025-06-01},
  abstract = {Describes a system developed at NIST to produce a composite automatic speech recognition (ASR) system output when the outputs of multiple ASR systems are available, and for which, in many cases, the composite ASR output has a lower error rate than any of the individual systems. The system implements a "voting" or rescoring process to reconcile differences in ASR system outputs. We refer to this system as the NIST Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources are added to an ASR system (e.g. acoustic and language models), error rates are typically decreased. This paper describes a post-recognition process which models the output generated by multiple ASR systems as independent knowledge sources that can be combined and used to generate an output with reduced error rate. To accomplish this, the outputs of multiple of ASR systems are combined into a single, minimal-cost word transition network (WTN) via iterative applications of dynamic programming (DP) alignments. The resulting network is searched by an automatic rescoring or "voting" process that selects the output sequence with the lowest score.},
  keywords = {Automatic speech recognition,Benchmark testing,Conferences,Costs,Dynamic programming,Error analysis,NIST,Statistical analysis,System testing,Voting}
}

@article{fjellandWhyGeneralArtificial2020,
  title = {Why General Artificial Intelligence Will Not Be Realized},
  author = {Fjelland, Ragnar},
  year = {2020},
  journal = {Humanities and Social Sciences Communications 2020 7:1},
  volume = {7},
  number = {1},
  pages = {1--9},
  issn = {2662-9992},
  doi = {10.1057/s41599-020-0494-4},
  abstract = {The modern project of creating human-like artificial intelligence (AI) started after World War II, when it was discovered that electronic computers are not just number-crunching machines, but can also manipulate symbols. It is possible to pursue this goal without assuming that machine intelligence is identical to human intelligence. This is known as weak AI. However, many AI researcher have pursued the aim of developing artificial intelligence that is in principle identical to human intelligence, called strong AI. Weak AI is less ambitious than strong AI, and therefore less controversial. However, there are important controversies related to weak AI as well. This paper focuses on the distinction between artificial general intelligence (AGI) and artificial narrow intelligence (ANI). Although AGI may be classified as weak AI, it is close to strong AI because one chief characteristics of human intelligence is its generality. Although AGI is less ambitious than strong AI, there were critics almost from the very beginning. One of the leading critics was the philosopher Hubert Dreyfus, who argued that computers, who have no body, no childhood and no cultural practice, could not acquire intelligence at all. One of Dreyfus' main arguments was that human knowledge is partly tacit, and therefore cannot be articulated and incorporated in a computer program. However, today one might argue that new approaches to artificial intelligence research have made his arguments obsolete. Deep learning and Big Data are among the latest approaches, and advocates argue that they will be able to realize AGI. A closer look reveals that although development of artificial intelligence for specific purposes (ANI) has been impressive, we have not come much closer to developing artificial general intelligence (AGI). The article further argues that this is in principle impossible, and it revives Hubert Dreyfus' argument that computers are not in the world.}
}

@techreport{fogelScrabbleGANSemiSupervisedVarying,
  title = {{{ScrabbleGAN}}: {{Semi-Supervised Varying Length Handwritten Text Generation}}},
  author = {Fogel, Sharon and {Averbuch-Elor}, Hadar and Cohen, Sarel and Mazor, Shai and Litman, Roee and Rekognition, Amazon},
  eprint = {2003.10557v1},
  urldate = {2020-10-15},
  abstract = {Optical character recognition (OCR) systems performance have improved significantly in the deep learning era. This is especially true for handwritten text recognition (HTR), where each author has a unique style, unlike printed text, where the variation is smaller by design. That said, deep learning based HTR is limited, as in every other task, by the number of training examples. Gathering data is a challenging and costly task, and even more so, the labeling task that follows, of which we focus here. One possible approach to reduce the burden of data annotation is semi-supervised learning. Semi supervised methods use, in addition to labeled data, some unlabeled samples to improve performance, compared to fully supervised ones. Consequently , such methods may adapt to unseen images during test time. We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten text images that are versatile both in style and lexicon. ScrabbleGAN relies on a novel gener-ative model which can generate images of words with an arbitrary length. We show how to operate our approach in a semi-supervised manner, enjoying the aforementioned benefits such as performance boost over state of the art supervised HTR. Furthermore, our generator can manipulate the resulting text style. This allows us to change, for instance , whether the text is cursive, or how thin is the pen stroke.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\89NQ4HW5\full-text.pdf}
}

@inproceedings{fornesICDAR2017CompetitionInformation2017,
  title = {{{ICDAR2017 Competition}} on {{Information Extraction}} in {{Historical Handwritten Records}}},
  booktitle = {2017 14th {{IAPR International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Forn{\'e}s, Alicia and Romero, Ver{\'o}nica and Bar{\'o}, Arnau and Toledo, Juan Ignacio and S{\'a}nchez, Joan Andreu and Vidal, Enrique and Llad{\'o}s, Josep},
  year = {2017},
  month = nov,
  volume = {01},
  pages = {1389--1394},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2017.227},
  urldate = {2025-06-01},
  abstract = {The extraction of relevant information from historical handwritten document collections is one of the key steps in order to make these manuscripts available for access and searches. In this competition, the goal is to detect the named entities and assign each of them a semantic category, and therefore, to simulate the filling in of a knowledge database. This paper describes the dataset, the tasks, the evaluation metrics, the participants methods and the results.},
  keywords = {Character recognition,Competition,Data models,Databases,Handwriting recognition,Handwriting Recognition,Image segmentation,Information Extraction,Semantic Recognition,Semantics,Training},
  file = {C:\Users\tarchibald\Zotero\storage\GBQQIGST\8270158.html}
}

@misc{fuDreamSimLearningNew2023,
  title = {{{DreamSim}}: {{Learning New Dimensions}} of {{Human Visual Similarity}} Using {{Synthetic Data}}},
  shorttitle = {{{DreamSim}}},
  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},
  year = {2023},
  month = dec,
  number = {arXiv:2306.09344},
  eprint = {2306.09344},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.09344},
  urldate = {2024-02-14},
  abstract = {Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\BUPAYUDM\\Fu et al. - 2023 - DreamSim Learning New Dimensions of Human Visual .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\EVLDQGWP\\2306.html}
}

@article{galDeepBayesianActive2017,
  title = {Deep {{Bayesian Active Learning}} with {{Image Data}}},
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  year = {2017},
  month = mar,
  journal = {34th International Conference on Machine Learning, ICML 2017},
  volume = {3},
  eprint = {1703.02910},
  pages = {1923--1932},
  publisher = {International Machine Learning Society (IMLS)},
  urldate = {2023-08-14},
  abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  archiveprefix = {arXiv},
  isbn = {9781510855144},
  file = {C:\Users\tarchibald\Zotero\storage\7EWY9MBW\full-text.pdf}
}

@inproceedings{galDeepBayesianActive2017a,
  title = {Deep {{Bayesian Active Learning}} with {{Image Data}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  year = {2017},
  month = jul,
  pages = {1183--1192},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-29},
  abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\7L768GVN\Gal et al. - 2017 - Deep Bayesian Active Learning with Image Data.pdf}
}

@inproceedings{gangehEndtoEndUnsupervisedDocument2021,
  title = {End-to-{{End Unsupervised Document Image Blind Denoising}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Gangeh, Mehrdad J. and Plata, Marcin and Nezhad, Hamid R. Motahari and Duffy, Nigel P.},
  year = {2021},
  pages = {7888--7897},
  urldate = {2023-08-16},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\M8LFGQTK\Gangeh et al. - 2021 - End-to-End Unsupervised Document Image Blind Denoi.pdf}
}

@article{gaoConvMAEMaskedConvolution2022,
  title = {{{ConvMAE}}: {{Masked Convolution Meets Masked Autoencoders}}},
  author = {Gao, Peng and Ma, Teli and Li, Hongsheng and Lin, Ziyi and Dai, Jifeng and Qiao, Yu},
  year = {2022},
  month = may,
  eprint = {2205.03892},
  urldate = {2023-04-12},
  abstract = {Vision Transformers (ViT) become widely-adopted architectures for various vision tasks. Masked auto-encoding for feature pretraining and multi-scale hybrid convolution-transformer architectures can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation. In this paper, our ConvMAE framework demonstrates that multi-scale hybrid convolution-transformer can learn more discriminative representations via the mask auto-encoding scheme. However, directly using the original masking strategy leads to the heavy computational cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the masked convolution to prevent information leakage in the convolution blocks. A simple block-wise masking strategy is proposed to ensure computational efficiency. We also propose to more directly supervise the multi-scale features of the encoder to boost multi-scale features. Based on our pretrained ConvMAE models, ConvMAE-Base improves ImageNet-1K finetuning accuracy by 1.4\% compared with MAE-Base. On object detection, ConvMAE-Base finetuned for only 25 epochs surpasses MAE-Base fined-tuned for 100 epochs by 2.9\% box AP and 2.2\% mask AP respectively. Code and pretrained models are available at https://github.com/Alpha-VL/ConvMAE.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\M9I5RDKL\full-text.pdf}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00257-z},
  urldate = {2023-08-24},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning's failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Human behaviour,Information technology,Network models},
  file = {C:\Users\tarchibald\Zotero\storage\G6KZWJZ4\Geirhos et al. - 2020 - Shortcut learning in deep neural networks.pdf}
}

@techreport{GeneratingRealisticBinarization,
  title = {Generating {{Realistic Binarization Data}} with {{Generative Adversarial Networks}}},
  abstract = {One of the limitations for using Deep Learning models to solve binarization tasks is that there is a lack of large quantities of labeled data available to train such models. Efforts to create synthetic data for binarization mostly rely on heuristic image processing techniques and generally lack realism. In this work, we propose a method to produce realistic synthetic data using an adversarially trained image translation model. We extend the popular CycleGAN model to be conditioned on the ground truth binarization mask as it translates images from the domain of synthetic images to the domain of real images. For evaluation, we train deep networks on synthetic datasets produced in different ways and measure their performance on the DIBCO datasets. We show that our proposed CycleGAN-D-GT model produces more realistic synthetic data than other models.},
  keywords = {Binarization,Deep Learning,Generative Adver-sarial Networks,Synthetic Data},
  file = {C:\Users\tarchibald\Zotero\storage\EI7Q9NWQ\Synthetic Compositing - Tensmeyer.pdf}
}

@misc{gholamianHandwrittenPrintedText2023,
  title = {Handwritten and {{Printed Text Segmentation}}: {{A Signature Case Study}}},
  shorttitle = {Handwritten and {{Printed Text Segmentation}}},
  author = {Gholamian, Sina and Vahdat, Ali},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07887},
  eprint = {2307.07887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.07887},
  urldate = {2023-08-18},
  abstract = {While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for handwritten and printed text segmentation task. Our best configuration outperforms the prior work on two different datasets by 17.9\% and 7.3\% on IoU scores.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\LMZNVU9K\\Gholamian and Vahdat - 2023 - Handwritten and Printed Text Segmentation A Signa.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\2VZX3JIR\\2307.html}
}

@misc{gongVisionTransformersPatch2021,
  title = {Vision {{Transformers}} with {{Patch Diversification}}},
  author = {Gong, Chengyue and Wang, Dilin and Li, Meng and Chandra, Vikas and Liu, Qiang},
  year = {2021},
  month = jun,
  number = {arXiv:2104.12753},
  eprint = {2104.12753},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.12753},
  urldate = {2025-05-14},
  abstract = {Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@misc{GPT4VIsionSystem2024,
  title = {{{GPT-4V}}(Ision) System Card},
  year = {2024},
  month = feb,
  urldate = {2025-06-01},
  howpublished = {https://openai.com/index/gpt-4v-system-card/},
  langid = {american},
  file = {C:\Users\tarchibald\Zotero\storage\LYQXGWJD\gpt-4v-system-card.html}
}

@article{grathwohlYOURCLASSIFIERSECRETLY,
  title = {{{YOUR CLASSIFIER IS SECRETLY AN ENERGY BASED MODEL AND YOU SHOULD TREAT IT LIKE ONE}}},
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\"o}rn-Henrik and Duvenaud, David and Swersky, Kevin and Norouzi, Mohammad},
  eprint = {1912.03263v3},
  urldate = {2022-03-18},
  abstract = {We propose to reinterpret a standard discriminative classifier of p(y{\textbar}x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x{\textbar}y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is able to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\662PXDRV\full-text.pdf}
}

@article{Graves2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2013},
  month = aug,
  journal = {arXiv preprint arXiv:1308.0850},
  eprint = {1308.0850},
  urldate = {2019-08-26},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\BK8YE48N\Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf}
}

@article{Graves2016,
  title = {Stochastic {{Backpropagation}} through {{Mixture Density Distributions}}},
  author = {Graves, Alex},
  year = {2016},
  month = jul,
  eprint = {1607.05690},
  urldate = {2020-05-15},
  abstract = {The ability to backpropagate stochastic gradients through continuous latent distributions has been crucial to the emergence of variational autoencoders and stochastic gradient variational Bayes. The key ingredient is an unbiased and low-variance way of estimating gradients with respect to distribution parameters from gradients evaluated at distribution samples. The "reparameterization trick" provides a class of transforms yielding such estimators for many continuous distributions, including the Gaussian and other members of the location-scale family. However the trick does not readily extend to mixture density models, due to the difficulty of reparameterizing the discrete distribution over mixture weights. This report describes an alternative transform, applicable to any continuous multivariate distribution with a differentiable density function from which samples can be drawn, and uses it to derive an unbiased estimator for mixture density weight derivatives. Combined with the reparameterization trick applied to the individual mixture components, this estimator makes it straightforward to train variational autoencoders with mixture-distributed latent variables, or to perform stochastic variational inference with a mixture density variational posterior.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\8TBKBCC6\full-text.pdf}
}

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist {{Temporal Classification}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}},
  shorttitle = {Connectionist {{Temporal Classification}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  series = {{{ICML}} '06},
  pages = {369--376},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1143844.1143891},
  urldate = {2018-11-20},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  isbn = {978-1-59593-383-6}
}

@article{gravesNeuralTuringMachines2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  eprint = {1410.5401},
  urldate = {2019-12-05},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\QKWIRWT6\full-text.pdf}
}

@inproceedings{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent a New Approach to Self-Supervised Learning},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = dec,
  series = {{{NIPS}} '20},
  pages = {21271--21284},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-05-22},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6\% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  isbn = {978-1-7138-2954-6},
  file = {C:\Users\tarchibald\Zotero\storage\Y9GQJ2CI\Grill et al. - 2020 - Bootstrap your own latent a new approach to self-s.pdf}
}

@misc{groleauShabbyPagesReproducibleDocument2023,
  title = {{{ShabbyPages}}: {{A Reproducible Document Denoising}} and {{Binarization Dataset}}},
  shorttitle = {{{ShabbyPages}}},
  author = {Groleau, Alexander and Chee, Kok Wei and Larson, Stefan and Maini, Samay and Boarman, Jonathan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09339},
  eprint = {2303.09339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09339},
  urldate = {2023-08-16},
  abstract = {Document denoising and binarization are fundamental problems in the document processing space, but current datasets are often too small and lack sufficient complexity to effectively train and benchmark modern data-driven machine learning models. To fill this gap, we introduce ShabbyPages, a new document image dataset designed for training and benchmarking document denoisers and binarizers. ShabbyPages contains over 6,000 clean "born digital" images with synthetically-noised counterparts ("shabby pages") that were augmented using the Augraphy document augmentation tool to appear as if they have been printed and faxed, photocopied, or otherwise altered through physical processes. In this paper, we discuss the creation process of ShabbyPages and demonstrate the utility of ShabbyPages by training convolutional denoisers which remove real noise features with a high degree of human-perceptible fidelity, establishing baseline performance for a new ShabbyPages benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\JNGSYWHI\\Groleau et al. - 2023 - ShabbyPages A Reproducible Document Denoising and.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\Y98YLMH8\\2303.html}
}

@article{gruningTwostageMethodText2019,
  title = {A Two-Stage Method for Text Line Detection in Historical Documents},
  author = {Gr{\"u}ning, Tobias and Leifert, Gundram and Strau{\ss}, Tobias and Michael, Johannes and Labahn, Roger},
  year = {2019},
  month = sep,
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  volume = {22},
  number = {3},
  pages = {285--302},
  issn = {1433-2825},
  doi = {10.1007/s10032-019-00332-1},
  urldate = {2023-08-16},
  abstract = {This work presents a two-stage text line detection method for historical documents. Each detected text line is represented by its baseline. In a first stage, a deep neural network called ARU-Net labels pixels to belong to one of the three classes: baseline, separator and other. The separator class marks beginning and end of each text line. The ARU-Net is trainable from scratch with manageably few manually annotated example images (\$\${$<\backslash$},50\$\$). This is achieved by utilizing data augmentation strategies. The network predictions are used as input for the second stage which performs a bottom-up clustering to build baselines. The developed method is capable of handling complex layouts as well as curved and arbitrarily oriented text lines. It substantially outperforms current state-of-the-art approaches. For example, for the complex track of the cBAD: ICDAR2017 Competition on Baseline Detection the F value is increased from 0.859 to 0.922. The framework to train and run the ARU-Net is open source.},
  langid = {english},
  keywords = {Baseline detection,Historical documents,Layout analysis,Pixel labeling,Text line detection,U-Net},
  file = {C:\Users\tarchibald\Zotero\storage\JZ94YH7F\Grning et al. - 2019 - A two-stage method for text line detection in hist.pdf}
}

@techreport{Gui,
  title = {{{ADAPTIVE CONTEXT-AWARE REINFORCED AGENT FOR HANDWRITTEN TEXT RECOG}} 1 {{Adaptive Context-aware Reinforced Agent}} for {{Handwritten Text Recognition}}},
  author = {Gui, Liangke and Liang, Xiaodan and Chang, Xiaojun and Hauptmann, Alexander G},
  urldate = {2019-09-24},
  abstract = {Handwritten text recognition has been a ubiquitous research problem in the field of computer vision. Most existing approaches focus on the recognition of handwritten words without considering the cursive nature and significant differences in the writing of individuals. In this paper, we address these problems by leveraging an adaptive context-aware reinforced agent which learns the actions to determine the scales of context regions during inference. We formulate our approach in a reinforcement learning framework. Specifically, we construct the action set with a number of context lengths. Given an image feature sequence, our model is trained to adaptively choose the optimal context length according to the observed state. An attention mechanism is then used to selectively attend the context region. Our model can generalize well from recognizing isolated words to recognizing individual lines of text while remain low computation overheads. We conduct extensive experiments on three large-scale handwritten text recognition datasets. The experimental results show that our proposed model is superior to the state-of-the-art alternatives.},
  file = {C:\Users\tarchibald\Zotero\storage\RBUBBA9A\full-text.pdf}
}

@misc{Gulcehre2015,
  title = {On {{Using Monolingual Corpora}} in {{Neural Machine Translation}}},
  author = {Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua and Cho, Kyunghyun and Barrault, Loic and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2015},
  month = mar,
  urldate = {2019-08-28},
  isbn = {1503.03535v2},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\5KRNX2PM\\Gulcehre et al. - 2015 - On Using Monolingual Corpora in Neural Machine Translation.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\W9HM6H3R\\Gulcehre et al. - Unknown - On Using Monolingual Corpora in Neural Machine Translation.pdf}
}

@misc{gutteridgeJudgeBookIts2025,
  title = {Judge a {{Book}} by Its {{Cover}}: {{Investigating Multi-Modal LLMs}} for {{Multi-Page Handwritten Document Transcription}}},
  shorttitle = {Judge a {{Book}} by Its {{Cover}}},
  author = {Gutteridge, Benjamin and Jackson, Matthew Thomas and Kukurin, Toni and Dong, Xiaowen},
  year = {2025},
  month = feb,
  number = {arXiv:2502.20295},
  eprint = {2502.20295},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.20295},
  urldate = {2025-06-05},
  abstract = {Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, '+first page', which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that '+first page' improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\VYS2DCWF\\Gutteridge et al. - 2025 - Judge a Book by its Cover Investigating Multi-Mod.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\T5S7AYLE\\2502.html}
}

@article{hahnTabulaNearlyRasa,
  title = {Tabula Nearly Rasa: {{Probing}} the Linguistic Knowledge of Character-Level Neural Language Models Trained on Unsegmented Text},
  author = {Hahn, Michael and Baroni, Marco},
  eprint = {1906.07285v1},
  urldate = {2021-09-09},
  abstract = {Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multilingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our "near tabula rasa" RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit , rigid word lexicon in language learning and usage.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\2DD4CICL\full-text.pdf}
}

@article{hainesMyTextYour2016,
  title = {My {{Text}} in {{Your Handwriting}}},
  author = {Haines, Tom S. F. and Mac Aodha, Oisin and Brostow, Gabriel J.},
  year = {2016},
  month = may,
  journal = {ACM Trans. Graph.},
  volume = {35},
  number = {3},
  pages = {26:1--26:18},
  issn = {0730-0301},
  doi = {10.1145/2886099},
  urldate = {2018-11-20},
  abstract = {There are many scenarios where we wish to imitate a specific author's pen-on-paper handwriting style. Rendering new text in someone's handwriting is difficult because natural handwriting is highly variable, yet follows both intentional and involuntary structure that makes a person's style self-consistent. The variability means that naive example-based texture synthesis can be conspicuously repetitive. We propose an algorithm that renders a desired input string in an author's handwriting. An annotated sample of the author's handwriting is required; the system is flexible enough that historical documents can usually be used with only a little extra effort. Experiments show that our glyph-centric approach, with learned parameters for spacing, line thickness, and pressure, produces novel images of handwriting that look hand-made to casual observers, even when printed on paper.},
  keywords = {generative models,handwriting,Texture synthesis}
}

@inproceedings{hamdaniCombiningMultipleHMMs2009,
  title = {Combining Multiple {{HMMs}} Using On-Line and off-Line Features for off-Line Arabic Handwriting Recognition},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Hamdani, Mahdi and El Abed, Haikal and Kherallah, Monji and Alimi, Adel M.},
  year = {2009},
  pages = {201--205},
  issn = {15205363},
  doi = {10.1109/ICDAR.2009.40},
  urldate = {2020-03-20},
  abstract = {This paper presents an off-line Arabic Handwriting recognition system based on the selection of different state of the art features and the combination of multiple Hidden Markov Models classifiers. Beside the classical use of the off-line features, we add the use of on-line features and the combination of the developed systems. The designed recognizer is implemented using the HMM-Toolkit. In a first step, we use different features to make the classification and we compare the performance of single classifiers. In a second step, we proceed to the combination of the on-line and the off-line based systems using different combination methods. The system is evaluated using the IFN/ENIT database. The recognition rate is in maximum 63.90\% for the individual systems. The combination of the on-line and the off-line systems allows to improve the system accuracy to 81.93\% which exceeds the best result of the ICDAR 2005 competition. {\copyright} 2009 IEEE.},
  isbn = {978-0-7695-3725-2},
  file = {C:\Users\tarchibald\Zotero\storage\5NVKRXTC\full-text.pdf}
}

@inproceedings{hamdaniCombiningMultipleHMMs2009a,
  title = {Combining Multiple {{HMMs}} Using On-Line and off-Line Features for off-Line Arabic Handwriting Recognition},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Hamdani, Mahdi and El Abed, Haikal and Kherallah, Monji and Alimi, Adel M.},
  year = {2009},
  pages = {201--205},
  issn = {15205363},
  doi = {10.1109/ICDAR.2009.40},
  urldate = {2020-07-27},
  abstract = {This paper presents an off-line Arabic Handwriting recognition system based on the selection of different state of the art features and the combination of multiple Hidden Markov Models classifiers. Beside the classical use of the off-line features, we add the use of on-line features and the combination of the developed systems. The designed recognizer is implemented using the HMM-Toolkit. In a first step, we use different features to make the classification and we compare the performance of single classifiers. In a second step, we proceed to the combination of the on-line and the off-line based systems using different combination methods. The system is evaluated using the IFN/ENIT database. The recognition rate is in maximum 63.90\% for the individual systems. The combination of the on-line and the off-line systems allows to improve the system accuracy to 81.93\% which exceeds the best result of the ICDAR 2005 competition. {\copyright} 2009 IEEE.},
  isbn = {978-0-7695-3725-2},
  file = {C:\Users\tarchibald\Zotero\storage\PB4GF7IF\full-text.pdf}
}

@misc{HandwrittenTextRecognition,
  title = {Handwritten Text Recognition: From Isolated Text Lines to Whole Documents - {{Archive}} Ouverte {{HAL}}},
  urldate = {2022-03-24},
  howpublished = {https://hal.archives-ouvertes.fr/hal-03339648/}
}

@article{hanifAutonomousCharacterRegion,
  title = {Autonomous {{Character Region Score Fusion}} for {{Word Detection}} in {{Camera-captured Handwriting Documents}}},
  author = {Hanif, Sidra and Jan Latecki, Longin},
  urldate = {2022-10-04},
  abstract = {Word detection is considered an object detection problem. However, characters are the basic building block in words, and the presence of characters makes word detection different from general object detection problems. Character region scores identification performs consistently for handwritten text in low-contrast camera-captured images, But detecting words from characters poses a challenge because of variable character spacing in words. Nevertheless, considering the only character and ignoring a word's entirety does not cope with overlapping words in handwriting text. In our work, we propose the fusion of character region scores with word detection. Since the character level annotations are not available for handwritten text, we estimate the character region scores in a weakly supervised manner. Character region scores are estimated autonomously from the word's bounding box estimation to learn the character level information in handwriting. We propose to fuse the character region scores and images to detect words in camera-captured handwriting images. Fusion of character region score with image has a higher recall of 88.4(+1.2) and outperforms the state of the state-of-the-art object detector with 92.2(+0.4) mAP@0.5 and 64.0(+0.4) mAP@0.5:0.95. The code and trained models are shared at the link: http://github.com/sidrahhanif/KDD-DI-Word\_detection-2022.},
  keywords = {Camera-captured images,Character scores,Handwritten text,Multi-channel input,Object detection},
  file = {C:\Users\tarchibald\Zotero\storage\VY2FGY7Q\full-text.pdf}
}

@misc{harleyEvaluationDeepConvolutional2015,
  title = {Evaluation of {{Deep Convolutional Nets}} for {{Document Image Classification}} and {{Retrieval}}},
  author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
  year = {2015},
  month = feb,
  number = {arXiv:1502.07058},
  eprint = {1502.07058},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.07058},
  urldate = {2023-08-23},
  abstract = {This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\BVT3E9XE\\Harley et al. - 2015 - Evaluation of Deep Convolutional Nets for Document.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\INFZPD3V\\1502.html}
}

@article{hartSelectionConvConvolutionalNeural2022,
  title = {{{SelectionConv}}: {{Convolutional Neural Networks}} for {{Non-rectilinear Image Data}}},
  author = {Hart, David and Whitney, Michael and Morse, Bryan},
  year = {2022},
  month = jul,
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {13667 LNCS},
  eprint = {2207.08979},
  pages = {317--333},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  issn = {16113349},
  doi = {10.1007/978-3-031-20071-7_19},
  urldate = {2023-04-12},
  abstract = {Convolutional Neural Networks have revolutionized vision applications. There are image domains and representations, however, that cannot be handled by standard CNNs (e.g., spherical images, superpixels). Such data are usually processed using networks and algorithms specialized for each type. In this work, we show that it may not always be necessary to use specialized neural networks to operate on such spaces. Instead, we introduce a new structured graph convolution operator that can copy 2D convolution weights, transferring the capabilities of already trained traditional CNNs to our new graph network. This network can then operate on any data that can be represented as a positional graph. By converting non-rectilinear data to a graph, we can apply these convolutions on these irregular image domains without requiring training on large domain-specific datasets. Results of transferring pre-trained image networks for segmentation, stylization, and depth prediction are demonstrated for a variety of such data forms.},
  archiveprefix = {arXiv},
  isbn = {9783031200700},
  keywords = {Graph convolution,Irregular images,Spherical images,Superpixels,Texture maps,Transfer learning}
}

@inproceedings{hassaineICDAR2013Competition2013,
  title = {{{ICDAR}} 2013 Competition on Handwriting Stroke Recovery from Offline Data},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Hassaine, Abdelaali and Al Maadeed, Somaya and Bouridane, Ahmed},
  year = {2013},
  pages = {1412--1416},
  issn = {15205363},
  doi = {10.1109/ICDAR.2013.285},
  urldate = {2020-02-28},
  abstract = {Stroke recovery from offline handwriting is a very interesting research field. However, no standard benchmark is available for researchers in this field. The aim of this competition is to gather researchers and compare recent advances in stroke recovery from offline handwriting. This competition has been hosted on Kaggle, it has attracted 45 teams from both academia and industry. This paper gives details on this competition, including the dataset used, the evaluation procedure and description of participating methods and their performances. {\copyright} 2013 IEEE.},
  file = {C:\Users\tarchibald\Zotero\storage\LBUJHWT2\full-text.pdf}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2024-02-17},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\9GMTABKJ\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\UGUKCTAF\\1512.html}
}

@article{heMaskedAutoencodersAre2021,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollar, Piotr and Girshick, Ross},
  year = {2021},
  month = nov,
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume = {2022-June},
  eprint = {2111.06377},
  pages = {15979--15988},
  publisher = {IEEE Computer Society},
  issn = {10636919},
  doi = {10.1109/CVPR52688.2022.01553},
  urldate = {2023-04-12},
  abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
  archiveprefix = {arXiv},
  isbn = {9781665469463},
  keywords = {Representation learning,Self- & semi- & meta- & unsupervised learning},
  file = {C:\Users\tarchibald\Zotero\storage\YLMLAQSI\full-text.pdf}
}

@inproceedings{hemmerConfidenceAwareDocumentOCR2024,
  title = {Confidence-{{Aware Document OCR Error Detection}}},
  booktitle = {Document {{Analysis Systems}}: 16th {{IAPR International Workshop}}, {{DAS}} 2024, {{Athens}}, {{Greece}}, {{August}} 30--31, 2024, {{Proceedings}}},
  author = {Hemmer, Arthur and Coustaty, Micka{\"e}l and Bartolo, Nicola and Ogier, Jean-Marc},
  year = {2024},
  month = sep,
  pages = {213--228},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-70442-0_13},
  urldate = {2025-07-07},
  abstract = {Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.},
  isbn = {978-3-031-70441-3}
}

@misc{hemmerConfidenceAwareDocumentOCR2024preprint,
  title = {Confidence-{{Aware Document OCR Error Detection}}},
  author = {Hemmer, Arthur and Coustaty, Micka{\"e}l and Bartolo, Nicola and Ogier, Jean-Marc},
  year = {2024},
  month = sep,
  number = {arXiv:2409.04117},
  eprint = {2409.04117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.04117},
  urldate = {2025-06-04},
  abstract = {Optical Character Recognition (OCR) continues to face accuracy challenges that impact subsequent applications. To address these errors, we explore the utility of OCR confidence scores for enhancing post-OCR error detection. Our study involves analyzing the correlation between confidence scores and error rates across different OCR systems. We develop ConfBERT, a BERT-based model that incorporates OCR confidence scores into token embeddings and offers an optional pre-training phase for noise adjustment. Our experimental results demonstrate that integrating OCR confidence scores can enhance error detection capabilities. This work underscores the importance of OCR confidence scores in improving detection accuracy and reveals substantial disparities in performance between commercial and open-source OCR technologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\EQKDSQ2D\\Hemmer et al. - 2024 - Confidence-Aware Document OCR Error Detection.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\S3RZCCPE\\2409.html}
}

@article{hernandezdiazgoogleresearchRethinkingTextLine2021,
  title = {Rethinking {{Text Line Recognition Models}}},
  author = {Hernandez Diaz Google Research, Daniel and Qin Google Research, Siyang and Ingle Google Research, Reeve and Fujii Google Research, Yasuhisa and Bissacco Google Research, Alessandro},
  year = {2021},
  month = apr,
  eprint = {2104.07787},
  doi = {10.48550/arxiv.2104.07787},
  urldate = {2022-03-24},
  abstract = {In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\2778VK3Y\full-text.pdf}
}

@article{hintonHowRepresentPartwhole2021,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  year = {2021},
  eprint = {2102.12627v1},
  urldate = {2021-09-09},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM 1. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\YIWLWHGL\full-text.pdf}
}

@article{hintonHowRepresentPartwhole2021a,
  title = {How to Represent Part-Whole Hierarchies in a Neural Network},
  author = {Hinton, Geoffrey},
  year = {2021},
  month = feb,
  eprint = {2102.12627},
  doi = {10.48550/arxiv.2102.12627},
  urldate = {2022-03-16},
  abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\HDBZJ2NT\full-text.pdf}
}

@techreport{Hochreiter1997,
  title = {{{LSTM Long Short Term Memory}}},
  author = {Hochreiter, Sepp and Urgen Schmidhuber, J J and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {MEMORY Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  institution = {MIT Press},
  urldate = {2019-10-10},
  abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called {\textbackslash}Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through {\textbackslash}constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {C:\Users\tarchibald\Zotero\storage\H6YYH9L4\Hochreiter, Urgen Schmidhuber - 1997 - LSTM Long Short Term Memory.pdf}
}

@article{hochreiter1997long,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT Press}
}

@incollection{hoeffdingProbabilityInequalitiesSums1994,
  title = {Probability {{Inequalities}} for Sums of {{Bounded Random Variables}}},
  booktitle = {The {{Collected Works}} of {{Wassily Hoeffding}}},
  author = {Hoeffding, Wassily},
  editor = {Fisher, N. I. and Sen, P. K.},
  year = {1994},
  pages = {409--426},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-0865-5_26},
  urldate = {2025-06-05},
  abstract = {Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for PrS --- ES{$\geq$}nt depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.},
  isbn = {978-1-4612-0865-5},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\DDXR46BR\Hoeffding - 1994 - Probability Inequalities for sums of Bounded Rando.pdf}
}

@article{hronVariationalGaussianDropout2017,
  title = {Variational {{Gaussian Dropout}} Is Not {{Bayesian}}},
  author = {Hron, Jiri and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
  year = {2017},
  month = nov,
  eprint = {1711.02989},
  urldate = {2021-10-21},
  abstract = {Gaussian multiplicative noise is commonly used as a stochastic regularisation technique in training of deterministic neural networks. A recent paper reinterpreted the technique as a specific algorithm for approximate inference in Bayesian neural networks; several extensions ensued. We show that the log-uniform prior used in all the above publications does not generally induce a proper posterior, and thus Bayesian inference in such models is ill-posed. Independent of the log-uniform prior, the correlated weight noise approximation has further issues leading to either infinite objective or high risk of overfitting. The above implies that the reported sparsity of obtained solutions cannot be explained by Bayesian or the related minimum description length arguments. We thus study the objective from a non-Bayesian perspective, provide its previously unknown analytical form which allows exact gradient evaluation, and show that the later proposed additive reparametrisation introduces minima not present in the original multiplicative parametrisation. Implications and future research directions are discussed.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\K9UNI2C4\full-text.pdf}
}

@techreport{Huang,
  title = {Arbitrary {{Style Transfer}} in {{Real-time}} with {{Adaptive Instance Normalization}}},
  author = {Huang, Xun and Belongie, Serge},
  eprint = {1703.06868v2},
  urldate = {2019-10-01},
  abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normaliza-tion (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition , our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\MSWR58Z7\full-text.pdf}
}

@article{huangLayoutLMv3PretrainingDocument2022,
  title = {{{LayoutLMv3}}: {{Pre-training}} for {{Document AI}} with {{Unified Text}} and {{Image Masking}}},
  author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  year = {2022},
  month = apr,
  eprint = {2204.08387},
  doi = {10.48550/arxiv.2204.08387},
  urldate = {2022-05-26},
  abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at https://aka.ms/layoutlmv3.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\SM98GUDY\full-text.pdf}
}

@article{hullDatabaseHandwrittenText1994,
  title = {A Database for Handwritten Text Recognition Research},
  author = {Hull, J.J.},
  year = {1994},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {5},
  pages = {550--554},
  issn = {1939-3539},
  doi = {10.1109/34.291440},
  urldate = {2024-02-19},
  abstract = {An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons.{$<>$}},
  keywords = {CEDAR,Cities and towns,Digital images,Gray-scale,Handwriting recognition,Image databases,Performance analysis,Postal services,Testing,Text recognition,Writing},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\EB6AET4T\\Hull - 1994 - A database for handwritten text recognition resear.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\JK5D2TZA\\291440.html}
}

@misc{IEEEXploreFullText,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2020-03-15},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=620553}
}

@misc{IEEEXploreFullTexta,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2020-03-15},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=1575507}
}

@misc{IEEEXploreFullTextb,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2020-07-26},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=5277568}
}

@misc{IEEEXploreFullTextc,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2020-07-26},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=5277731}
}

@misc{IEEEXploreFullTextd,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2022-09-28},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=9366523}
}

@misc{IEEEXploreFullTexte,
  title = {{{IEEE Xplore Full-Text PDF}}:},
  urldate = {2024-02-01},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=888707}
}

@article{ImportedHttpsWww,
  title = {Imported from {{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7456511/}}}
}

@misc{ingleScalableHandwrittenText2019,
  title = {A {{Scalable Handwritten Text Recognition System}}},
  author = {Ingle, Reeve and Fujii, Yasuhisa and Deselaers, Thomas and Baccash, Jonathan Michael and Popat, Ashok},
  year = {2019},
  urldate = {2020-04-15}
}

@misc{IntroducingClaude3,
  title = {Introducing the next Generation of {{Claude}}},
  urldate = {2025-06-01},
  abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
  howpublished = {https://www.anthropic.com/news/claude-3-family},
  langid = {english}
}

@misc{IntroducingClaude4,
  title = {Introducing {{Claude}} 4},
  urldate = {2025-06-01},
  abstract = {Discover Claude 4's breakthrough AI capabilities. Experience more reliable, interpretable assistance for complex tasks across work and learning.},
  howpublished = {https://www.anthropic.com/news/claude-4},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\MPT4D45U\claude-4.html}
}

@article{isolaImagetoImageTranslationConditional2016,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2016},
  month = nov,
  journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume = {2017-January},
  eprint = {1611.07004},
  pages = {5967--5976},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2020-10-28},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\LQ8C3GWR\full-text.pdf}
}

@techreport{jaderbergDecoupledNeuralInterfaces2017,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  year = {2017},
  month = jul,
  pages = {1627--1635},
  institution = {PMLR},
  issn = {2640-3498},
  urldate = {2020-12-24},
  abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers , or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropa-gated error gradients we decouple subgraphs, and can update them independently and asyn-chronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models , where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass-amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  file = {C:\Users\tarchibald\Zotero\storage\4L36QV33\full-text.pdf}
}

@article{jaderbergSpatialTransformerNetworks2015,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2015},
  month = jun,
  journal = {Advances in Neural Information Processing Systems},
  volume = {2015-January},
  eprint = {1506.02025},
  pages = {2017--2025},
  publisher = {Neural information processing systems foundation},
  issn = {10495258},
  urldate = {2023-04-12},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\KSWVJVNR\full-text.pdf}
}

@inproceedings{jadhavPix2PixGenerativeAdversarial2022,
  title = {{{Pix2Pix Generative Adversarial Network}} with {{ResNet}} for {{Document Image Denoising}}},
  booktitle = {2022 4th {{International Conference}} on {{Inventive Research}} in {{Computing Applications}} ({{ICIRCA}})},
  author = {Jadhav, Pranjal and Sawal, Mayuree and Zagade, Anushka and Kamble, Prerna and Deshpande, Prajakta},
  year = {2022},
  month = sep,
  pages = {1489--1494},
  doi = {10.1109/ICIRCA54612.2022.9985695},
  abstract = {Noise degrades quality of scanned document images and adversely affects the accuracy of document digitization and text extraction tasks such as optical character recognition (OCR). Denoising and quality enhancement is the preprocessing stage of the processing pipeline in OCR. This research work proposes an effective end-to-end framework that uses the pre-trained pix2pix Generative Adversarial Network (GAN) to denoise degraded electronic document images. To increase the capacity of the generator network, a variation of the baseline model is developed by replacing the U-net architecture with ResNet6. Along with the discriminator patchGAN in pix2pix model, a pipeline has been developed to extract the patches from input images, predict clean patches using the trained model and finally merge the output patches smoothly. For training, a noisy scanned document dataset created by synthetically adding noises has been utilized to clean the images. Finally, the proposed model is tested by performing quantitative analysis based on different metrics - Structural Similarity Index Measure (SSIM) and Peak Signal to Noise Ratio (PSNR) as well as qualitative analysis by using OCR test on test dataset and real-time documents.},
  keywords = {conditional-Generative Adversarial Network,Deep learning,Document Image Denoising,Generative adversarial networks,Generators,Optical character recognition,Optical Character Recognition,Pipelines,pix2pix,Predictive models,PSNR,ResNet,Training},
  file = {C:\Users\tarchibald\Zotero\storage\LZTKLBP6\Jadhav et al. - 2022 - Pix2Pix Generative Adversarial Network with ResNet.pdf}
}

@article{Jain2002,
  title = {On-Line Signature Verification},
  author = {Jain, Anil K. and Griess, Friederike D. and Connell, Scott D.},
  year = {2002},
  month = dec,
  journal = {Pattern Recognition},
  volume = {35},
  number = {12},
  pages = {2963--2972},
  publisher = {Pergamon},
  doi = {10.1016/S0031-3203(01)00240-0},
  urldate = {2019-10-12},
  keywords = {Biometric authentication,Feature detection,On-line signatures,String matching,Template,Verification,Writer-dependent threshold},
  file = {C:\Users\tarchibald\Zotero\storage\44MXRBE5\Jain, Griess, Connell - 2002 - On-line signature verification.pdf}
}

@article{jaumeFUNSDDatasetForm2019,
  title = {{{FUNSD}}: {{A Dataset}} for {{Form Understanding}} in {{Noisy Scanned Documents}}},
  author = {Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
  year = {2019},
  month = may,
  eprint = {1905.13538},
  pages = {1--6},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.48550/arxiv.1905.13538},
  urldate = {2022-06-04},
  abstract = {We present a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms. The dataset comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. To the best of our knowledge, this is the first publicly available dataset with comprehensive annotations to address FoUn task. We also present a set of baselines and introduce metrics to evaluate performance on the FUNSD dataset, which can be downloaded at https://guillaumejaume.github.io/FUNSD/.},
  archiveprefix = {arXiv},
  keywords = {Form Understanding,Optical Character Recognition,Spatial Layout Analysis,Text detection},
  file = {C:\Users\tarchibald\Zotero\storage\MVVDXC7F\full-text.pdf}
}

@techreport{Ji2019,
  title = {Generative {{Adversarial Network}} for {{Handwritten Text}}},
  author = {Ji, Bo and Chen, Tianyi},
  year = {2019},
  eprint = {1907.11845v2},
  urldate = {2019-10-05},
  abstract = {Generative adversarial networks (GANs) have proven hugely successful in variety of applications of image processing. However, generative adversarial networks for handwriting is relatively rare somehow because of difficulty of handling sequential handwriting data by Convolutional Neural Network (CNN). In this paper, we propose a handwriting generative adversarial network framework (HWGANs) for synthesising handwritten stroke data. The main features of the new framework include: (i) A discriminator consists of an integrated CNN-Long-Short-Term-Memory (LSTM) based feature extraction with Path Signature Features (PSF) as input and a Feedforward Neural Network (FNN) based binary classifier; (ii) A recurrent latent variable model as generator for synthesizing sequential handwritten data. The numerical experiments show the effectivity of the new model. Moreover, comparing with sole handwriting generator, the HWGANs synthesize more natural and realistic handwritten text.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\5TJD6XSB\full-text.pdf}
}

@article{jiaoSurveyDeepLearningbased2019,
  title = {A {{Survey}} of {{Deep Learning-based Object Detection}}},
  author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
  year = {2019},
  month = jul,
  journal = {IEEE Access},
  volume = {7},
  eprint = {1907.09408},
  pages = {128837--128868},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/ACCESS.2019.2939201},
  urldate = {2020-09-22},
  abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in peoples life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning networks for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline, thoroughly and deeply, in this survey, we first analyze the methods of existing typical detection models and describe the benchmark datasets. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efficient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
  archiveprefix = {arXiv},
  keywords = {Classification,deep learning,localization,object detection,typical pipelines},
  file = {C:\Users\tarchibald\Zotero\storage\9QDP4ZGB\full-text.pdf}
}

@article{jiGenerativeAdversarialNetwork2019,
  title = {Generative {{Adversarial Network}} for {{Handwritten Text}}},
  author = {Ji, Bo and Chen, Tianyi},
  year = {2019},
  month = jul,
  eprint = {1907.11845},
  urldate = {2020-02-27},
  abstract = {Generative adversarial networks (GANs) have proven hugely successful in variety of applications of image processing. However, generative adversarial networks for handwriting is relatively rare somehow because of difficulty of handling sequential handwriting data by Convolutional Neural Network (CNN). In this paper, we propose a handwriting generative adversarial network framework (HWGANs) for synthesizing handwritten stroke data. The main features of the new framework include: (i) A discriminator consists of an integrated CNN-Long-Short-Term- Memory (LSTM) based feature extraction with Path Signature Features (PSF) as input and a Feedforward Neural Network (FNN) based binary classifier; (ii) A recurrent latent variable model as generator for synthesizing sequential handwritten data. The numerical experiments show the effectivity of the new model. Moreover, comparing with sole handwriting generator, the HWGANs synthesize more natural and realistic handwritten text.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\MB8SYMBM\full-text.pdf}
}

@article{joHandwrittenTextSegmentation2020,
  title = {Handwritten {{Text Segmentation}} via {{End-to-End Learning}} of {{Convolutional Neural Networks}}},
  author = {Jo, Junho and Koo, Hyung Il and Soh, Jae Woong and Cho, Nam Ik},
  year = {2020},
  month = nov,
  journal = {Multimedia Tools and Applications},
  volume = {79},
  number = {43},
  pages = {32137--32150},
  issn = {1573-7721},
  doi = {10.1007/s11042-020-09624-9},
  urldate = {2023-08-18},
  abstract = {We present a method that separates handwritten and machine-printed components that are mixed and overlapped in documents. Many conventional methods addressed this problem by extracting connected components (CCs) and classifying the extracted CCs into two classes. They were based on the assumption that two types of components are not overlapping each other, while we are focusing on more challenging and realistic cases where the components are often overlapping each other. For this, we propose a new method that performs pixel-level classification with a convolutional neural network. Unlike conventional neural network methods, our method works in an end-to-end manner and does not require any preprocessing steps (e.g., foreground extraction, handcrafted feature extraction, and so on). For the training of our network, we develop a cross-entropy based loss function to alleviate the class imbalance problem. Regarding the training dataset, although there are some datasets of mixed printed characters and handwritten scripts, most of them do not have overlapping cases and do not provide pixel-level annotations. Hence, we also propose a data synthesis method that generates realistic pixel-level training samples having many overlappings of printed and handwritten components. Experimental results on synthetic and real images have shown the effectiveness of the proposed method. Although the proposed network has been trained only with synthetic images, it also improves the OCR rate of real documents. Specifically, the OCR rate for machine-printed texts is increased from 0.8087 to 0.9442 by removing the overlapped handwritten scribbles by our method.},
  langid = {english},
  keywords = {Class imbalance problem,Data synthesis,Handwritten text segmentation,Optical character recognition,Text separation},
  annotation = {https://github.com/jottue/HTSNet},
  file = {C:\Users\tarchibald\Zotero\storage\K9IIG7BT\Jo et al. - 2020 - Handwritten Text Segmentation via End-to-End Learn.pdf}
}

@misc{joHTSNet2023,
  title = {{{HTSNet}}},
  author = {Jo, Junho},
  year = {2023},
  month = jun,
  urldate = {2023-08-23}
}

@techreport{Jozefowicz,
  title = {Exploring the {{Limits}} of {{Language Modeling}}},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui and Brain, Google},
  eprint = {1602.02410v2},
  urldate = {2019-09-18},
  abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplex-ity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\KLJCNBRD\full-text.pdf}
}

@article{jyothiLayoutVAEStochasticScene2019,
  title = {{{LayoutVAE}}: {{Stochastic Scene Layout Generation From}} a {{Label Set}}},
  author = {Jyothi, Akash Abdu and Durand, Thibaut and He, Jiawei and Sigal, Leonid and Mori, Greg},
  year = {2019},
  month = jul,
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2019-October},
  eprint = {1907.10719},
  pages = {9894--9903},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10.1109/ICCV.2019.00999},
  urldate = {2023-04-12},
  abstract = {Recently there is an increasing interest in scene generation within the research community. However, models used for generating scene layouts from textual description largely ignore plausible visual variations within the structure dictated by the text. We propose LayoutVAE, a variational autoencoder based framework for generating stochastic scene layouts. LayoutVAE is a versatile modeling framework that allows for generating full image layouts given a label set, or per label layouts for an existing image given a new label. In addition, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem. Extensive experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset verifies the effectiveness of our proposed framework.},
  archiveprefix = {arXiv},
  isbn = {9781728148038},
  file = {C:\Users\tarchibald\Zotero\storage\RSJQSLRQ\full-text.pdf}
}

@misc{kadavathLanguageModelsMostly2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and {Hatfield-Dodds}, Zac and DasSarma, Nova and {Tran-Johnson}, Eli and Johnston, Scott and {El-Showk}, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2022},
  month = nov,
  number = {arXiv:2207.05221},
  eprint = {2207.05221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.05221},
  urldate = {2025-02-06},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@techreport{Kang,
  title = {Unsupervised {{Adaptation}} for {{Synthetic-to-Real Handwritten Word Recognition}}},
  author = {Kang, Lei and Rusi{\~n}ol, Mar{\c c}al and Forn{\'e}s, Alicia and Riba, Pau and Villegas, Mauricio},
  eprint = {1909.08473v1},
  urldate = {2019-09-23},
  abstract = {Handwritten Text Recognition (HTR) is still a challenging problem because it must deal with two important difficulties: the variability among writing styles, and the scarcity of labelled data. To alleviate such problems, synthetic data generation and data augmentation are typically used to train HTR systems. However, training with such data produces encouraging but still inaccurate transcriptions in real words. In this paper, we propose an unsuper-vised writer adaptation approach that is able to automatically adjust a generic handwritten word recognizer, fully trained with synthetic fonts, towards a new incoming writer. We have experimentally validated our proposal using five different datasets, covering several challenges (i) the document source: modern and historic samples, which may involve paper degradation problems; (ii) different handwriting styles: single and multiple writer collections; and (iii) language, which involves different character combinations. Across these challenging collections, we show that our system is able to maintain its performance, thus, it provides a practical and generic approach to deal with new document collections without requiring any expensive and tedious manual annotation step.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\YY62QIGH\full-text.pdf}
}

@inproceedings{Kang2019,
  title = {Convolve, {{Attend}} and {{Spell}}: {{An Attention-based Sequence-to-Sequence Model}} for {{Handwritten Word Recognition}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Kang, Lei and Toledo, J. Ignacio and Riba, Pau and Villegas, Mauricio and Forn{\'e}s, Alicia and Rusi{\~n}ol, Mar{\c c}al},
  year = {2019},
  volume = {11269 LNCS},
  pages = {459--472},
  publisher = {Springer Verlag},
  issn = {16113349},
  doi = {10.1007/978-3-030-12939-2_32},
  urldate = {2019-08-28},
  abstract = {This paper proposes Convolve, Attend and Spell, an attentionbased sequence-to-sequence model for handwritten word recognition. The proposed architecture has three main parts: an encoder, consisting of a CNN and a bi-directional GRU, an attention mechanism devoted to focus on the pertinent features and a decoder formed by a one-directional GRU, able to spell the corresponding word, character by character. Compared with the recent state-of-the-art, our model achieves competitive results on the IAM dataset without needing any pre-processing step, predefined lexicon nor language model. Code and additional results are available in https://github.com/omni-us/research-seq2seq-HTR.},
  isbn = {978-3-030-12938-5},
  file = {C:\Users\tarchibald\Zotero\storage\NCWHP73Q\full-text.pdf}
}

@article{kangGANwritingContentConditionedGeneration2020,
  title = {{{GANwriting}}: {{Content-Conditioned Generation}} of {{Styled Handwritten Word Images}}},
  author = {Kang, Lei and Riba, Pau and Wang, Yaxing and Rusi{\~n}ol, Mar{\c c}al and Forn{\'e}s, Alicia and Villegas, Mauricio},
  year = {2020},
  month = mar,
  eprint = {2003.02567},
  urldate = {2020-04-24},
  abstract = {Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwritten words. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\LM37IZML\full-text.pdf}
}

@article{Kar2019,
  title = {Evidence That Recurrent Circuits Are Critical to the Ventral Stream's Execution of Core Object Recognition Behavior},
  author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  year = {2019},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {6},
  pages = {974--983},
  publisher = {Nature Publishing Group},
  issn = {15461726},
  doi = {10.1038/s41593-019-0392-5},
  urldate = {2021-02-27},
  abstract = {Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these `challenge' images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged {\textasciitilde}30 ms later in the IT cortex for challenge images compared with primate performance-matched `control' images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development.},
  pmid = {31036945},
  keywords = {Neural decoding,Neural encoding,Object vision},
  file = {C:\Users\tarchibald\Zotero\storage\7VXUJ4B3\full-text.pdf}
}

@article{karEvidenceThatRecurrent2019,
  title = {Evidence That Recurrent Circuits Are Critical to the Ventral Stream's Execution of Core Object Recognition Behavior},
  author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  year = {2019},
  month = jun,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {6},
  pages = {974--983},
  publisher = {Nature Publishing Group},
  issn = {15461726},
  doi = {10.1038/s41593-019-0392-5},
  urldate = {2021-02-27},
  abstract = {Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these `challenge' images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged {\textasciitilde}30 ms later in the IT cortex for challenge images compared with primate performance-matched `control' images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development.},
  pmid = {31036945},
  keywords = {Neural decoding,Neural encoding,Object vision}
}

@article{Karimi2019,
  title = {Reducing the {{Hausdorff Distance}} in {{Medical Image Segmentation}} with {{Convolutional Neural Networks}}},
  author = {Karimi, Davood and Salcudean, Septimiu E.},
  year = {2019},
  month = apr,
  eprint = {1904.10030},
  urldate = {2019-10-01},
  abstract = {The Hausdorff Distance (HD) is widely used in evaluating medical image segmentation methods. However, existing segmentation methods do not attempt to reduce HD directly. In this paper, we present novel loss functions for training convolutional neural network (CNN)-based segmentation methods with the goal of reducing HD directly. We propose three methods to estimate HD from the segmentation probability map produced by a CNN. One method makes use of the distance transform of the segmentation boundary. Another method is based on applying morphological erosion on the difference between the true and estimated segmentation maps. The third method works by applying circular/spherical convolution kernels of different radii on the segmentation probability maps. Based on these three methods for estimating HD, we suggest three loss functions that can be used for training to reduce HD. We use these loss functions to train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound, magnetic resonance, and computed tomography images and compare the results with commonly-used loss functions. Our results show that the proposed loss functions can lead to approximately 18-45 \% reduction in HD without degrading other segmentation performance criteria such as the Dice similarity coefficient. The proposed loss functions can be used for training medical image segmentation methods in order to reduce the large segmentation errors.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\GXE9XX7R\full-text.pdf}
}

@techreport{Karlsson2018,
  title = {Generative {{Adversarial Networks}} for {{Image-to-Image Translation}} on {{Street View}} and {{MR Images}}},
  author = {Karlsson, Simon},
  year = {2018},
  urldate = {2019-09-25},
  file = {C:\Users\tarchibald\Zotero\storage\4W2PM8QB\full-text.pdf}
}

@article{karrasAliasFreeGenerativeAdversarial,
  title = {Alias-{{Free Generative Adversarial Networks}}},
  author = {Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  eprint = {2106.12423v4},
  urldate = {2021-10-21},
  abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\9GK75MDL\full-text.pdf}
}

@inproceedings{karrasStylebasedGeneratorArchitecture2019,
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  month = jun,
  volume = {2019-June},
  eprint = {1812.04948},
  pages = {4396--4405},
  publisher = {IEEE Computer Society},
  issn = {10636919},
  doi = {10.1109/CVPR.2019.00453},
  urldate = {2020-10-28},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  isbn = {978-1-7281-3293-8},
  keywords = {Deep Learning,Image and Video Synthesis,Representation Learning},
  file = {C:\Users\tarchibald\Zotero\storage\VJ3VMPTM\full-text.pdf}
}

@misc{kasemDeepLearningTable2022,
  title = {Deep Learning for Table Detection and Structure Recognition: {{A}} Survey},
  shorttitle = {Deep Learning for Table Detection and Structure Recognition},
  author = {Kasem, Mahmoud and Abdallah, Abdelrahman and Berendeyev, Alexander and Elkady, Ebrahem and Abdalla, Mahmoud and Mahmoud, Mohamed and Hamada, Mohamed and Nurseitov, Daniyar and {Taj-Eddin}, Islam},
  year = {2022},
  month = nov,
  number = {arXiv:2211.08469},
  eprint = {2211.08469},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08469},
  urldate = {2023-08-18},
  abstract = {Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/table-detection-structure-recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\X4QY73HD\\Kasem et al. - 2022 - Deep learning for table detection and structure re.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\GJ3ZVCNI\\2211.html}
}

@article{katoRecoveryDrawingOrder2000,
  title = {Recovery of Drawing Order from Single-Stroke Handwriting Images},
  author = {Kato, Yoshiharu and Yasuhara, Makoto},
  year = {2000},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {9},
  pages = {938--949},
  publisher = {IEEE},
  issn = {01628828},
  doi = {10.1109/34.877517},
  urldate = {2020-03-15},
  abstract = {This paper describes a new method to recover a drawing order of a handwritten script from a static 2D image. The script should be written in a single stroke and may include double-traced lines. After the script is scanned in and preprocessed, we apply our recovery method which consists of two phases. In the first phase, we globally analyze the graph constructed from the skeletal image and label the graph by determining the types of each edge. In the second phase, we trace the graph from the start vertex to the end vertex using the labeling information. This method does not enumerate the possible cases, for example, by solving the traveling salesman problem and, therefore, does not cause a combinatorial explosion even if the script is very complex. By recovering a drawing order of a handwritten script, the temporal information can be recovered from a static 2D image. Hence, this method will be used as a bridge from the offline handwriting character recognition problem to the online one.},
  file = {C:\Users\tarchibald\Zotero\storage\5VTY5HX9\full-text.pdf}
}

@article{katoRecoveryDrawingOrder2000a,
  title = {Recovery of Drawing Order from Single-Stroke Handwriting Images},
  author = {Kato, Yoshiharu and Yasuhara, Makoto},
  year = {2000},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {9},
  pages = {938--949},
  publisher = {IEEE},
  issn = {01628828},
  doi = {10.1109/34.877517},
  urldate = {2020-07-25},
  abstract = {This paper describes a new method to recover a drawing order of a handwritten script from a static 2D image. The script should be written in a single stroke and may include double-traced lines. After the script is scanned in and preprocessed, we apply our recovery method which consists of two phases. In the first phase, we globally analyze the graph constructed from the skeletal image and label the graph by determining the types of each edge. In the second phase, we trace the graph from the start vertex to the end vertex using the labeling information. This method does not enumerate the possible cases, for example, by solving the traveling salesman problem and, therefore, does not cause a combinatorial explosion even if the script is very complex. By recovering a drawing order of a handwritten script, the temporal information can be recovered from a static 2D image. Hence, this method will be used as a bridge from the offline handwriting character recognition problem to the online one.}
}

@article{keelingAttributionConfidenceLarge2025,
  title = {On the Attribution of Confidence to Large Language Models},
  author = {Keeling, Geoff and Street, Winnie},
  year = {2025},
  month = jan,
  journal = {Inquiry},
  eprint = {2407.08388},
  primaryclass = {cs},
  pages = {1--27},
  issn = {0020-174X, 1502-3923},
  doi = {10.1080/0020174X.2025.2450598},
  urldate = {2025-06-04},
  abstract = {Credences are mental states corresponding to degrees of confidence in propositions. Attribution of credences to Large Language Models (LLMs) is commonplace in the empirical literature on LLM evaluation. Yet the theoretical basis for LLM credence attribution is unclear. We defend three claims. First, our semantic claim is that LLM credence attributions are (at least in general) correctly interpreted literally, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences. Second, our metaphysical claim is that the existence of LLM credences is at least plausible, although current evidence is inconclusive. Third, our epistemic claim is that LLM credence attributions made in the empirical literature on LLM evaluation are subject to non-trivial sceptical concerns. It is a distinct possibility that even if LLMs have credences, LLM credence attributions are generally false because the experimental techniques used to assess LLM credences are not truth-tracking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\JQUR82FJ\\Keeling and Street - 2025 - On the attribution of confidence to large language.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\ICL3BIV8\\2407.html}
}

@article{keysersMultiLanguageOnlineHandwriting2017,
  title = {Multi-{{Language Online Handwriting Recognition}}},
  author = {Keysers, Daniel and Deselaers, Thomas and Rowley, Henry A. and Wang, Li Lun and Carbune, Victor},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {6},
  pages = {1180--1194},
  publisher = {IEEE Computer Society},
  issn = {01628828},
  doi = {10.1109/TPAMI.2016.2572693},
  urldate = {2020-07-24},
  abstract = {We describe Google's online handwriting recognition system that currently supports 22 scripts and 97 languages. The system's focus is on fast, high-accuracy text entry for mobile, touch-enabled devices. We use a combination of state-of-the-art components and combine them with novel additions in a flexible framework. This architecture allows us to easily transfer improvements between languages and scripts. This made it possible to build recognizers for languages that, to the best of our knowledge, are not handled by any other online handwriting recognition system. The approach also enabled us to use the same architecture both on very powerful machines for recognition in the cloud as well as on mobile devices with more limited computational power by changing some of the settings of the system. In this paper we give a general overview of the system architecture and the novel components, such as unified time- and position-based input interpretation, trainable segmentation, minimum-error rate training for feature combination, and a cascade of pruning strategies. We present experimental results for different setups. The system is currently publicly available in several Google products, for example in Google Translate and as an input method for Android devices.},
  keywords = {handwriting recognition,Online handwriting recognition},
  file = {C:\Users\tarchibald\Zotero\storage\6JJ3ZFRW\full-text.pdf}
}

@article{keysersMultiLanguageOnlineHandwriting2017a,
  title = {Multi-{{Language Online Handwriting Recognition}}},
  author = {Keysers, Daniel and Deselaers, Thomas and Rowley, Henry A. and Wang, Li Lun and Carbune, Victor},
  year = {2017},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {6},
  pages = {1180--1194},
  publisher = {IEEE Computer Society},
  issn = {01628828},
  doi = {10.1109/TPAMI.2016.2572693},
  urldate = {2021-05-26},
  abstract = {We describe Google's online handwriting recognition system that currently supports 22 scripts and 97 languages. The system's focus is on fast, high-accuracy text entry for mobile, touch-enabled devices. We use a combination of state-of-the-art components and combine them with novel additions in a flexible framework. This architecture allows us to easily transfer improvements between languages and scripts. This made it possible to build recognizers for languages that, to the best of our knowledge, are not handled by any other online handwriting recognition system. The approach also enabled us to use the same architecture both on very powerful machines for recognition in the cloud as well as on mobile devices with more limited computational power by changing some of the settings of the system. In this paper we give a general overview of the system architecture and the novel components, such as unified time- and position-based input interpretation, trainable segmentation, minimum-error rate training for feature combination, and a cascade of pruning strategies. We present experimental results for different setups. The system is currently publicly available in several Google products, for example in Google Translate and as an input method for Android devices.},
  pmid = {27244718},
  keywords = {handwriting recognition,Online handwriting recognition},
  file = {C:\Users\tarchibald\Zotero\storage\5MD8LIHX\full-text.pdf}
}

@misc{khanmohammadiCalibratingLLMConfidence2025,
  title = {Calibrating {{LLM Confidence}} by {{Probing Perturbed Representation Stability}}},
  author = {Khanmohammadi, Reza and Miahi, Erfan and Mardikoraem, Mehrsa and Kaur, Simerjot and Brugere, Ivan and Smiley, Charese H. and Thind, Kundan and Ghassemi, Mohammad M.},
  year = {2025},
  month = may,
  number = {arXiv:2505.21772},
  eprint = {2505.21772},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.21772},
  urldate = {2025-06-04},
  abstract = {Miscalibration in Large Language Models (LLMs) undermines their reliability, highlighting the need for accurate confidence estimation. We introduce CCPS (Calibrating LLM Confidence by Probing Perturbed Representation Stability), a novel method analyzing internal representational stability in LLMs. CCPS applies targeted adversarial perturbations to final hidden states, extracts features reflecting the model's response to these perturbations, and uses a lightweight classifier to predict answer correctness. CCPS was evaluated on LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and open-ended formats. Our results show that CCPS significantly outperforms current approaches. Across four LLMs and three MMLU variants, CCPS reduces Expected Calibration Error by approximately 55\% and Brier score by 21\%, while increasing accuracy by 5 percentage points, Area Under the Precision-Recall Curve by 4 percentage points, and Area Under the Receiver Operating Characteristic Curve by 6 percentage points, all relative to the strongest prior method. CCPS delivers an efficient, broadly applicable, and more accurate solution for estimating LLM confidence, thereby improving their trustworthiness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\WV2ZBXR4\\Khanmohammadi et al. - 2025 - Calibrating LLM Confidence by Probing Perturbed Re.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\FLJSGBBJ\\2505.html}
}

@incollection{khetanAchievingBudgetoptimalityAdaptive2016,
  title = {Achieving Budget-Optimality with Adaptive Schemes in Crowdsourcing},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Khetan, Ashish and Oh, Sewoong},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {4844--4852},
  publisher = {Curran Associates, Inc.},
  urldate = {2018-11-20},
  file = {C:\Users\tarchibald\Zotero\storage\6QKMQB3C\Khetan et al. - 2016 - Achieving budget-optimality with adaptive schemes .html}
}

@article{kietzmannRecurrenceRequiredCapture2019,
  title = {Recurrence Is Required to Capture the Representational Dynamics of the Human Visual System},
  author = {Kietzmann, Tim C and Spoerer, Courtney J and S{\"o}rensen, Lynn and Cichy, Radoslaw M and Hauk, Olaf and Kriegeskorte, Nikolaus},
  year = {2019},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {116},
  number = {43},
  eprint = {1903.05946},
  pages = {21854--21863},
  publisher = {National Academy of Sciences},
  doi = {10.1073/pnas.1905544116},
  urldate = {2021-02-27},
  abstract = {The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multi-region cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.},
  archiveprefix = {arXiv},
  keywords = {Deep recurrent neural networks,Dynamics,Magnetoencephalography,Object recognition,Representational,Virtual cooling},
  file = {C:\Users\tarchibald\Zotero\storage\48SJ5TPA\full-text.pdf}
}

@misc{kimEarlyEvidenceHow2025,
  title = {Early Evidence of How {{LLMs}} Outperform Traditional Systems on {{OCR}}/{{HTR}} Tasks for Historical Records},
  author = {Kim, Seorin and Baudru, Julien and Ryckbosch, Wouter and Bersini, Hugues and Ginis, Vincent},
  year = {2025},
  month = jan,
  number = {arXiv:2501.11623},
  eprint = {2501.11623},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.11623},
  urldate = {2025-06-01},
  abstract = {We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to transcribe historical handwritten documents in a tabular format and compare their performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract, and TrOCR. Considering the tabular form of the data, two types of experiments are executed: one where the images are split line by line and the other where the entire scan is used as input. Based on CER and BLEU, we demonstrate that LLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the evaluated CER and BLEU scores to human evaluations to better judge the outputs of whole-scan experiments and understand influential factors for CER and BLEU. Combining judgments from all the evaluation metrics, we conclude that two-shot GPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan images yield the transcriptions of the historical records most similar to the ground truth.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\NJ9CJCVS\\Kim et al. - 2025 - Early evidence of how LLMs outperform traditional .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\D6AJYEZZ\\2501.html}
}

@article{kimImprovedRobustnessVision2023,
  title = {Improved Robustness of Vision Transformers via Prelayernorm in Patch Embedding},
  author = {Kim, Bum Jun and Choi, Hyeyeon and Jang, Hyeonah and Lee, Dong Gu and Jeong, Wonseok and Kim, Sang Woo},
  year = {2023},
  month = sep,
  journal = {Pattern Recognition},
  volume = {141},
  pages = {109659},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2023.109659},
  urldate = {2025-05-14},
  abstract = {Vision Transformers (ViTs) have recently demonstrated state-of-the-art performance in various vision tasks, replacing convolutional neural networks (CNNs). However, because ViT has a different architectural design than CNN, it may behave differently. To investigate whether ViT has a different performance or robustness, we tested ViT and CNN under various imaging conditions in practical vision tasks. We confirmed that for most image transformations, ViT's robustness was comparable or even better than that of CNN. However, for contrast enhancement, ViT performed particularly poorly. We show that this is because positional embedding in ViT's patch embedding can work improperly when the color scale changes. We demonstrate that the use of PreLayerNorm, a modified patch embedding structure, ensures the consistent behavior of ViT. Results demonstrate that ViT with PreLayerNorm exhibited improved robustness in the contrast-varying environments.},
  keywords = {Contrast enhancement,Convolutional neural network,Deep learning,Layer normalization,Patch embedding,Robustness,Vision transformer}
}

@inproceedings{kimOCRFreeDocumentUnderstanding2022,
  title = {{{OCR-Free Document Understanding Transformer}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022: 17th {{European Conference}}, {{Tel Aviv}}, {{Israel}}, {{October}} 23--27, 2022, {{Proceedings}}, {{Part XXVIII}}},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year = {2022},
  month = oct,
  pages = {498--517},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-031-19815-1_29},
  urldate = {2025-05-31},
  abstract = {Understanding document images (e.g., invoices) is a core but challenging task since it requires complex functions such as reading text and a holistic understanding of the document. Current Visual Document Understanding (VDU) methods outsource the task of reading text to off-the-shelf Optical Character Recognition (OCR) engines and focus on the understanding task with the OCR outputs. Although such OCR-based approaches have shown promising performance, they suffer from 1) high computational costs for using OCR; 2) inflexibility of OCR models on languages or types of documents; 3) OCR error propagation to the subsequent process. To address these issues, in this paper, we introduce a novel OCR-free VDU model named Donut, which stands for Document understanding transformer. As the first step in OCR-free VDU research, we propose a simple architecture (i.e., Transformer) with a pre-training objective (i.e., cross-entropy loss). Donut is conceptually simple yet effective. Through extensive experiments and analyses, we show a simple OCR-free VDU model, Donut, achieves state-of-the-art performances on various VDU tasks in terms of both speed and accuracy. In addition, we offer a synthetic data generator that helps the model pre-training to be flexible in various languages and domains. The code, trained model, and synthetic data are available at .},
  isbn = {978-3-031-19814-4},
  file = {C:\Users\tarchibald\Zotero\storage\QSBBNTII\Kim et al. - 2022 - OCR-free Document Understanding Transformer.pdf}
}

@inproceedings{Kingma2015,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015 - {{Conference Track Proceedings}}},
  author = {Kingma, Diederik P. and Ba, Jimmy Lei},
  year = {2015},
  month = dec,
  eprint = {1412.6980},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2020-07-23},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\IRCMIYAJ\full-text.pdf}
}

@article{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  urldate = {2018-11-20},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{kirillovSegmentAnything2023a,
  title = {Segment {{Anything}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2023},
  month = apr,
  number = {arXiv:2304.02643},
  eprint = {2304.02643},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.02643},
  urldate = {2023-09-21},
  abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\F2MVNVUX\\Kirillov et al. - 2023 - Segment Anything.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\PMITIWRZ\\2304.html}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {114},
  number = {13},
  eprint = {1612.00796},
  pages = {3521--3526},
  publisher = {National Academy of Sciences},
  issn = {10916490},
  doi = {10.1073/PNAS.1611835114/SUPPL_FILE/PNAS.201611835SI.PDF},
  urldate = {2022-04-15},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arXiv},
  pmid = {28292907},
  keywords = {Artificial intelligence,Continual learning,Deep learning,Stability plasticity,Synaptic consolidation},
  file = {C:\Users\tarchibald\Zotero\storage\BVMASAS4\full-text.pdf}
}

@book{kishSurveySampling1965,
  title = {Survey Sampling},
  author = {Kish, Leslie},
  year = {1965},
  series = {Survey Sampling},
  publisher = {Wiley},
  address = {Oxford, England},
  abstract = {Describes methodological and theoretical issues in survey sampling.  Harvard Book List (edited) 1971 \#494 (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\tarchibald\Zotero\storage\42E94BV3\1965-35018-000.html}
}

@inproceedings{kleberCVLDataBaseOffLineDatabase2013,
  title = {{{CVL-DataBase}}: {{An Off-Line Database}} for {{Writer Retrieval}}, {{Writer Identification}} and {{Word Spotting}}},
  shorttitle = {{{CVL-DataBase}}},
  booktitle = {2013 12th {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Kleber, Florian and Fiel, Stefan and Diem, Markus and Sablatnig, Robert},
  year = {2013},
  month = aug,
  pages = {560--564},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  doi = {10.1109/ICDAR.2013.117},
  urldate = {2024-02-19},
  abstract = {In this paper a public database for writer retrieval, writer identification and word spotting is presented. The CVL-Database consists of 7 different handwritten texts (1 German and 6 English Texts) and 311 different writers. For each text an RGB color image (300 dpi) comprising the handwritten text and the printed text sample are available as well as a cropped version (only handwritten). A unique ID identifies the writer, whereas the bounding boxes for each single word are stored in an XML file. An evaluation of the best algorithms of the ICDAR and ICHFR writer identification contest has been performed on the CVL-database.},
  isbn = {978-0-7695-4999-6},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\3RK8LBRU\Kleber et al. - 2013 - CVL-DataBase An Off-Line Database for Writer Retr.pdf}
}

@article{kornblithSimilarityNeuralNetwork2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  month = may,
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-June},
  eprint = {1905.00414},
  pages = {6156--6175},
  publisher = {International Machine Learning Society (IMLS)},
  urldate = {2021-02-28},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\N5LZZXBM\full-text.pdf}
}

@article{kornblithSimilarityNeuralNetwork2019a,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  eprint = {1905.00414},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\Y6VDNPCX\-258322361.pdf}
}

@misc{kostumovUncertaintyAwareEvaluationVisionLanguage2024,
  title = {Uncertainty-{{Aware Evaluation}} for {{Vision-Language Models}}},
  author = {Kostumov, Vasily and Nutfullin, Bulat and Pilipenko, Oleg and Ilyushin, Eugene},
  year = {2024},
  month = feb,
  number = {arXiv:2402.14418},
  eprint = {2402.14418},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.14418},
  urldate = {2025-06-04},
  abstract = {Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\5B8ZZPTX\\Kostumov et al. - 2024 - Uncertainty-Aware Evaluation for Vision-Language M.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\AKU94Y6G\\2402.html}
}

@incollection{Kotu2019,
  title = {Deep {{Learning}}},
  booktitle = {Data {{Science}}},
  author = {Kotu, Vijay and Deshpande, Bala},
  year = {2019},
  pages = {307--342},
  publisher = {Elsevier},
  doi = {10.1016/b978-0-12-814761-0.00010-1},
  abstract = {NATURE METHODS {\textbar} VOL.13 NO.1 {\textbar} JANUARY 2016 {\textbar} 35 METHODS TO WATCH {\textbar} SPECIAL FEATURE and high computational costs are being tackled. Researchers in academic settings as well as in startup companies such as Deep Genomics, launched July 22, 2015, by some of the authors of DeepBind, will increasingly apply deep learning to genome analysis and precision medicine. The goal is to predict the effect of genetic variants--- both naturally occurring and introduced by genome editing---on a cell's regulatory landscape and how this in turn affects dis-ease development. Nicole Rusk Deep learning New computational tools learn complex motifs from large sequence data sets. A powerful form of machine learning that enables computers to solve perceptual problems such as image and speech rec-ognition is increasingly making an entry into the biological sciences. These deep-learning methods, such as deep artificial neural networks, use multiple processing layers to discover patterns and structure in very large data sets. Each layer learns a concept from the data that subsequent lay-ers build on; the higher the level, the more abstract the concepts that are learned. Deep learning does not depend on prior data processing and automatically extracts features. To use a simple example, a deep neural network tasked with interpreting shapes would learn to recognize simple edges in the first layer and then add recog-nition of the more complex shapes com-posed of those edges in subsequent lay-ers. There is no hard and fast rule for how many layers are needed to constitute deep learning, but most experts agree that more than two are required. Recent examples show the power of deep learning to derive regulatory fea-tures in genomes from DNA sequence alone: DeepSEA (Nat. Methods 12, 931-- 934, 2015) uses genomic sequence as input, trains on chromatin profiles from large consortia such as ENCODE and the Epigenomics Roadmap, and predicts the effect of single-nucleotide variants on reg-ulatory regions such as DNase hypersen-sitive sites, transcription factor--binding sites and histone marks. Basset (bioRxiv, doi:10.1101/028399, 2015) uses similar deep neural networks to predict the effect of single-nucleotide polymorphisms on chromatin accessibility. DeepBind (Nat. Biotechnol. 33, 831--838, 2015) finds protein-binding sites on RNA and DNA and predicts the effects of mutations. Deep learning will be invaluable in the context of big data, as it extracts high-level information from very large volumes of data. As it gains traction in genome analy-sis, initial challenges such as overfitting due to rare dependencies in the training data},
  file = {C:\Users\tarchibald\Zotero\storage\GZIDVIMY\Deep Learning Book.pdf}
}

@article{kramerNonlinearPrincipalComponent1991,
  title = {Nonlinear Principal Component Analysis Using Autoassociative Neural Networks},
  author = {Kramer, Mark A.},
  year = {1991},
  journal = {AIChE Journal},
  volume = {37},
  number = {2},
  pages = {233--243},
  issn = {1547-5905},
  doi = {10.1002/aic.690370209},
  urldate = {2023-08-23},
  abstract = {Nonlinear principal component analysis is a novel technique for multivariate data analysis, similar to the well-known method of principal component analysis. NLPCA, like PCA, is used to identify and remove correlations among problem variables as an aid to dimensionality reduction, visualization, and exploratory data analysis. While PCA identifies only linear correlations between variables, NLPCA uncovers both linear and nonlinear correlations, without restriction on the character of the nonlinearities present in the data. NLPCA operates by training a feedforward neural network to perform the identity mapping, where the network inputs are reproduced at the output layer. The network contains an internal ``bottleneck'' layer (containing fewer nodes than input or output layers), which forces the network to develop a compact representation of the input data, and two additional hidden layers. The NLPCA method is demonstrated using time-dependent, simulated batch reaction data. Results show that NLPCA successfully reduces dimensionality and produces a feature space map resembling the actual distribution of the underlying system parameters.},
  copyright = {Copyright {\copyright} 1991 American Institute of Chemical Engineers},
  langid = {english},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\9UY35JNE\\Kramer - 1991 - Nonlinear principal component analysis using autoa.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\Y5MHM6F9\\aic.html}
}

@article{kreimanFeedforwardSweepFeedback2020,
  title = {Beyond the Feedforward Sweep: Feedback Computations in the Visual Cortex},
  author = {Kreiman, Gabriel and Serre, Thomas},
  year = {2020},
  journal = {Annals of the New York Academy of Sciences},
  volume = {1464},
  number = {1},
  eprint = {32112444},
  eprinttype = {pubmed},
  pages = {222},
  issn = {1749-6632},
  doi = {10.1111/nyas.14320},
  abstract = {Visual perception involves the rapid formation of a coarse image representation at the onset of visual processing, which is iteratively refined by late computational processes. These early versus late time windows approximately map onto feedforward and feedback processes, respectively. State-of-the-art convolutional neural networks, the main engine behind recent machine vision successes, are feedforward architectures. Their successes and limitations provide critical information regarding which visual tasks can be solved by purely feedforward processes and which require feedback mechanisms. We provide an overview of recent work in cognitive neuroscience and machine vision that highlights the possible role of feedback processes for both visual recognition and beyond. We conclude by discussing important open questions for future research.},
  pmid = {32112444}
}

@article{krishnanMatchingHandwrittenDocument2016,
  title = {Matching {{Handwritten Document Images}}},
  author = {Krishnan, Praveen and Jawahar, C. V.},
  year = {2016},
  month = may,
  journal = {arXiv:1605.05923 [cs]},
  eprint = {1605.05923},
  primaryclass = {cs},
  urldate = {2018-11-20},
  abstract = {We address the problem of predicting similarity between a pair of handwritten document images written by different individuals. This has applications related to matching and mining in image collections containing handwritten content. A similarity score is computed by detecting patterns of text re-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (CNN) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@techreport{Krizhevsky,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  urldate = {2019-09-24},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {C:\Users\tarchibald\Zotero\storage\85W3BZFW\full-text.pdf}
}

@article{krotovUnsupervisedLearningCompeting2019,
  title = {Unsupervised Learning by Competing Hidden Units},
  author = {Krotov, Dmitry and Hopfield, John J.},
  year = {2019},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {116},
  number = {16},
  eprint = {1806.10181},
  pages = {7723--7731},
  publisher = {National Academy of Sciences},
  issn = {10916490},
  doi = {10.1073/pnas.1820458116},
  urldate = {2020-12-24},
  abstract = {It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb's idea that change of the synapse strength should be local---i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks.},
  archiveprefix = {arXiv},
  pmid = {30926658},
  keywords = {Backpropagation,Biological deep learning,Hebbian-like plasticity},
  file = {C:\Users\tarchibald\Zotero\storage\AJIAV5LK\full-text.pdf}
}

@article{kubiliusBrainlikeObjectRecognition2019,
  title = {Brain-like Object Recognition with High-Performing Shallow Recurrent {{ANNs}}},
  author = {Kubilius, Jonas and Schrimpf, Martin and Kar, Kohitij and Rajalingham, Rishi and Hong, Ha and Majaj, Najib J. and Issa, Elias B. and Bashivan, Pouya and {Prescott-Roy}, Jonathan and Schmidt, Kailyn and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L.K. and DiCarlo, James J.},
  year = {2019},
  month = sep,
  journal = {arXiv},
  eprint = {1909.06161},
  publisher = {arXiv},
  issn = {23318422},
  urldate = {2021-03-02},
  abstract = {Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\U26UB4PP\full-text.pdf}
}

@techreport{KumarBhunia,
  title = {Handwriting {{Recognition}} in {{Low-resource Scripts}} Using {{Adversarial Learning}}},
  author = {Kumar Bhunia, Ayan and Das, Abhirup and Kumar Bhunia, Ankan and Sai, Perla and Kishore, Raj and Roy, Partha P},
  eprint = {1811.01396v5},
  urldate = {2019-10-06},
  abstract = {Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neu-ral network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose the Adversar-ial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by the AFDM, not only on extensive Latin word datasets but also sparser Indic scripts. We record results for varying training data sizes, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\YGK6RP9P\full-text.pdf}
}

@inproceedings{Kumarbhunia2018,
  title = {Handwriting {{Trajectory Recovery}} Using {{End-to-End Deep Encoder-Decoder Network}}},
  booktitle = {Proceedings - {{International Conference}} on {{Pattern Recognition}}},
  author = {Kumarbhunia, Ayan and Bhowmick, Abir and Kumar Bhunia, Ankan and Konwer, Aishik and Banerjee, Prithaj and Pratim Roy, Partha and Pal, Umapada and Bhunia, Ankan Kumar Ayan Kumar and Bhowmick, Abir and Bhunia, Ankan Kumar Ayan Kumar and Konwer, Aishik and Banerjee, Prithaj and Roy, Partha Pratim and Pal, Umapada},
  year = {2018},
  month = nov,
  volume = {2018-Augus},
  eprint = {1801.07211},
  pages = {3639--3644},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10514651},
  doi = {10.1109/ICPR.2018.8546093},
  urldate = {2019-10-05},
  abstract = {In this paper, we introduce a novel technique to recover the pen trajectory of offline characters which is a crucial step for handwritten character recognition. Generally, online acquisition approach has more advantage than its offline counterpart as the online technique keeps track of the pen movement. Hence, pen tip trajectory retrieval from offline text can bridge the gap between online and offline methods. Our proposed framework employs sequence to sequence model which consists of an encoder-decoder LSTM module. Our encoder module consists of Convolutional LSTM network, which takes an offline character image as the input and encodes the feature sequence to a hidden representation. The output of the encoder is fed to a decoder LSTM and we get the successive coordinate points from every time step of the decoder LSTM. Although the sequence to sequence model is a popular paradigm in various computer vision and language translation tasks, the main contribution of our work lies in designing an end-to-end network for a decade old popular problem in Document Image Analysis community. Tamil, Telugu and Devanagari characters of LIPI Toolkit dataset are used for our experiments. Our proposed method has achieved superior performance compared to the other conventional approaches.},
  archiveprefix = {arXiv},
  isbn = {978-1-5386-3788-3},
  keywords = {Deep Learning,Encoder-Decoder Network,Handwriting Trajectory Recovery,Sequence to Sequence Model},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\2JTJAVQ8\\Bhunia et al. - 2018 - Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder Network.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\MR8ZZJXU\\Kumarbhunia et al. - 2018 - Handwriting Trajectory Recovery using End-to-End Deep Encoder-Decoder Network.pdf}
}

@article{KumarBoyat2015,
  title = {Signal \& {{Image Processing}}},
  author = {Kumar Boyat, Ajay and Kumar Joshi, Brijendra},
  year = {2015},
  journal = {An International Journal (SIPIJ)},
  volume = {6},
  number = {2},
  doi = {10.5121/sipij.2015.6206},
  urldate = {2019-10-11},
  abstract = {Noise is always presents in digital images during image acquisition, coding, transmission, and processing steps. Noise is very difficult to remove it from the digital images without the prior knowledge of noise model. That is why, review of noise models are essential in the study of image denoising techniques. In this paper, we express a brief overview of various noise models. These noise models can be selected by analysis of their origin. In this way, we present a complete and quantitative analysis of noise models available in digital images.},
  keywords = {Digital images,Noise model,Power spectral density (PDF),Probability density function},
  file = {C:\Users\tarchibald\Zotero\storage\8MT6YRKU\full-text.pdf}
}

@article{kumarStructuralSimilarityDocument2014,
  title = {Structural Similarity for Document Image Classification and Retrieval},
  author = {Kumar, Jayant and Ye, Peng and Doermann, David},
  year = {2014},
  month = jul,
  journal = {Pattern Recognition Letters},
  series = {{{ICPR2012 Awarded Papers}}},
  volume = {43},
  pages = {119--126},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2013.10.030},
  urldate = {2023-08-23},
  abstract = {This paper presents a novel approach to defining document image structural similarity for the applications of classification and retrieval. We first build a codebook of SURF descriptors extracted from a set of representative training images. We then encode each document and model the spatial relationships between them by recursively partitioning the image and computing histograms of codewords in each partition. A random forest classifier is trained with the resulting features, and used for classification and retrieval. We demonstrate the effectiveness of our approach on table and tax form retrieval, and show that the proposed method outperforms previous approaches even when the training data is limited.},
  keywords = {Classification,Random forest,Retrieval,Structural similarity}
}

@misc{LabeledPDFDataset,
  title = {Labeled {{PDF Dataset}} from {{UNT}}.Edu - ({{Listing Multiple Items}}). {{UNT Digital Library}}},
  urldate = {2023-09-07},
  howpublished = {https://digital.library.unt.edu/ark:/67531/metadc1757662/m1/},
  file = {C:\Users\tarchibald\Zotero\storage\MMXRRGZR\m1.html}
}

@inproceedings{larsonEvaluationDocumentClassification2023,
  title = {On {{Evaluation}} of {{Document Classification}} with {{RVL-CDIP}}},
  booktitle = {Proceedings of the 17th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Larson, Stefan and Lim, Gordon and Leach, Kevin},
  editor = {Vlachos, Andreas and Augenstein, Isabelle},
  year = {2023},
  month = may,
  pages = {2665--2678},
  publisher = {Association for Computational Linguistics},
  address = {Dubrovnik, Croatia},
  doi = {10.18653/v1/2023.eacl-main.195},
  urldate = {2024-05-09},
  abstract = {The RVL-CDIP benchmark is widely used for measuring performance on the task of document classification. Despite its widespread use, we reveal several undesirable characteristics of the RVL-CDIP benchmark. These include (1) substantial amounts of label noise, which we estimate to be 8.1\% (ranging between 1.6\% to 16.9\% per document category); (2) presence of many ambiguous or multi-label documents; (3) a large overlap between test and train splits, which can inflate model performance metrics; and (4) presence of sensitive personally-identifiable information like US Social Security numbers (SSNs). We argue that there is a risk in using RVL-CDIP for benchmarking document classifiers, as its limited scope, presence of errors (state-of-the-art models now achieve accuracy error rates that are within our estimated label error rate), and lack of diversity make it less than ideal for benchmarking. We further advocate for the creation of a new document classification benchmark, and provide recommendations for what characteristics such a resource should include.},
  file = {C:\Users\tarchibald\Zotero\storage\Q2E99XVE\Larson et al. - 2023 - On Evaluation of Document Classification with RVL-.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\HVSDDL53\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\YTXYUIFE\\726791.html}
}

@inproceedings{leePageSegmentationUsing2019,
  title = {Page {{Segmentation}} Using a {{Convolutional Neural Network}} with {{Trainable Co-Occurrence Features}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Lee, Joonho and Hayashi, Hideaki and Ohyama, Wataru and Uchida, Seiichi},
  year = {2019},
  month = sep,
  pages = {1023--1028},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00167},
  abstract = {In document analysis, page segmentation is a fundamental task that divides a document image into semantic regions. In addition to local features, such as pixel-wise information, co-occurrence features are also useful for extracting texture-like periodic information for accurate segmentation. However, existing convolutional neural network (CNN)-based methods do not have any mechanisms that explicitly extract co-occurrence features. In this paper, we propose a method for page segmentation using a CNN with trainable multiplication layers (TMLs). The TML is specialized for extracting co-occurrences from feature maps, thereby supporting the detection of objects with similar textures and periodicities. This property is also considered to be effective for document image analysis because of regularity in text line structures, tables, etc. In the experiment, we achieved promising performance on a pixel-wise page segmentation task by combining TMLs with U-Net. The results demonstrate that TMLs can improve performance compared to the original U-Net. The results also demonstrate that TMLs are helpful for detecting regions with periodically repeating features, such as tables and main text.},
  keywords = {Decoding,Feature extraction,Image segmentation,Layout,layout analysis,page segmentation,se,Semantics,Task analysis,Text analysis,trainable multiplication layer,U-Net},
  file = {C:\Users\tarchibald\Zotero\storage\ET3MPHVN\8978118.html}
}

@inproceedings{levonianTradeoffsSamplingSearch2022,
  title = {Trade-Offs in {{Sampling}} and {{Search}} for {{Early-stage Interactive Text Classification}}},
  booktitle = {27th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Levonian, Zachary and Lee, Chia-Jung and Murdock, Vanessa and Harper, F. Maxwell},
  year = {2022},
  month = mar,
  series = {{{IUI}} '22},
  pages = {566--583},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3490099.3511134},
  urldate = {2023-08-16},
  abstract = {For many automated classification tasks, collecting labeled data is the key barrier to training a useful supervised model. Interfaces for interactive labeling tighten the loop of labeled data collection and model development, enabling a subject-matter expert to quickly establish the feasibility of a classifier to address a problem of interest. These interactive machine learning (IML) interfaces iteratively sample unlabeled data for annotation, train a new model, and display feedback on the model's estimated performance. Different sampling strategies affect both the rate at which the model improves and the bias of performance estimates. We compare the performance of three sampling strategies in the ``early-stage'' of label collection, starting from zero labeled data. By simulating a user's interactions with an IML labeling interface, we demonstrate a trade-off between improving a text classifier's performance and computing unbiased estimates of that performance. We show that supplementing early-stage sampling with user-guided text search can effectively ``seed'' a classifier with positive documents without compromising generalization performance---particularly for imbalanced tasks where positive documents are rare. We argue for the benefits of incorporating search alongside active learning in IML interfaces and identify design trade-offs around the use of non-random sampling strategies.},
  isbn = {978-1-4503-9144-3},
  keywords = {classification,interactive machine learning,sampling},
  annotation = {Amazon},
  file = {C:\Users\tarchibald\Zotero\storage\CHQLXLIN\Levonian et al. - 2022 - Trade-offs in Sampling and Search for Early-stage .pdf}
}

@inproceedings{lewisBuildingTestCollection2006,
  title = {Building a Test Collection for Complex Document Information Processing},
  booktitle = {Proceedings of the 29th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Lewis, D. and Agam, G. and Argamon, S. and Frieder, O. and Grossman, D. and Heard, J.},
  year = {2006},
  month = aug,
  series = {{{SIGIR}} '06},
  pages = {665--666},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1148170.1148307},
  urldate = {2023-08-23},
  abstract = {Research and development of information access technology for scanned paper documents has been hampered by the lack of public test collections of realistic scope and complexity. As part of a project to create a prototype system for search and mining of masses of document images, we are assembling a 1.5 terabyte dataset to support evaluation of both end-to-end complex document information processing (CDIP) tasks (e.g., text retrieval and data mining) as well as component technologies such as optical character recognition (OCR), document structure analysis, signature matching, and authorship attribution.},
  isbn = {978-1-59593-369-0},
  keywords = {corpora,metadata,queries,relevance judgments,TREC}
}

@inproceedings{Li2014,
  title = {Teaching a Calligraphy Robot via a Touch Screen},
  booktitle = {{{IEEE International Conference}} on {{Automation Science}} and {{Engineering}}},
  author = {Li, Jun and Sun, Wei and Zhou, Mengchu and Dai, Xianzhong},
  year = {2014},
  volume = {2014-Janua},
  pages = {221--226},
  publisher = {IEEE Computer Society},
  issn = {21618089},
  doi = {10.1109/CoASE.2014.6899330},
  urldate = {2019-10-16},
  abstract = {Chinese calligraphy as a Chinese character writing art is an important part of Chinese art. As a representative of the intangible cultural heritage of humanity, Chinese calligraphy is under protection and its universal education is a very important inheritance means. This paper proposes a new method to teach a robot to implement the vivid replication of personal writing without losing the shape and effect of a calligraphy character. In the method, a capacitive touch screen used as an input device to obtain the features such as touch point positions, strokes, width, writing velocity and acceleration. After a series of processes, the writing is eventually transformed to a language program for a calligraphy robot to perform the repeated writing. The writing on the touch screen, writing with a brush by hand and one with a brush by the robot are compared. The results show that the robot taught with the proposed method can preserve the writing features of a writer and achieve the effect of brush writing.},
  keywords = {calligraphy,Chinese characters,robot,teaching},
  file = {C:\Users\tarchibald\Zotero\storage\M59WCQRP\full-text.pdf}
}

@article{liaoBridgingGapsResidual2016,
  title = {Bridging the {{Gaps Between Residual Learning}}, {{Recurrent Neural Networks}} and {{Visual Cortex}}},
  author = {Liao, Qianli and Poggio, Tomaso},
  year = {2016},
  month = apr,
  eprint = {1604.03640},
  urldate = {2021-02-27},
  abstract = {We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a special type of shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 and ImageNet dataset.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\ECG6AV8H\full-text.pdf}
}

@misc{liDiTSelfsupervisedPretraining2022,
  title = {{{DiT}}: {{Self-supervised Pre-training}} for {{Document Image Transformer}}},
  shorttitle = {{{DiT}}},
  author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
  year = {2022},
  month = jul,
  number = {arXiv:2203.02378},
  eprint = {2203.02378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.02378},
  urldate = {2023-08-23},
  abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose {\textbackslash}textbf\{DiT\}, a self-supervised pre-trained {\textbackslash}textbf\{D\}ocument {\textbackslash}textbf\{I\}mage {\textbackslash}textbf\{T\}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9), table detection (94.23 \${\textbackslash}rightarrow\$ 96.55) and text detection for OCR (93.07 \${\textbackslash}rightarrow\$ 94.29). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\IML4UFWA\\Li et al. - 2022 - DiT Self-supervised Pre-training for Document Ima.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\BXGU2X38\\2203.html}
}

@misc{liDynamicComputationalTime2017,
  title = {Dynamic {{Computational Time}} for {{Visual Attention}}},
  author = {Li, Zhichao and Yang, Yi and Liu, Xiao and Zhou, Feng and Wen, Shilei and Xu, Wei},
  year = {2017},
  month = sep,
  number = {arXiv:1703.10332},
  eprint = {1703.10332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.10332},
  urldate = {2023-08-25},
  abstract = {We propose a dynamic computational time model to accelerate the average processing time for recurrent visual attention (RAM). Rather than attention with a fixed number of steps for each input image, the model learns to decide when to stop on the fly. To achieve this, we add an additional continue/stop action per time step to RAM and use reinforcement learning to learn both the optimal attention policy and stopping policy. The modification is simple but could dramatically save the average computational time while keeping the same recognition performance as RAM. Experimental results on CUB-200-2011 and Stanford Cars dataset demonstrate the dynamic computational model can work effectively for fine-grained image recognition.The source code of this paper can be obtained from https://github.com/baidu-research/DT-RAM},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\BNTXG8JV\\Li et al. - 2017 - Dynamic Computational Time for Visual Attention.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\74GG8DLV\\1703.html}
}

@inproceedings{lieblEvaluationDNNArchitectures2021,
  title = {An {{Evaluation}} of {{DNN Architectures}} for {{Page Segmentation}} of {{Historical Newspapers}}},
  booktitle = {2020 25th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Liebl, Bernhard and Burghardt, Manuel},
  year = {2021},
  month = jan,
  pages = {5153--5160},
  issn = {1051-4651},
  doi = {10.1109/ICPR48806.2021.9412571},
  abstract = {One important and particularly challenging step in the optical character recognition of historical documents with complex layouts, such as newspapers, is the separation of text from non-text content (e.g. page borders or illustrations). This step is commonly referred to as page segmentation. While various rule-based algorithms have been proposed, the applicability of Deep Neural Networks for this task recently has gained a lot of attention. In this paper, we perform a systematic evaluation of 11 different published backbone architectures and 9 different tiling and scaling configurations for separating text, tables or table column lines. We also show the influence of the number of labels and the number of training pages on the segmentation quality, which we measure using the Matthews Correlation Coefficient. Our results show that (depending on the task) Inception-ResNet-v2 and EfficientNet backbones work best, vertical tiling is generally preferable to other tiling approaches, and training data that comprises 30 to 40 pages will be sufficient most of the time.},
  keywords = {Computer architecture,image segmentation,Layout,neural networks,optical character recognition software,Particle separators,segmentation,Systematics,Training,Training data,Transfer learning},
  file = {C:\Users\tarchibald\Zotero\storage\6P29L6BH\Liebl and Burghardt - 2021 - An Evaluation of DNN Architectures for Page Segmen.pdf}
}

@article{liFourierNeuralOperator2020,
  title = {Fourier {{Neural Operator}} for {{Parametric Partial Differential Equations}}},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  year = {2020},
  month = oct,
  eprint = {2010.08895},
  urldate = {2020-11-02},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.},
  archiveprefix = {arXiv}
}

@inproceedings{liInstanceAwareDocument2019,
  title = {Instance {{Aware Document Image Segmentation}} Using {{Label Pyramid Networks}} and {{Deep Watershed Transformation}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Li, Xiao-Hui and Yin, Fei and Xue, Tao and Liu, Long and Ogier, Jean-Marc and Liu, Cheng-Lin},
  year = {2019},
  month = sep,
  pages = {514--519},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00088},
  abstract = {Segmentation of complex document images remains a challenge due to the large variability of layout and image degradation. In this paper, we propose a method to segment complex document images based on Label Pyramid Network (LPN) and Deep Watershed Transform (DWT). The method can segment document images into instance aware regions including text lines, text regions, figures, tables, etc. The backbone of LPN can be any type of Fully Convolutional Networks (FCN), and in training, label map pyramids on training images are provided to exploit the hierarchical boundary information of regions efficiently through multi-task learning. The label map pyramid is transformed from region class label map by distance transformation and multi-level thresholding. In segmentation, the outputs of multiple tasks of LPN are summed into one single probability map, on which watershed transformation is carried out to segment the document image into instance aware regions. In experiments on four public databases, our method is demonstrated effective and superior, yielding state of the art performance for text line segmentation, baseline detection and region segmentation.},
  keywords = {Convolution,Databases,Deep learning,deep watershed transformation,document image segmentation,Image segmentation,instance segmentation,label pyramid network,Semantics,Task analysis,Training}
}

@article{liLayoutGANGeneratingGraphic2019,
  title = {{{LayoutGAN}}: {{Generating Graphic Layouts}} with {{Wireframe Discriminators}}},
  author = {Li, Jianan and Yang, Jimei and Hertzmann, Aaron and Zhang, Jianming and Xu, Tingfa},
  year = {2019},
  month = jan,
  journal = {7th International Conference on Learning Representations, ICLR 2019},
  eprint = {1901.06767},
  publisher = {International Conference on Learning Representations, ICLR},
  urldate = {2023-04-12},
  abstract = {Layout is important for graphic design and scene generation. We propose a novel Generative Adversarial Network, called LayoutGAN, that synthesizes layouts by modeling geometric relations of different types of 2D elements. The generator of LayoutGAN takes as input a set of randomly-placed 2D graphic elements and uses self-attention modules to refine their labels and geometric parameters jointly to produce a realistic layout. Accurate alignment is critical for good layouts. We thus propose a novel differentiable wireframe rendering layer that maps the generated layout to a wireframe image, upon which a CNN-based discriminator is used to optimize the layouts in image space. We validate the effectiveness of LayoutGAN in various experiments including MNIST digit generation, document layout generation, clipart abstract scene generation and tangram graphic design.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\XU9TV69Q\full-text.pdf}
}

@misc{linTeachingModelsExpress2022,
  title = {Teaching {{Models}} to {{Express Their Uncertainty}} in {{Words}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14334},
  eprint = {2205.14334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14334},
  urldate = {2025-02-06},
  abstract = {We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. "90\% confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{liTrOCRTransformerbasedOptical2021,
  title = {{{TrOCR}}: {{Transformer-based Optical Character Recognition}} with {{Pre-trained Models}}},
  author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
  year = {2021},
  month = sep,
  eprint = {2109.10282},
  urldate = {2023-04-10},
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at {\textbackslash}url\{https://aka.ms/trocr\}.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\3L94JYCX\\full-text.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\KDH4FVLT\\full-text.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\TDP8DBGZ\\full-text.pdf}
}

@article{Liu2018,
  title = {An {{Intriguing Failing}} of {{Convolutional Neural Networks}} and the {{CoordConv Solution}}},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  year = {2018},
  month = jul,
  eprint = {1807.03247},
  urldate = {2019-10-14},
  abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24\% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\6M445UW7\full-text.pdf}
}

@article{liuModelbasedStrokeExtraction2001,
  title = {Model-Based Stroke Extraction and Matching for Handwritten {{Chinese}} Character Recognition},
  author = {Liu, Cheng Lin and Kim, In Jung and Kim, Jin H.},
  year = {2001},
  month = dec,
  journal = {Pattern Recognition},
  volume = {34},
  number = {12},
  pages = {2339--2352},
  publisher = {Pergamon},
  issn = {00313203},
  doi = {10.1016/S0031-3203(00)00165-5},
  urldate = {2020-03-15},
  abstract = {This paper proposes a model-based structural matching method for handwritten Chinese character recognition (HCCR). This method is able to obtain reliable stroke correspondence and enable structural interpretation. In the model base, the reference character of each category is described in an attributed relational graph (ARG). The input character is described with feature points and line segments. The strokes and inter-stroke relations of input character are not determined until being matched with a reference character. The structural matching is accomplished in two stages: candidate stroke extraction and consistent matching. All candidate input strokes to match the reference strokes are extracted by line following and then the consistent matching is achieved by heuristic search. Some structural post- processing operations are applied to improve the stroke correspondence. Recognition experiments were implemented on an image database collected in KAIST, and promising results have been achieved. {\copyright} 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
  keywords = {Chinese character recognition,Heuristic search,Model-based stroke extraction,Semi-admissible search,Structural matching},
  file = {C:\Users\tarchibald\Zotero\storage\JE4DNFQA\full-text.pdf}
}

@misc{liuUncertaintyQuantificationConfidence2025,
  title = {Uncertainty {{Quantification}} and {{Confidence Calibration}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Uncertainty {{Quantification}} and {{Confidence Calibration}} in {{Large Language Models}}},
  author = {Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
  year = {2025},
  month = mar,
  number = {arXiv:2503.15850},
  eprint = {2503.15850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.15850},
  urldate = {2025-05-31},
  abstract = {Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\tarchibald\Zotero\storage\38GHL5Z9\Liu et al. - 2025 - Uncertainty Quantification and Confidence Calibrat.pdf}
}

@article{liuWeakAILikely2021,
  title = {"{{Weak AI}}" Is {{Likely}} to {{Never Become}} "{{Strong AI}}", {{So What}} Is Its {{Greatest Value}} for Us?},
  author = {Liu, Bin},
  year = {2021},
  month = mar,
  eprint = {2103.15294},
  doi = {10.48550/arxiv.2103.15294},
  urldate = {2022-03-18},
  abstract = {AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, "Starcraft" and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed "weak AI" and "strong AI" (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of "weak AI" if it has no chance to develop into "strong AI".},
  archiveprefix = {arXiv},
  keywords = {artificial general intelligence,deep learning,Index Terms Artificial intelligence,strong AI,weak AI},
  file = {C:\Users\tarchibald\Zotero\storage\HSVEDMJ2\full-text.pdf}
}

@inproceedings{Liwicki2005,
  title = {{{IAM-OnDB}} - {{An}} on-Line {{English}} Sentence Database Acquired from Handwritten Text on a Whiteboard},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Liwicki, Marcus and Bunke, Horst},
  year = {2005},
  volume = {2005},
  pages = {956--961},
  issn = {15205363},
  doi = {10.1109/ICDAR.2005.132},
  urldate = {2019-10-14},
  abstract = {In this paper we present IAM-OnDB - a new large online handwritten sentences database. It is publicly available and consists of text acquired via an electronic interface from a whiteboard. The database contains about 86 K word instances from an 11K dictionary written by more than 200 writers. We also describe a recognizer for unconstrained English text that was trained and tested using this database. This recognizer is based on Hidden Markov Models (HMMs). In our experiments we show that by using larger training sets we can significantly increase the word recognition rate. This recognizer may serve as a benchmark reference for future research. {\copyright} 2005 IEEE.},
  isbn = {0-7695-2420-6},
  file = {C:\Users\tarchibald\Zotero\storage\GE4CTN9S\full-text.pdf}
}

@article{lopesLearnedRepresentationScalable2019,
  title = {A {{Learned Representation}} for {{Scalable Vector Graphics}}},
  author = {Lopes, Raphael Gontijo and Ha, David and Eck, Douglas and Shlens, Jonathon},
  year = {2019},
  month = apr,
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2019-October},
  eprint = {1904.02632},
  pages = {7929--7938},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2020-06-11},
  abstract = {Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\9PVVY5SB\full-text.pdf}
}

@article{loprestiUsingConsensusSequence1997,
  title = {Using {{Consensus Sequence Voting}} to {{Correct OCR Errors}}},
  author = {Lopresti, Daniel and Zhou, Jiangying},
  year = {1997},
  month = jul,
  journal = {Computer Vision and Image Understanding},
  volume = {67},
  number = {1},
  pages = {39--47},
  issn = {1077-3142},
  doi = {10.1006/cviu.1996.0502},
  urldate = {2025-06-01},
  abstract = {We present experimental results suggesting that between 20 and 50\% of the errors caused by a single OCR package can be eliminated by simply scanning a page three times and running a ``consensus sequence'' voting procedure. This technique, which originates from molecular biology, takes exponential time in general, but can be specialized to a fast heuristic guaranteed to be optimal for the cases of interest. The improvement in recognition accuracy is achieved without makinga prioriassumptions about the distribution of OCR errors (i.e., no ``training'' is required).},
  file = {C:\Users\tarchibald\Zotero\storage\HDY75P46\S1077314296905020.html}
}

@inproceedings{loshchilovDecoupledWeightDecay2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = sep,
  urldate = {2024-05-10},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/AdamW-and-SGDW\}},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\8UQYKRMY\Loshchilov and Hutter - 2018 - Decoupled Weight Decay Regularization.pdf}
}

@misc{luBoundingBoxWorth2025,
  title = {A {{Bounding Box}} Is {{Worth One Token}}: {{Interleaving Layout}} and {{Text}} in a {{Large Language Model}} for {{Document Understanding}}},
  shorttitle = {A {{Bounding Box}} Is {{Worth One Token}}},
  author = {Lu, Jinghui and Yu, Haiyang and Wang, Yanjie and Ye, Yongjie and Tang, Jingqun and Yang, Ziwei and Wu, Binghong and Liu, Qi and Feng, Hao and Wang, Han and Liu, Hao and Huang, Can},
  year = {2025},
  month = may,
  number = {arXiv:2407.01976},
  eprint = {2407.01976},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01976},
  urldate = {2025-06-01},
  abstract = {Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)\vphantom\{\} for document understanding. LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in KIE and VQA. Comprehensive benchmark evaluations reveal significant improvements of LayTextLLM, with a 15.2\% increase on KIE tasks and 10.7\% on VQA tasks compared to previous SOTA OCR-based LLMs. All resources are available at https://github.com/LayTextLLM/LayTextLLM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multimedia},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\FXMBIQUP\\Lu et al. - 2025 - A Bounding Box is Worth One Token Interleaving La.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\R8ICZJ3W\\2407.html}
}

@article{luDeepLearningImage,
  title = {Deep {{Learning For Image Registration}}},
  author = {Lu, Yiping},
  urldate = {2023-04-12},
  abstract = {Image registration is an important task in computer vision and image processing and widely used in medical image and self-driving cars. In this paper, we reviewed popular method in deep learning for image registration, both supervised and unsupervised one.},
  file = {C:\Users\tarchibald\Zotero\storage\FE9IUQ6M\full-text.pdf}
}

@techreport{luViLBERTPretrainingTaskAgnostic,
  title = {{{ViLBERT}}: {{Pretraining Task-Agnostic Visiolinguistic Representations}} for {{Vision-and-Language Tasks}}},
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  urldate = {2021-02-26},
  abstract = {We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks-visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval-by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models-achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.},
  file = {C:\Users\tarchibald\Zotero\storage\WCX36JAI\full-text.pdf}
}

@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  urldate = {2023-08-25},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {C:\Users\tarchibald\Zotero\storage\772TP5EA\Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@article{madasuWhatLargeLanguage,
  title = {What Do {{Large Language Models Learn}} beyond {{Language}}?},
  author = {Madasu, Avinash and Srivastava, Shashank},
  eprint = {2210.12302v1},
  urldate = {2022-10-26},
  abstract = {Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful 'inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outper-form comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multilingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models 1 .},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\GPGNTD3S\full-text.pdf}
}

@article{manguFindingConsensusSpeech2000,
  title = {Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Networks},
  shorttitle = {Finding Consensus in Speech Recognition},
  author = {Mangu, L. and Brill, E. and Stolcke, A.},
  year = {2000},
  month = oct,
  journal = {Computer Speech \& Language},
  volume = {14},
  number = {4},
  eprint = {cs/0010012},
  pages = {373--400},
  issn = {08852308},
  doi = {10.1006/csla.2000.0152},
  urldate = {2025-06-01},
  abstract = {We describe a new framework for distilling information from word lattices to improve the accuracy of speech recognition and obtain a more perspicuous representation of a set of alternative hypotheses. In the standard MAP decoding approach the recognizer outputs the string of words corresponding to the path with the highest posterior probability given the acoustics and a language model. However, even given optimal models, the MAP decoder does not necessarily minimize the commonly used performance metric, word error rate (WER). We describe a method for explicitly minimizing WER by extracting word hypotheses with the highest posterior probabilities from word lattices. We change the standard problem formulation by replacing global search over a large set of sentence hypotheses with local search over a small set of word candidates. In addition to improving the accuracy of the recognizer, our method produces a new representation of the set of candidate hypotheses that specifies the sequence of word-level confusions in a compact lattice format. We study the properties of confusion networks and examine their use for other tasks, such as lattice compression, word spotting, confidence annotation, and reevaluation of recognition hypotheses using higher-level knowledge sources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\FYIK7KXW\\Mangu et al. - 2000 - Finding consensus in speech recognition word erro.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\4DBN2DZU\\0010012.html}
}

@article{maoInterpolatedConvolutionalNetworks2019,
  title = {Interpolated {{Convolutional Networks}} for {{3D Point Cloud Understanding}}},
  author = {Mao, Jiageng and Wang, Xiaogang and Li, Hongsheng},
  year = {2019},
  month = aug,
  eprint = {1908.04512},
  urldate = {2019-12-05},
  abstract = {Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\PU9QQYVP\full-text.pdf}
}

@book{Marsland,
  title = {Machine {{Learning}}: {{An Algorithmic Perspective}}},
  author = {Marsland, Stephen},
  file = {C:\Users\tarchibald\Zotero\storage\SCE24LGF\CS 478 Machine Learning_ An Algorithmic Perspective (2nd ed.) [Marsland 2014-10-08].pdf}
}

@techreport{martiIAMdatabaseEnglishSentence2002,
  title = {The {{IAM-database}}: An {{English}} Sentence Database for Offline Handwriting Recognition},
  author = {Marti, U.-V and Bunke, H},
  year = {2002},
  volume = {5},
  pages = {39--46},
  urldate = {2020-07-23},
  abstract = {In this paper we describe a database that consists of handwritten English sentences. It is based on the Lancaster-Oslo/Bergen (LOB) corpus. This corpus is a collection of texts that comprise about one million word instances. The database includes 1,066 forms produced by approximately 400 different writers. A total of 82,227 word instances out of a vocabulary of 10,841 words occur in the collection. The database consists of full English sentences. It can serve as a basis for a variety of handwriting recognition tasks. However, it is expected that the database would be particularly useful for recognition tasks where linguistic knowledge beyond the lexicon level is used, because this knowledge can be automatically derived from the underlying corpus. The database also includes a few image-processing procedures for extracting the handwritten text from the forms and the segmenta-tion of the text into lines and words.},
  keywords = {Corpus-,Database-,English sentences-,Handwriting recognition-,Linguistic knowl-edge,Un-constrained},
  file = {C:\Users\tarchibald\Zotero\storage\XJGMVBRE\full-text.pdf}
}

@misc{MatchingHandwrittenDocument,
  title = {Matching {{Handwritten Document Images}}},
  urldate = {2019-09-24},
  howpublished = {https://cvit.iiit.ac.in/research/projects/cvit-projects/matchdocimgs}
}

@misc{maTestTimeGenerativeAugmentation2024,
  title = {Test-{{Time Generative Augmentation}} for {{Medical Image Segmentation}}},
  author = {Ma, Xiao and Tao, Yuhui and Zhang, Yuhan and Ji, Zexuan and Zhang, Yizhe and Chen, Qiang},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17608},
  eprint = {2406.17608},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.17608},
  urldate = {2025-06-03},
  abstract = {In this paper, we propose a novel approach to enhance medical image segmentation during test time. Instead of employing hand-crafted transforms or functions on the input test image to create multiple views for test-time augmentation, we advocate for the utilization of an advanced domain-fine-tuned generative model (GM), e.g., stable diffusion (SD), for test-time augmentation. Given that the GM has been trained to comprehend and encapsulate comprehensive domain data knowledge, it is superior than segmentation models in terms of representing the data characteristics and distribution. Hence, by integrating the GM into test-time augmentation, we can effectively generate multiple views of a given test sample, aligning with the content and appearance characteristics of the sample and the related local data distribution. This approach renders the augmentation process more adaptable and resilient compared to conventional handcrafted transforms. Comprehensive experiments conducted across three medical image segmentation tasks (nine datasets) demonstrate the efficacy and versatility of the proposed TTGA in enhancing segmentation outcomes. Moreover, TTGA significantly improves pixel-wise error estimation, thereby facilitating the deployment of a more reliable segmentation system. Code will be released at: https://github.com/maxiao0234/TTGA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\7WE9Y6A4\\Ma et al. - 2024 - Test-Time Generative Augmentation for Medical Imag.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\TKH3HQ6W\\2406.html}
}

@article{mathewDocVQADatasetVQA2020,
  title = {{{DocVQA}}: {{A Dataset}} for {{VQA}} on {{Document Images}}},
  author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, C. V.},
  year = {2020},
  month = jul,
  journal = {Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
  eprint = {2007.00398},
  pages = {2199--2208},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.48550/arxiv.2007.00398},
  urldate = {2022-05-27},
  abstract = {We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36\% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  archiveprefix = {arXiv},
  isbn = {9780738142661},
  file = {C:\Users\tarchibald\Zotero\storage\287L3IF5\full-text.pdf}
}

@article{mathewDocVQADatasetVQA2020a,
  title = {{{DocVQA}}: {{A Dataset}} for {{VQA}} on {{Document Images}}},
  author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, C. V.},
  year = {2020},
  month = jul,
  journal = {Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
  eprint = {2007.00398},
  pages = {2199--2208},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.48550/arxiv.2007.00398},
  urldate = {2022-06-04},
  abstract = {We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36\% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  archiveprefix = {arXiv},
  isbn = {9780738142661}
}

@article{mathewDocVQADatasetVQA2020b,
  title = {{{DocVQA}}: {{A Dataset}} for {{VQA}} on {{Document Images}}},
  author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, C. V.},
  year = {2020},
  month = jul,
  journal = {Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
  eprint = {2007.00398},
  pages = {2199--2208},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.48550/arxiv.2007.00398},
  urldate = {2022-06-04},
  abstract = {We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36\% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org},
  archiveprefix = {arXiv},
  isbn = {9780738142661}
}

@misc{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  number = {arXiv:1802.03426},
  eprint = {1802.03426},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.03426},
  urldate = {2023-08-17},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\EXDJJ6KK\\McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\9K23W5GS\\1802.html}
}

@article{Melo2019,
  title = {Deep Learning Approach to Generate Offline Handwritten Signatures Based on Online Samples},
  author = {Melo, Victor K.S.L. and Bezerra, Byron Leite Dantas and Impedovo, Donato and Pirlo, Giuseppe and Lundgren, Antonio},
  year = {2019},
  month = may,
  journal = {IET Biometrics},
  volume = {8},
  number = {3},
  pages = {215--220},
  publisher = {{Institution of Engineering and Technology}},
  issn = {20474946},
  doi = {10.1049/iet-bmt.2018.5091},
  urldate = {2019-10-05},
  abstract = {Reduced training sets is a major problem typically found on the task of offline signature verification. In order to increase the number of samples, the use of synthetic signatures can be taken into account. In this work, a new method for the generation of synthetic offline signatures by using dynamic and static (real) ones is presented. The synthesis is here faced under the perspective of supervised training: the learning model is trained to perform the task of online to offline signature conversion. The approach is based on a Deep Convolutional Neural Network. The main goal is to enlarge offline training dataset in order to improve the performance of the offline signature verification systems. For this purpose, a machine-oriented evaluation on the BiosecurID signature dataset is carried out. The use of synthetic samples (in the training phase) generated with the proposed method on a state-of-the-art classification system exhibits performance similar to those obtained using real signatures, moreover the combination of real and synthetic signature in the training set is also able to show improvements of the Equal Error Rate.}
}

@techreport{Michael,
  title = {Evaluating {{Sequence-to-Sequence Models}} for {{Handwritten Text Recognition}}},
  author = {Michael, Johannes and Labahn, Roger and Gr{\"u}ning, Tobias and Z{\"o}llner, Jochen},
  year = {2019},
  journal = {arXiv preprint arXiv:1903.07377},
  eprint = {1903.07377},
  urldate = {2019-09-23},
  abstract = {Encoder-decoder models have become an effective approach for sequence learning tasks like machine translation, image captioning and speech recognition, but have yet to show competitive results for handwritten text recognition. To this end, we propose an attention-based sequence-to-sequence model. It combines a convolutional neural network as a generic feature extractor with a recurrent neural network to encode both the visual information, as well as the temporal context between characters in the input image, and uses a separate recurrent neural network to decode the actual character sequence. We make experimental comparisons between various attention mechanisms and positional encodings, in order to find an appropriate alignment between the input and output sequence. The model can be trained end-to-end and the optional integration of a hybrid loss allows the encoder to retain an interpretable and usable output, if desired. We achieve competitive results on the IAM and ICFHR2016 READ data sets compared to the state-of-the-art without the use of a language model, and we significantly improve over any recent sequence-to-sequence approaches.},
  archiveprefix = {arXiv},
  keywords = {,attention,encoder-decoder,handwritten text recognition,HTR,Seq2Seq,sequence-to-sequence},
  file = {C:\Users\tarchibald\Zotero\storage\QLKNIAXW\Michael et al. - Unknown - Evaluating Sequence-to-Sequence Models for Handwritten Text Recognition.pdf}
}

@misc{mielkeWordsCharactersBrief2021,
  title = {Between Words and Characters: {{A Brief History}} of {{Open-Vocabulary Modeling}} and {{Tokenization}} in {{NLP}}},
  shorttitle = {Between Words and Characters},
  author = {Mielke, Sabrina J. and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y. and Sagot, Beno{\^i}t and Tan, Samson},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10508},
  eprint = {2112.10508},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10508},
  urldate = {2025-03-16},
  abstract = {What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subword-based approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\57HQNA9E\\Mielke et al. - 2021 - Between words and characters A Brief History of O.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\EXRF7SWL\\2112.html}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-08-16},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
  file = {C:\Users\tarchibald\Zotero\storage\PGJ4IKQ9\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@techreport{milzVisualSLAMAutomated,
  title = {Visual {{SLAM}} for {{Automated Driving}}: {{Exploring}} the {{Applications}} of {{Deep Learning}}},
  author = {Milz, Stefan and Arbeiter, Georg and Witt, Christian and Yogamani, Senthil},
  urldate = {2020-11-22},
  abstract = {Deep learning has become the standard model for object detection and recognition. Recently, there is progress on using CNN models for geometric vision tasks like depth estimation, optical flow prediction or motion segmentation. However, Visual SLAM remains to be one of the areas of automated driving where CNNs are not mature for deployment in commercial automated driving systems. In this paper, we explore how deep learning can be used to replace parts of the classical Visual SLAM pipeline. Firstly, we describe the building blocks of Visual SLAM pipeline composed of standard geometric vision tasks. Then we provide an overview of Visual SLAM use cases for automated driving based on the authors' experience in commercial deployment. Finally, we discuss the opportunities of using Deep Learning to improve upon state-of-the-art classical methods.},
  file = {C:\Users\tarchibald\Zotero\storage\3BQG46D9\full-text.pdf}
}

@article{mitchellDebateUnderstandingAI2022,
  title = {The {{Debate Over Understanding}} in {{AI}}'s {{Large Language Models}}},
  author = {Mitchell, Melanie and Krakauer, David C.},
  year = {2022},
  month = oct,
  eprint = {2210.13966},
  doi = {10.48550/arxiv.2210.13966},
  urldate = {2022-10-26},
  abstract = {We survey a current, heated debate in the AI research community on whether large pre-trained language models can be said to "understand" language -- and the physical and social situations language encodes -- in any important sense. We describe arguments that have been made for and against such understanding, and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that a new science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\QBEN99NA\full-text.pdf}
}

@misc{mohamedyousefOrigamiNetWeaklySupervisedSegmentationFree,
  title = {{{OrigamiNet}}: {{Weakly-Supervised}}, {{Segmentation-Free}}, {{One-Step}}, {{Full Page Text Recognition}} by Learning to Unfold},
  author = {Mohamed Yousef, Tom E. Bishop},
  urldate = {2022-03-10},
  howpublished = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Yousef\_OrigamiNet\_Weakly-Supervised\_Segmentation-Free\_One-Step\_Full\_Page\_Text\_Recognition\_by\_learning\_CVPR\_2020\_paper.html}
}

@misc{MomentMatchingMultiSource,
  title = {Moment {{Matching}} for {{Multi-Source Domain Adaptation}}},
  urldate = {2023-09-01},
  howpublished = {http://ai.bu.edu/DomainNet/},
  file = {C:\Users\tarchibald\Zotero\storage\GCU8WCXH\DomainNet.html}
}

@techreport{munkhdalaiRapidAdaptationConditionally2018,
  title = {Rapid {{Adaptation}} with {{Conditionally Shifted Neurons}}},
  author = {Munkhdalai, Tsendsuren and Yuan, Xingdi and Mehri, Soroush and Trischler, Adam},
  year = {2018},
  month = jul,
  pages = {3664--3673},
  institution = {PMLR},
  issn = {2640-3498},
  urldate = {2020-12-24},
  abstract = {We describe a mechanism by which artificial neu-ral networks can learn rapid adaptation-the ability to adapt on the fly, with little data, to new tasks-that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearn-ing benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.},
  file = {C:\Users\tarchibald\Zotero\storage\8W9NR8B4\full-text.pdf}
}

@article{natekarDemystifyingBrainTumor2020,
  title = {Demystifying {{Brain Tumor Segmentation Networks}}: {{Interpretability}} and {{Uncertainty Analysis}}},
  author = {Natekar, Parth and Kori, Avinash and Krishnamurthi, Ganapathy},
  year = {2020},
  journal = {Frontiers in Computational Neuroscience},
  volume = {14},
  issn = {1662-5188},
  doi = {10.3389/fncom.2020.00006},
  abstract = {{$<$}p{$>$}The accurate automatic segmentation of gliomas and its intra-tumoral structures is important not only for treatment planning but also for follow-up evaluations. Several methods based on 2D and 3D Deep Neural Networks (DNN) have been developed to segment brain tumors and to classify different categories of tumors from different MRI modalities. However, these networks are often black-box models and do not provide any evidence regarding the process they take to perform this task. Increasing transparency and interpretability of such deep learning techniques is necessary for the complete integration of such methods into medical practice. In this paper, we explore various techniques to explain the functional organization of brain tumor segmentation models and to extract visualizations of internal concepts to understand how these networks achieve highly accurate tumor segmentations. We use the BraTS 2018 dataset to train three different networks with standard architectures and outline similarities and differences in the process that these networks take to segment brain tumors. We show that brain tumor segmentation networks learn certain human-understandable disentangled concepts on a filter level. We also show that they take a top-down or hierarchical approach to localizing the different parts of the tumor. We then extract visualizations of some internal feature maps and also provide a measure of uncertainty with regards to the outputs of the models to give additional qualitative evidence about the predictions of these networks. We believe that the emergence of such human-understandable organization and concepts might aid in the acceptance and integration of such methods in medical diagnosis.{$<$}/p{$>$}}
}

@article{needlemanGeneralMethodApplicable1970,
  title = {A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins},
  author = {Needleman, Saul B. and Wunsch, Christian D.},
  year = {1970},
  month = mar,
  journal = {Journal of Molecular Biology},
  volume = {48},
  number = {3},
  pages = {443--453},
  issn = {0022-2836},
  doi = {10.1016/0022-2836(70)90057-4},
  urldate = {2025-06-01},
  abstract = {A computer adaptable method for finding similarities in the amino acid sequences of two proteins has been developed. From these findings it is possible to determine whether significant homology exists between the proteins. This information is used to trace their possible evolutionary development. The maximum match is a number dependent upon the similarity of the sequences. One of its definitions is the largest number of amino acids of one protein that can be matched with those of a second protein allowing for all possible interruptions in either of the sequences. While the interruptions give rise to a very large number of comparisons, the method efficiently excludes from consideration those comparisons that cannot contribute to the maximum match. Comparisons are made from the smallest unit of significance, a pair of amino acids, one from each protein. All possible pairs are represented by a two-dimensional array, and all possible comparisons are represented by pathways through the array. For this maximum match only certain of the possible pathways must be evaluated. A numerical value, one in this case, is assigned to every cell in the array representing like amino acids. The maximum match is the largest number that would result from summing the cell values of every pathway.},
  file = {C:\Users\tarchibald\Zotero\storage\U7I2BANR\0022283670900574.html}
}

@article{neftciEventDrivenRandomBackPropagation2017,
  title = {Event-{{Driven Random Back-Propagation}}: {{Enabling Neuromorphic Deep Learning Machines}}},
  author = {Neftci, Emre O. and Augustine, Charles and Paul, Somnath and Detorakis, Georgios},
  year = {2017},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {11},
  number = {JUN},
  pages = {324},
  publisher = {Frontiers Media S.A.},
  issn = {1662-453X},
  doi = {10.3389/fnins.2017.00324},
  urldate = {2020-12-24},
  abstract = {An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. One increasingly popular and successful approach is to take inspiration from inference and learning algorithms used in deep neural networks. However, the workhorse of deep learning, the gradient descent Gradient Back Propagation (BP) rule, often relies on the immediate availability of network-wide information stored with high-precision memory during learning, and precise operations that are difficult to realize in neuromorphic hardware. Remarkably, recent work showed that exact backpropagated gradients are not essential for learning deep representations. Building on these results, we demonstrate an event-driven random BP (eRBP) rule that uses an error-modulated synaptic plasticity for learning deep representations. Using a two-compartment Leaky Integrate \& Fire (I\&F) neuron, the rule requires only one addition and two comparisons for each synaptic weight, making it very suitable for implementation in digital or mixed-signal neuromorphic hardware. Our results show that using eRBP, deep representations are rapidly learned, achieving classification accuracies on permutation invariant datasets comparable to those obtained in artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning.},
  keywords = {Backpropagation algorithm,Embedded cognition,Feedback alignment,Spiking neural networks,Stochastic processes},
  file = {C:\Users\tarchibald\Zotero\storage\XRDG3GIL\full-text.pdf}
}

@article{neftciSurrogateGradientLearning2019,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  year = {2019},
  month = jan,
  journal = {arXiv},
  eprint = {1901.09948},
  publisher = {arXiv},
  urldate = {2020-12-24},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\8LIRS9ZY\full-text.pdf}
}

@article{nejiBlur2sharpGanbasedModel2021,
  title = {Blur2sharp: {{A}} Gan-Based Model for Document Image Deblurring},
  author = {Neji, Hala and Halima, Mohamed Ben and Hamdani, Tarek M. and {Nogueras-Iso}, Javier and Alimi, Adel M.},
  year = {2021},
  journal = {Int. j. comput. intell. syst.},
  volume = {14},
  number = {1},
  pages = {1315--1321},
  publisher = {Atlantis Press},
  issn = {18756883},
  doi = {10.2991/IJCIS.D.210407.001},
  urldate = {2023-08-15},
  abstract = {The advances in mobile technology and portable cameras have facilitated enormously the acquisition of text images. However, the blur caused by camera shake or out-of-focus problems may affect the quality of acquired images and their use as input for optical character recognition (OCR) or other types of document processing. This work proposes an end-to-end model for document deblurring using cycle-consistent adversarial networks. The main novelty of this work is to achieve blind document deblurring, i.e., deblurring without knowledge of the blur kernel. Our method, named ``Blur2Sharp CycleGAN,'' generates a sharp image from a blurry one and shows how cycle-consistent generative adversarial networks (CycleGAN) can be used in document deblurring. Using only a blurred image as input, we try to generate the sharp image. Thus, no information about the blur kernel is required. In the evaluation part, we use peak signal to noise ratio (PSNR) and structural similarity index (SSIM) to compare the deblurring images. The experiments demonstrate a clear improvement in visual quality with respect to the state-of-the-art using a dataset of text images.},
  keywords = {Articles,Generative adversarial network (GAN),Motion blur,Out-of-focus blur,Universidad de Zaragoza Repository,WebSearch},
  file = {C:\Users\tarchibald\Zotero\storage\MT8WIMPU\full-text.pdf}
}

@misc{Nel2005,
  title = {Estimating the Pen Trajectories of Static Signatures Using Hidden {{Markov}} Models},
  author = {Nel, Emli Mari and {du Preez}, Johan A. and Herbst, B. M.},
  year = {2005},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {27},
  number = {11},
  pages = {1733--1746},
  publisher = {IEEE},
  issn = {01628828},
  doi = {10.1109/TPAMI.2005.221},
  urldate = {2019-10-08},
  abstract = {Static signatures originate as handwritten images on documents and by definition do not contain any dynamic information. This lack of information makes static signature verification systems significantly less reliable than their dynamic counterparts. This study involves extracting dynamic information from static images, specifically the pen trajectory while the signature was created. We assume that a dynamic version of the static image is available (typically obtained during an earlier registration process). We then derive a hidden Markov model from the static image and match it to the dynamic version of the image. This match results in the estimated pen trajectory of the static image. {\copyright} 2005 IEEE.},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=1512054},
  keywords = {Document analysis,Document and text processing,Handwriting analysis,Pattern recognition},
  file = {C:\Users\tarchibald\Zotero\storage\EKSABWCJ\Nel, du Preez, Herbst - 2005 - Estimating the pen trajectories of static signatures using hidden Markov models.pdf}
}

@inproceedings{nelEstimatingPenTrajectories2005,
  title = {Estimating the Pen Trajectories of Multi-Path Static Scripts Using {{Hidden Markov Models}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Nel, E. and Du Preez, J. A. and Herbst, B. M.},
  year = {2005},
  volume = {2005},
  pages = {41--45},
  issn = {15205363},
  doi = {10.1109/ICDAR.2005.106},
  urldate = {2020-03-15},
  abstract = {Static handwritten scripts are available only as images on documents and by definition do not contain dynamic information. This study is about extracting dynamic information from a static handwritten script, specifically the sequence of pen positions that created the script. We assume that a dynamic representative of the static image is available (a different version typically obtained during an earlier registration process). A Hidden Markov Model (HMM) of the static image is compared with the dynamic representative to extract the dynamic information from the static image. {\copyright} 2005 IEEE.},
  isbn = {0-7695-2420-6},
  file = {C:\Users\tarchibald\Zotero\storage\JWDE5SGA\full-text.pdf}
}

@inproceedings{newellPixelsGraphsAssociative2017,
  title = {Pixels to {{Graphs}} by {{Associative Embedding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Newell, Alejandro and Deng, Jia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-08-25},
  abstract = {Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph definition. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and demonstrate state-of-the-art performance on the challenging task of scene graph generation.},
  file = {C:\Users\tarchibald\Zotero\storage\U7SFU43A\Newell and Deng - 2017 - Pixels to Graphs by Associative Embedding.pdf}
}

@inproceedings{Nguyen2010,
  title = {Techniques for Static Handwriting Trajectory Recovery},
  booktitle = {Proceedings of the 8th {{IAPR International Workshop}} on {{Document Analysis Systems}} - {{DAS}} '10},
  author = {Nguyen, Vu and Blumenstein, Michael},
  year = {2010},
  pages = {463--470},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/1815330.1815390},
  urldate = {2019-10-12},
  isbn = {978-1-60558-773-8},
  keywords = {handwriting tracing,off-line recognition,trajectory recovery},
  file = {C:\Users\tarchibald\Zotero\storage\4LG3J4AK\full-text.pdf}
}

@article{nguyenSurveyPostOCRProcessing2021,
  title = {Survey of {{Post-OCR Processing Approaches}}},
  author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Coustaty, Mickael and Doucet, Antoine},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {124:1--124:37},
  issn = {0360-0300},
  doi = {10.1145/3453476},
  urldate = {2022-12-07},
  abstract = {Optical character recognition (OCR) is one of the most popular techniques used for converting printed documents into machine-readable ones. While OCR engines can do well with modern text, their performance is unfortunately significantly reduced on historical materials. Additionally, many texts have already been processed by various out-of-date digitisation techniques. As a consequence, digitised texts are noisy and need to be post-corrected. This article clarifies the importance of enhancing quality of OCR results by studying their effects on information retrieval and natural language processing applications. We then define the post-OCR processing problem, illustrate its typical pipeline, and review the state-of-the-art post-OCR processing approaches. Evaluation metrics, accessible datasets, language resources, and useful toolkits are also reported. Furthermore, the work identifies the current trend and outlines some research directions of this field.},
  keywords = {error model,language model,machine learning,OCR merging,Post-OCR processing,statistical and neural machine translation},
  file = {C:\Users\tarchibald\Zotero\storage\HLSAMPDG\Nguyen et al. - 2021 - Survey of Post-OCR Processing Approaches.pdf}
}

@article{NoTitle,
  title = {No {{Title}}}
}

@article{NoTitlea,
  title = {No {{Title}}}
}

@misc{NoTitleb,
  title = {({{No Title}})},
  urldate = {2021-05-25}
}

@inproceedings{noubighSurveyHandwritingRecognition2017,
  title = {A Survey on Handwriting Recognition Based on the Trajectory Recovery Technique},
  author = {Noubigh, Zouhaira and Kherallah, Monji},
  year = {2017},
  month = oct,
  pages = {69--73},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/asar.2017.8067762},
  urldate = {2020-02-27},
  abstract = {---The success of online recognition systems leads to improving the performance of offline recognition systems using the recovery of dynamic information. Therefore, it appears the techniques that extract dynamic information from static text. Several techniques have been proposed in the field of trajectory recovery. The principle of these techniques is to find an oriented path similar to that used by the writer in the writing operation. Many works have proved that the results of handwriting recognition systems can be improved if the temporal order of original trajectory will be restored correctly. The aim of this study is as follow. Firstly, we introduce the principle of the trajectory recovery approach and the basic concepts of its processes. Secondly, we give a bibliography of trajectory recovery-based handwriting recognition system. We consider our work as the first survey that focuses on trajectory recovery-based handwriting recognition system.},
  file = {C:\Users\tarchibald\Zotero\storage\KXNP9QJH\full-text.pdf}
}

@misc{ObjectiveEvaluationMethodologyBinarization,
  title = {An {{Objective Evaluation Methodology}} for {{Document Image Binarization Techniques}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-02-19},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/4669964},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\L58S2XUE\\An Objective Evaluation Methodology for Document I.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\B3JJVDHB\\4669964.html}
}

@misc{Ocrodeg2023,
  title = {Ocrodeg},
  year = {2023},
  month = jun,
  urldate = {2023-08-22},
  abstract = {document image degradation},
  howpublished = {NVIDIA Research Projects}
}

@article{Odaibo2019,
  title = {Tutorial: {{Deriving}} the {{Standard Variational Autoencoder}} ({{VAE}}) {{Loss Function}}},
  author = {Odaibo, Stephen},
  year = {2019},
  month = jul,
  eprint = {1907.08956},
  urldate = {2019-11-08},
  abstract = {In Bayesian machine learning, the posterior distribution is typically computationally intractable, hence variational inference is often required. In this approach, an evidence lower bound on the log likelihood of data is maximized during training. Variational Autoencoders (VAE) are one important example where variational inference is utilized. In this tutorial, we derive the variational lower bound loss function of the standard variational autoencoder. We do so in the instance of a gaussian latent prior and gaussian approximate posterior, under which assumptions the Kullback-Leibler term in the variational lower bound has a closed form solution. We derive essentially everything we use along the way; everything from Bayes' theorem to the Kullback-Leibler divergence.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\A4UGECM2\\Odaibo - 2019 - Tutorial Deriving the Standard Variational Autoencoder (VAE) Loss Function.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\LCMNG8VJ\\Odaibo - 2019 - Tutorial Deriving the Standard Variational Autoencoder (VAE) Loss Function(2).pdf}
}

@article{omurcaDocumentImageClassification2023,
  title = {A Document Image Classification System Fusing Deep and Machine Learning Models},
  author = {Omurca, Sevin{\c c} {\.I}lhan and Ekinci, Ekin and Sevim, Semih and Edin{\c c}, Eren Berk and Eken, S{\"u}leyman and Sayar, Ahmet},
  year = {2023},
  month = jun,
  journal = {Applied Intelligence},
  volume = {53},
  number = {12},
  pages = {15295--15310},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-04306-5},
  urldate = {2023-08-17},
  abstract = {Artificial Intelligence (AI) technologies are now widely employed to overcome human-induced faults in a variety of systems used in our daily lives, thanks to the digital transformation.One example of such systems is online document tracking systems (DTS). The DTS's reliability and preferability are enhanced by automatic document classification and understanding features. Although automatic document classification systems can assist humans in document understanding tasks, most of of them are not designed to function with Portable Document Format (PDF), which contains text, tables or figures. In this study, we investigate separate ways to efficiently classify student documents that are uploaded in PDF format and are required for university education. We propose three possible techniques for this issue. The first approach is based on Optical Character Recognition (OCR) and traditional machine learning methods. The second is purely on deep learning. The third one is based on fusion of deep learning methods based on entropy. The proposed techniques can classify twelve distinct types of digital documents. The validity of the proposed methods has been verified by student affairs department of Kocaeli University in Turkey. The system has not only increased the efficiency of online document uploading steps for students, but also reduced the human cost for tracking the documents. The highest F-score (94.45\%) is obtained by the ensemble of EfficientNetB3 and ExtraTree.},
  langid = {english},
  keywords = {Deep learning,Document image classification,Document understanding,Ensemble learning,Machine learning},
  file = {C:\Users\tarchibald\Zotero\storage\PDTEMEE7\Omurca et al. - 2023 - A document image classification system fusing deep.pdf}
}

@inproceedings{oquabLearningTransferringMidlevel2014,
  title = {Learning and {{Transferring Mid-level Image Representations Using Convolutional Neural Networks}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  year = {2014},
  month = jun,
  pages = {1717--1724},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.222},
  abstract = {Convolutional neural networks (CNN) have recently shown outstanding image classification performance in the large- scale visual recognition challenge (ILSVRC2012). The success of CNNs is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification methods. Learning CNNs, however, amounts to estimating millions of parameters and requires a very large number of annotated image samples. This property currently prevents application of CNNs to problems with limited training data. In this work we show how image representations learned with CNNs on large-scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data. We design a method to reuse layers trained on the ImageNet dataset to compute mid-level image representation for images in the PASCAL VOC dataset. We show that despite differences in image statistics and tasks in the two datasets, the transferred representation leads to significantly improved results for object and action classification, outperforming the current state of the art on Pascal VOC 2007 and 2012 datasets. We also show promising results for object and action localization.},
  keywords = {Computer vision,Image recognition,Image representation,Neural networks,Training,Training data,Visualization},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\HCHL5925\\Oquab et al. - 2014 - Learning and Transferring Mid-level Image Represen.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\2JXCV5YI\\6909618.html}
}

@misc{paliwalTableNetDeepLearning2020,
  title = {{{TableNet}}: {{Deep Learning}} Model for End-to-End {{Table}} Detection and {{Tabular}} Data Extraction from {{Scanned Document Images}}},
  shorttitle = {{{TableNet}}},
  author = {Paliwal, Shubham and D, Vishwanath and Rahul, Rohit and Sharma, Monika and Vig, Lovekesh},
  year = {2020},
  month = jan,
  number = {arXiv:2001.01469},
  eprint = {2001.01469},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.01469},
  urldate = {2023-08-18},
  abstract = {With the widespread use of mobile phones and scanners to photograph and upload documents, the need for extracting the information trapped in unstructured document images such as retail receipts, insurance claim forms and financial invoices is becoming more acute. A major hurdle to this objective is that these images often contain information in the form of tables and extracting data from tabular sub-images presents a unique set of challenges. This includes accurate detection of the tabular region within an image, and subsequently detecting and extracting information from the rows and columns of the detected table. While some progress has been made in table detection, extracting the table contents is still a challenge since this involves more fine grained table structure(rows \& columns) recognition. Prior approaches have attempted to solve the table detection and structure recognition problems independently using two separate models. In this paper, we propose TableNet: a novel end-to-end deep learning model for both table detection and structure recognition. The model exploits the interdependence between the twin tasks of table detection and table structure recognition to segment out the table and column regions. This is followed by semantic rule-based row extraction from the identified tabular sub-regions. The proposed model and extraction approach was evaluated on the publicly available ICDAR 2013 and Marmot Table datasets obtaining state of the art results. Additionally, we demonstrate that feeding additional semantic features further improves model performance and that the model exhibits transfer learning across datasets. Another contribution of this paper is to provide additional table structure annotations for the Marmot data, which currently only has annotations for table detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\R5NVF6NH\\Paliwal et al. - 2020 - TableNet Deep Learning model for end-to-end Table.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\SIF3W9JH\\2001.html}
}

@article{papamakariosNormalizingFlowsProbabilistic2021,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  pages = {1--64},
  urldate = {2022-03-18},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions , only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  keywords = {generative models,invertible neural networks,normalizing flows,proba-bilistic inference,probabilistic modeling},
  file = {C:\Users\tarchibald\Zotero\storage\964VTPZS\full-text.pdf}
}

@misc{PaperPageResLoRA2024,
  title = {Paper Page - {{ResLoRA}}: {{Identity Residual Mapping}} in {{Low-Rank Adaption}}},
  shorttitle = {Paper Page - {{ResLoRA}}},
  year = {2024},
  month = mar,
  urldate = {2024-03-01},
  abstract = {Join the discussion on this paper page},
  howpublished = {https://huggingface.co/papers/2402.18039},
  file = {C:\Users\tarchibald\Zotero\storage\SXNGQJUT\2024 - Paper page - ResLoRA Identity Residual Mapping in.html}
}

@misc{parkHierarchicalVisualFeature2024,
  title = {Hierarchical {{Visual Feature Aggregation}} for {{OCR-Free Document Understanding}}},
  author = {Park, Jaeyoo and Choi, Jin Young and Park, Jeonghyung and Han, Bohyung},
  year = {2024},
  month = nov,
  number = {arXiv:2411.05254},
  eprint = {2411.05254},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.05254},
  urldate = {2025-06-01},
  abstract = {We present a novel OCR-free document understanding framework based on pretrained Multimodal Large Language Models (MLLMs). Our approach employs multi-scale visual features to effectively handle various font sizes within document images. To address the increasing costs of considering the multi-scale visual inputs for MLLMs, we propose the Hierarchical Visual Feature Aggregation (HVFA) module, designed to reduce the number of input tokens to LLMs. Leveraging a feature pyramid with cross-attentive pooling, our approach effectively manages the trade-off between information loss and efficiency without being affected by varying document image sizes. Furthermore, we introduce a novel instruction tuning task, which facilitates the model's text-reading capability by learning to predict the relative positions of input text, eventually minimizing the risk of truncated text caused by the limited capacity of LLMs. Comprehensive experiments validate the effectiveness of our approach, demonstrating superior performance in various document understanding tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\YLRFUD39\\Park et al. - 2024 - Hierarchical Visual Feature Aggregation for OCR-Fr.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\RF7NE9FX\\2411.html}
}

@article{Parr2018,
  title = {The {{Matrix Calculus You Need For Deep Learning}}},
  author = {Parr, Terence and Howard, Jeremy},
  year = {2018},
  month = feb,
  eprint = {1802.01528},
  abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\Z6F54AA9\Matrix Calculus for Deep Learning.pdf}
}

@article{pawitanConfidenceReasoningLarge2025,
  title = {Confidence in the {{Reasoning}} of {{Large Language Models}}},
  author = {Pawitan, Yudi and Holmes, Chris},
  year = {2025},
  month = jan,
  journal = {Harvard Data Science Review},
  volume = {7},
  number = {1},
  publisher = {The MIT Press},
  issn = {2644-2353,},
  doi = {10.1162/99608f92.b033a087},
  urldate = {2025-06-01},
  abstract = {There is a growing literature on reasoning by large language models (LLMs), but the discussion on the uncertainty in their responses is still lacking. Our aim is to assess the extent of confidence that LLMs have in their answers and how it correlates with accuracy. Confidence is measured (i) qualitatively in terms of persistence in keeping their answer when prompted to reconsider, and (ii) quantitatively in terms of self-reported confidence score. We investigate the performance of three LLMs---GPT4o, GPT4-turbo, and Mistral--on two benchmark sets of questions on causal judgment and formal fallacies, and a set of probability and statistical puzzles and paradoxes. Although the LLMs show significantly better performance than random guessing, there is a wide variability in their tendency to change their initial answers. There is a positive correlation between qualitative confidence and accuracy, but the overall accuracy for the second answer is often worse than for the first answer. There is a strong tendency to overstate the self-reported confidence score. Confidence is only partially explained by the underlying token-level probability. The material effects of prompting on qualitative confidence and the strong tendency for overconfidence indicate that current LLMs do not have any internally coherent sense of confidence.},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\P73WMV3Q\Pawitan and Holmes - 2025 - Confidence in the Reasoning of Large Language Mode.pdf}
}

@misc{PDFUNCONSTRAINEDHANDWRITING,
  title = {({{PDF}}) {{UNCONSTRAINED HANDWRITING RECOGNITION}}: {{LANGUAGE MODELS}}, {{PERPLEXITY}}, {{AND SYSTEM PERFORMANCE}}},
  shorttitle = {({{PDF}}) {{UNCONSTRAINED HANDWRITING RECOGNITION}}},
  journal = {ResearchGate},
  urldate = {2018-11-20},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  howpublished = {https://www.researchgate.net/publication/30469917\_UNCONSTRAINED\_HANDWRITING\_RECOGNITION\_LANGUAGE\_MODELS\_PERPLEXITY\_AND\_SYSTEM\_PERFORMANCE},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\P4FR2W6C\(PDF) UNCONSTRAINED HANDWRITING RECOGNITION LANGU.html}
}

@article{pearsonLIIILinesPlanes1901,
  title = {{{LIII}}. {{On}} Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = {1901},
  month = nov,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  publisher = {Taylor \& Francis},
  issn = {1941-5982},
  doi = {10.1080/14786440109462720},
  urldate = {2023-08-17},
  file = {C:\Users\tarchibald\Zotero\storage\VGHJF4N2\Pearson - 1901 - LIII. On lines and planes of closest fit to system.pdf}
}

@article{pengModalitySpecificCrossModalSimilarity2018,
  title = {Modality-{{Specific Cross-Modal Similarity Measurement}} with {{Recurrent Attention Network}}},
  author = {Peng, Yuxin and Qi, Jinwei and Yuan, Yuxin},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {11},
  eprint = {1708.04776},
  pages = {5585--5599},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10577149},
  doi = {10.1109/TIP.2018.2852503},
  urldate = {2021-02-27},
  abstract = {Nowadays, cross-modal retrieval plays an important role to flexibly find useful information across different modalities of data. Effectively measuring the similarity between different modalities of data is the key of cross-modal retrieval. Different modalities, such as image and text, have imbalanced and complementary relationship, and they contain unequal amount of information when describing the same semantics. For example, images often contain more details that cannot be demonstrated by textual descriptions and vice versa. Existing works based on deep neural network mostly construct one common space for different modalities, to find the latent alignments between them, which lose their exclusive modality-specific characteristics. Therefore, we propose modality-specific cross-modal similarity measurement approach by constructing the independent semantic space for each modality, which adopts an end-to-end framework to directly generate the modality-specific cross-modal similarity without explicit common representation. For each semantic space, modality-specific characteristics within one modality are fully exploited by recurrent attention network, while the data of another modality is projected into this space with attention based joint embedding, which utilizes the learned attention weights for guiding the fine-grained cross-modal correlation learning, and captures the imbalanced and complementary relationship between different modalities. Finally, the complementarity between the semantic spaces for different modalities is explored by adaptive fusion of the modality-specific cross-modal similarities to perform the cross-modal retrieval. Experiments on the widely used Wikipedia, Pascal Sentence, and MS-COCO data sets as well as our constructed large-scale XMediaNet data set verify the effectiveness of our proposed approach, outperforming nine state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {adaptive fusion,attention based joint embedding,Modality-specific cross-modal similarity measurement,recurrent attention network},
  file = {C:\Users\tarchibald\Zotero\storage\TSIBE4TY\full-text.pdf}
}

@article{pengNegativeCorrelationLearningbased2020,
  title = {Negative Correlation Learning-Based {{RELM}} Ensemble Model Integrated with {{OVMD}} for Multi-Step Ahead Wind Speed Forecasting},
  author = {Peng, Tian and Zhang, Chu and Zhou, Jianzhong and Nazir, Muhammad Shahzad},
  year = {2020},
  month = aug,
  journal = {Renewable Energy},
  volume = {156},
  pages = {804--819},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2020.03.168},
  urldate = {2025-06-05},
  abstract = {Accurate and reliable wind speed forecasting is vital in power system scheduling and management. Ensemble techniques are widely employed to enhance wind speed forecasting accuracy. This paper proposes a negative correlation learning-based regularized extreme learning machine ensemble model (NCL-RELM) integrated with optimal variational mode decomposition (OVMD) and sample entropy (SampEn) for multi-step ahead wind speed forecasting. For this purpose, the original wind speed time series is firstly decomposed into a few variational modes and a residue using OVMD, and then the decomposed subseries with approximate SampEn values are aggregated into a new subseries to reduce the computational burden. Secondly, a NCL-RELM ensemble model is employed to model each aggregated subseries. The NCL technique is employed to enhance the diversity among multiple sub-RELM models such that the predictability of a single RELM model can be enhanced. Finally, the prediction results of all subseries are added up to obtain an aggregated result for the original wind speed. The simulation results indicate that: (1) the NCL-RELM model performs better than other ensemble approaches including BAGTREE, BOOST and random forest; (2) the proposed OS-NCL-RELM model obtains the best statistical metrics from 1- to 3-step ahead forecasting compared with the other nine benchmark models.},
  keywords = {Negative correlation learning,Optimal variational mode decomposition,Regularized extreme learning machine,Sample entropy,Wind speed forecasting},
  file = {C:\Users\tarchibald\Zotero\storage\Q4EZNM9K\S0960148120305012.html}
}

@article{perezPoissonImageEditing2003,
  title = {Poisson Image Editing},
  author = {P{\'e}rez, Patrick and Gangnet, Michel and Blake, Andrew},
  year = {2003},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {22},
  number = {3},
  pages = {313--318},
  issn = {0730-0301},
  doi = {10.1145/882262.882269},
  urldate = {2024-02-19},
  abstract = {Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The first set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.},
  keywords = {guided interpolation,image gradient,interactive image editing,Poisson equation,seamless cloning,selection editing},
  file = {C:\Users\tarchibald\Zotero\storage\SDQHZF4T\Prez et al. - 2003 - Poisson image editing.pdf}
}

@misc{perotLMDXLanguageModelbased2023,
  title = {{{LMDX}}: {{Language Model-based Document Information Extraction}} and {{Localization}}},
  shorttitle = {{{LMDX}}},
  author = {Perot, Vincent and Kang, Kai and Luisier, Florian and Su, Guolong and Sun, Xiaoyu and Boppana, Ramya Sree and Wang, Zilong and Mu, Jiaqi and Zhang, Hao and Hua, Nan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10952},
  eprint = {2309.10952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10952},
  urldate = {2024-04-17},
  abstract = {Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and localizing the entities within the document. In particular, we apply LMDX to the PaLM 2-S LLM and evaluate it on VRDU and CORD benchmarks, setting a new state-of-the-art and showing how LMDX enables the creation of high quality, data-efficient parsers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\HX7V5FYH\\Perot et al. - 2023 - LMDX Language Model-based Document Information Ex.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\RXQ44X5Y\\2309.html}
}

@techreport{Perspective,
  title = {Chapman \& {{Hall}}/{{CRC Machine Learning}} \& {{Pattern Recognition Series Chapman}} \& {{Hall}}/{{CRC Machine Learning}} \& {{Pattern Recognition Series Machine Learning M AC H I N E LEARNING}}},
  author = {Perspective, An Algorithmic},
  file = {C:\Users\tarchibald\Zotero\storage\GANMM2HF\Machine Learning - An Algorithmic Perspective (2nd ed.).pdf}
}

@article{piotBridgingGapImitation2017,
  title = {Bridging the Gap between Imitation Learning and Inverse Reinforcement Learning},
  author = {Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {8},
  pages = {1814--1826},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21622388},
  doi = {10.1109/TNNLS.2016.2543000},
  urldate = {2020-05-04},
  abstract = {Learning from demonstrations is a paradigm by which an apprentice agent learns a control policy for a dynamic environment by observing demonstrations delivered by an expert agent. It is usually implemented as either imitation learning (IL) or inverse reinforcement learning (IRL) in the literature. On the one hand, IRL is a paradigm relying on the Markov decision processes, where the goal of the apprentice agent is to find a reward function from the expert demonstrations that could explain the expert behavior. On the other hand, IL consists in directly generalizing the expert strategy, observed in the demonstrations, to unvisited states (and it is therefore close to classification, when there is a finite set of possible decisions). While these two visions are often considered as opposite to each other, the purpose of this paper is to exhibit a formal link between these approaches from which new algorithms can be derived. We show that IL and IRL can be redefined in a way that they are equivalent, in the sense that there exists an explicit bijective operator (namely, the inverse optimal Bellman operator) between their respective spaces of solutions. To do so, we introduce the set-policy framework that creates a clear link between the IL and the IRL. As a result, the IL and IRL solutions making the best of both worlds are obtained. In addition, it is a unifying framework from which existing IL and IRL algorithms can be derived and which opens the way for the IL methods able to deal with the environment's dynamics. Finally, the IRL algorithms derived from the set-policy framework are compared with the algorithms belonging to the more common trajectory-matching family. Experiments demonstrate that the set-policy-based algorithms outperform both the standard IRL and IL ones and result in more robust solutions.},
  keywords = {Imitation learning (IL),inverse reinforcement learning (IRL),learning from demonstrations (LfD)},
  file = {C:\Users\tarchibald\Zotero\storage\WTX48RWB\full-text.pdf}
}

@article{Plamondon1999,
  title = {The Segmentation of Cursive Handwriting: {{An}} Approach Based on off-Line Recovery of the Motor-Temporal Information},
  author = {Plamondon, R{\'e}jean and Privitera, Claudio M.},
  year = {1999},
  journal = {IEEE Transactions on Image Processing},
  volume = {8},
  number = {1},
  pages = {80--91},
  issn = {10577149},
  doi = {10.1109/83.736691},
  urldate = {2019-10-14},
  abstract = {This paper presents a segmentation method that partly mimics the cognitive-behavioral process used by human subjects to recover motor-temporal information from the image of a handwritten word. The approach does not exploit any thinning or skeletonization procedure, but rather a different type of information is manipulated concerning the curvature function of the word contour. In this way, it is possible to detect the parts of the image where the original odometric information is lost or ambiguous (such as, for example, at an intersection of the handwritten lines) and interpret them to finally recover a part of the original temporal information. The algorithm scans the word, following the natural course of the line, and attempts to reproduce the same movement as executed by the writer during the generation of the word. It segments the cursive trace where the contour shows the slow-down of the original movement (corresponding to the maximum curvature points of the curve). At the end of the scanning process, a temporal sequence of motor strokes is obtained which plausibly composed the original intended movement.},
  keywords = {Curvature,Handwriting segmentation,Off-line motor analysis,Stroke recovery},
  file = {C:\Users\tarchibald\Zotero\storage\UIKJMJWV\full-text.pdf}
}

@article{Plamondon2000,
  title = {On-{{Line}} and {{Off-Line Handwriting Recognition}}: {{A Comprehensive Survey}}},
  author = {Plamondon, Rejean and Srihari, S.N. and {Sargur N. Srihari}},
  year = {2000},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {1},
  pages = {63--84},
  issn = {01628828},
  doi = {10.1109/34.824821},
  urldate = {2019-10-12},
  isbn = {9291271195},
  file = {C:\Users\tarchibald\Zotero\storage\HAQ3MRB7\Unknown - Unknown - (No Title).pdf}
}

@article{plamondonOnlineOfflineHandwriting2000,
  title = {Online and Off-Line Handwriting Recognition: A Comprehensive Survey},
  shorttitle = {Online and Off-Line Handwriting Recognition},
  author = {Plamondon, R. and Srihari, S.N.},
  year = {2000},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {1},
  pages = {63--84},
  issn = {01628828},
  doi = {10.1109/34.824821},
  urldate = {2018-11-20}
}

@techreport{Poulos2019,
  title = {Character-{{Based Handwritten Text Transcription}} with {{Attention Networks}}},
  author = {Poulos, Jason and Valle, Rafael},
  year = {2019},
  eprint = {1712.04046v2},
  urldate = {2019-09-17},
  abstract = {The paper approaches the task of handwritten text transcription with attentional encoder-decoder networks that are trained on sequences of characters. We experiment on lines of text from a popular handwriting database and compare different attention mechanisms for the decoder. The model trained with softmax attention achieves the lowest test error, outperforming several other RNN-based models. Softmax attention is able to learn a linear alignment between image pixels and target characters whereas the alignment generated by sigmoid attention is linear but much less precise. When no function is used to obtain attention weights, the model performs poorly because it lacks a precise alignment between the source and text output.},
  archiveprefix = {arXiv},
  keywords = {attention i,convolutional neural networks,encoder-decoder networks,Handwriting recognition,image-to-text},
  file = {C:\Users\tarchibald\Zotero\storage\U8Y9VVKG\full-text.pdf}
}

@inproceedings{pratikakisICDAR2017CompetitionDocument2017,
  title = {{{ICDAR2017 Competition}} on {{Document Image Binarization}} ({{DIBCO}} 2017)},
  booktitle = {2017 14th {{IAPR International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Pratikakis, Ioannis and Zagoris, Konstantinos and Barlas, George and Gatos, Basilis},
  year = {2017},
  month = nov,
  volume = {01},
  pages = {1395--1403},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2017.228},
  abstract = {DIBCO 2017 is the international Competition on Document Image Binarization organized in conjunction with the ICDAR 2017 conference. The general objective of the contest is to identify current advances in document image binarization of machine-printed and handwritten document images using performance evaluation measures that are motivated by document image analysis and recognition requirements. This paper describes the competition details including the evaluation measures used as well as the performance of the 26 submitted methods along with a brief description of each method.},
  keywords = {Algorithm design and analysis,binarization,Games,Gray-scale,handwritten document image,Image edge detection,machine-printed,Microsoft Windows,performance evaluation,Software algorithms},
  file = {C:\Users\tarchibald\Zotero\storage\EPH46WDL\Pratikakis et al. - 2017 - ICDAR2017 Competition on Document Image Binarizati.pdf}
}

@inproceedings{pratikakisICDAR2019Competition2019,
  title = {{{ICDAR}} 2019 {{Competition}} on {{Document Image Binarization}} ({{DIBCO}} 2019)},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Pratikakis, Ioannis and Zagoris, Konstantinos and Karagiannis, Xenofon and Tsochatzidis, Lazaros and Mondal, Tanmoy and {Marthot-Santaniello}, Isabelle},
  year = {2019},
  month = sep,
  pages = {1547--1556},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00249},
  abstract = {DIBCO 2019 is the international Competition on Document Image Binarization organized in conjunction with the ICDAR 2019 conference. The general objective of the contest is to identify current advances in document image binarization of machine-printed and handwritten document images using performance evaluation measures that are motivated by document image analysis and recognition requirements. This paper describes the competition details including the evaluation measures used as well as the performance of the 24 submitted methods along with a brief description of each method.},
  keywords = {Convolution,Gray-scale,Handwriting recognition,Image recognition,Image segmentation,machine-printed handwritten document image binarization performance evaluation,Text analysis,Training}
}

@inproceedings{pratikakisICDAR2019Competition2019a,
  title = {{{ICDAR}} 2019 {{Competition}} on {{Document Image Binarization}} ({{DIBCO}} 2019)},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Pratikakis, Ioannis and Zagoris, Konstantinos and Karagiannis, Xenofon and Tsochatzidis, Lazaros and Mondal, Tanmoy and {Marthot-Santaniello}, Isabelle},
  year = {2019},
  month = sep,
  pages = {1547--1556},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00249},
  abstract = {DIBCO 2019 is the international Competition on Document Image Binarization organized in conjunction with the ICDAR 2019 conference. The general objective of the contest is to identify current advances in document image binarization of machine-printed and handwritten document images using performance evaluation measures that are motivated by document image analysis and recognition requirements. This paper describes the competition details including the evaluation measures used as well as the performance of the 24 submitted methods along with a brief description of each method.},
  keywords = {Convolution,Gray-scale,Handwriting recognition,Image recognition,Image segmentation,machine-printed handwritten document image binarization performance evaluation,Text analysis,Training},
  file = {C:\Users\tarchibald\Zotero\storage\2I6UX3HM\Pratikakis et al. - 2019 - ICDAR 2019 Competition on Document Image Binarizat.pdf}
}

@inproceedings{pratikakisICFHR2012Competition2012,
  title = {{{ICFHR}} 2012 {{Competition}} on {{Handwritten Document Image Binarization}} ({{H-DIBCO}} 2012)},
  booktitle = {2012 {{International Conference}} on {{Frontiers}} in {{Handwriting Recognition}}},
  author = {Pratikakis, Ioannis and Gatos, Basilis and Ntirogiannis, Konstantinos},
  year = {2012},
  month = sep,
  pages = {817--822},
  doi = {10.1109/ICFHR.2012.216},
  urldate = {2024-02-19},
  abstract = {H-DIBCO 2012 is the International Document Image Binarization Competition which is dedicated to handwritten document images organized in conjunction with ICFHR 2012 conference. The objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures. This paper reports on the contest details including the evaluation measures used as well as the performance of the 24 submitted methods along with a short description of each method.},
  keywords = {binarization,Clustering algorithms,Distortion measurement,Educational institutions,Filtering algorithms,handwritten document image,Image edge detection,Informatics,performance evaluation,PSNR},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\BFJ7IW5V\\Pratikakis et al. - 2012 - ICFHR 2012 Competition on Handwritten Document Ima.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\ZZQ6RY2I\\6424498.html}
}

@techreport{prattFCNNFourierConvolutional,
  title = {{{FCNN}}: {{Fourier Convolutional Neural Networks}}},
  author = {Pratt, Harry and Williams, Bryan and Coenen, Frans and Zheng, Yalin},
  urldate = {2021-03-03},
  abstract = {The Fourier domain is used in computer vision and machine learning as image analysis tasks in the Fourier domain are analogous to spatial domain methods but are achieved using different operations. Convolutional Neu-ral Networks (CNNs) use machine learning to achieve state-of-the-art results with respect to many computer vision tasks. One of the main limiting aspects of CNNs is the computational cost of updating a large number of convolution parameters. Further, in the spatial domain, larger images take exponentially longer than smaller image to train on CNNs due to the operations involved in convolu-tion methods. Consequently, CNNs are often not a viable solution for large image computer vision tasks. In this paper a Fourier Convolution Neural Network (FCNN) is proposed whereby training is conducted entirely within the Fourier domain. The advantage offered is that there is a significant speed up in training time without loss of effectiveness. Using the proposed approach larger images can therefore be processed within viable computation time. The FCNN is fully described and evaluated. The evaluation was conducted using the benchmark Ci-far10 and MNIST datasets, and a bespoke fundus retina image dataset. The results demonstrate that convolution in the Fourier domain gives a significant speed up without adversely affecting accuracy. For simplicity the proposed FCNN concept is presented in the context of a basic CNN architecture, however, the FCNN concept has the potential to improve the speed of any neural network system involving convolution.},
  file = {C:\Users\tarchibald\Zotero\storage\2N8LDRJL\full-text.pdf}
}

@article{Qiao2018,
  title = {An Adaptive Deep {{Q-learning}} Strategy for Handwritten Digit Recognition},
  author = {Qiao, Junfei and Wang, Gongming and Li, Wenjing and Chen, Min},
  year = {2018},
  month = nov,
  journal = {Neural Networks},
  volume = {107},
  pages = {61--71},
  publisher = {Elsevier Ltd},
  issn = {18792782},
  doi = {10.1016/j.neunet.2018.02.010},
  urldate = {2019-10-05},
  abstract = {Handwritten digits recognition is a challenging problem in recent years. Although many deep learning-based classification algorithms are studied for handwritten digits recognition, the recognition accuracy and running time still need to be further improved. In this paper, an adaptive deep Q-learning strategy is proposed to improve accuracy and shorten running time for handwritten digit recognition. The adaptive deep Q-learning strategy combines the feature-extracting capability of deep learning and the decision-making of reinforcement learning to form an adaptive Q-learning deep belief network (Q-ADBN). First, Q-ADBN extracts the features of original images using an adaptive deep auto-encoder (ADAE), and the extracted features are considered as the current states of Q-learning algorithm. Second, Q-ADBN receives Q-function (reward signal) during recognition of the current states, and the final handwritten digits recognition is implemented by maximizing the Q-function using Q-learning algorithm. Finally, experimental results from the well-known MNIST dataset show that the proposed Q-ADBN has a superiority to other similar methods in terms of accuracy and running time.},
  keywords = {Adaptive deep auto-encoder,Adaptive Q-learning deep belief network,Deep learning,Handwritten digits recognition,Reinforcement learning},
  file = {C:\Users\tarchibald\Zotero\storage\HKMJRZU4\Qiao et al. - 2018 - An adaptive deep Q-learning strategy for handwritten digit recognition.pdf}
}

@article{qiaoFrameworkRestorationWriting2006,
  title = {A Framework toward Restoration of Writing Order from Single-Stroked Handwriting Image},
  author = {Qiao, Yu and Nishiara, Mikihiko and Yasuhara, Makoto},
  year = {2006},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {28},
  number = {11},
  eprint = {17063679},
  eprinttype = {pubmed},
  pages = {1724--1737},
  issn = {01628828},
  doi = {10.1109/TPAMI.2006.216},
  urldate = {2020-03-15},
  abstract = {Restoration of writing order from a single-stroked handwriting image can be seen as the problem of finding the smoothest path in its graph representation. In this paper, a 3-phase approach to restore a writing order is proposed within the framework of the Edge Continuity Relation (ECR). In the initial, local phase, in order to obtain possible ECRs at an even-degree node, a neural network is used for the node of degree 4 and a theoretical approach is presented for the node of degree higher than 4 by introducing certain reasonable assumptions. In the second phase, we identify double-traced lines by employing maximum weighted matching. This makes it possible to transform the problem of obtaining possible ECRs at odd-degree node to that at even-degree node. In the final, global phase, we find all the candidates of single-stroked paths by depth first search and select the best one by evaluating SLALOM smoothness. Experiments on static images converted from online data in the Unipen database show that our method achieves a restoration rate of 96.0 percent. {\copyright} 2006 IEEE.},
  keywords = {Edge continuity relation,Euler path,Graph matching,Handwriting recognition,Temporal information,Writing order restoration}
}

@misc{qiaoUniViTARUnifiedVision2025,
  title = {{{UniViTAR}}: {{Unified Vision Transformer}} with {{Native Resolution}}},
  shorttitle = {{{UniViTAR}}},
  author = {Qiao, Limeng and Gan, Yiyang and Wang, Bairui and Qin, Jie and Xu, Shuang and Yang, Siqi and Ma, Lin},
  year = {2025},
  month = may,
  number = {arXiv:2504.01792},
  eprint = {2504.01792},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.01792},
  urldate = {2025-06-05},
  abstract = {Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially investigated native resolution modeling, existing approaches still lack systematic analysis from a visual representation perspective. To bridge this gap, we introduce UniViTAR, a family of homogeneous vision foundation models tailored for unified visual modality and native resolution scenario in the era of multimodal. Our framework first conducts architectural upgrades to the vanilla paradigm by integrating multiple advanced components. Building upon these improvements, a progressive training paradigm is introduced, which strategically combines two core mechanisms: (1) resolution curriculum learning, transitioning from fixed-resolution pretraining to native resolution tuning, thereby leveraging ViT's inherent adaptability to variable-length sequences, and (2) visual modality adaptation via inter-batch image-video switching, which balances computational efficiency with enhanced temporal reasoning. In parallel, a hybrid training framework further synergizes sigmoid-based contrastive loss with feature distillation from a frozen teacher model, thereby accelerating early-stage convergence. Finally, trained exclusively on public datasets, externsive experiments across multiple model scales from 0.3B to 1B demonstrate its effectiveness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\3LN4HHCK\\Qiao et al. - 2025 - UniViTAR Unified Vision Transformer with Native R.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\M83TVMB6\\2504.html}
}

@article{rabhiHandwritingRecognitionBased2019,
  title = {Handwriting {{Recognition Based On Temporal Order Restored By The End-To-End System}}},
  author = {Rabhi, Besma and Elbaati, Abdelkarim and Hamdi, Yahia and Alimi, Adel M},
  year = {2019},
  pages = {1231--1236},
  doi = {10.1109/ICDAR.2019.00199},
  urldate = {2020-02-11},
  keywords = {cnn,deep,lstm,offline handwriting recognition,restoration,sequence-to-sequence model,temporal order},
  file = {C:\Users\tarchibald\Zotero\storage\K5HK8ZFP\full-text.pdf}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8748--8763},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-09},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\4YWU2XB6\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\JXU2BM4G\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf}
}

@article{rahamanSpectralBiasNeural,
  title = {On the {{Spectral Bias}} of {{Neural Networks}}},
  author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A and Bengio, Yoshua and Courville, Aaron},
  eprint = {1806.08734v3},
  urldate = {2021-12-13},
  abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions-i.e. functions that vary globally without local fluctuations-which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\N4IYW4WP\full-text.pdf}
}

@article{rajagopalHopfieldNeuralNetwork2018,
  title = {A {{Hopfield}} Neural Network with Multiple Attractors and Its {{FPGA}} Design},
  author = {Rajagopal, Karthikeyan and {Munoz-Pacheco}, Jesus M. and Pham, Viet Thanh and Hoang, Duy Vo and Alsaadi, Fawaz E. and Alsaadi, Fuad E.},
  year = {2018},
  month = oct,
  journal = {European Physical Journal: Special Topics},
  volume = {227},
  number = {7-9},
  pages = {811--820},
  publisher = {Springer Verlag},
  issn = {19516401},
  doi = {10.1140/epjst/e2018-800018-7},
  urldate = {2021-04-22},
  abstract = {Neural network is important for a wide range of applications. Especially, a small neural network can display various complex behaviors. In this work, the investigations of a Hopfield neural network and its field programmable gate array (FPGA) implementation have been reported. The considered Hopfield neural network is simple because it includes only three neurons. It is interesting that we observed chaos and numerous coexisting attractors in such a network. In addition, the network has been implemented via an FPGA platform to verify its feasibility.},
  keywords = {Atomic,Classical and Continuum Physics,Condensed Matter Physics,general,Materials Science,Measurement Science and Instrumentation,Molecular,Optical and Plasma Physics,Physics},
  file = {C:\Users\tarchibald\Zotero\storage\5SDWG4RR\full-text.pdf}
}

@techreport{rameshZeroShotTexttoImageGeneration,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  eprint = {2102.12092v1},
  urldate = {2021-02-26},
  abstract = {Text-to-image generation has traditionally fo-cused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\7NDKBEN3\full-text.pdf}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12092},
  eprint = {2102.12092},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.12092},
  urldate = {2024-02-12},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\5C962PYF\\Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\KM2BYUHF\\2102.html}
}

@article{ramirez-ortaPostOCRDocumentCorrection2022,
  title = {Post-{{OCR Document Correction}} with {{Large Ensembles}} of {{Character Sequence-to-Sequence Models}}},
  author = {{Ramirez-Orta}, Juan Antonio and Xamena, Eduardo and Maguitman, Ana and Milios, Evangelos and Soto, Axel J.},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {10},
  eprint = {2109.06264},
  pages = {11192--11199},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  issn = {2374-3468},
  doi = {10.1609/AAAI.V36I10.21369},
  urldate = {2022-12-07},
  abstract = {In this paper, we propose a novel method to extend sequence-to-sequence models to accurately process sequences much longer than the ones used during training while being sample- and resource-efficient, supported by thorough experimentation. To investigate the effectiveness of our method, we apply it to the task of correcting documents already processed with Optical Character Recognition (OCR) systems using sequence-to-sequence models based on characters. We test our method on nine languages of the ICDAR 2019 competition on post-OCR text correction and achieve a new state-of-the-art performance in five of them. The strategy with the best performance involves splitting the input document in character n-grams and combining their individual corrections into the final output using a voting scheme that is equivalent to an ensemble of a large number of sequence models. We further investigate how to weigh the contributions from each one of the members of this ensemble. Our code for post-OCR correction is shared at https://github.com/jarobyte91/post\_ocr\_correction.},
  archiveprefix = {arXiv},
  keywords = {Speech & Natural Language Processing (SNLP)}
}

@article{ramsauerHopfieldNetworksAll2020,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2020},
  month = jul,
  eprint = {2008.02217},
  doi = {10.48550/arxiv.2008.02217},
  urldate = {2022-03-21},
  abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\YDSQ4WL9\full-text.pdf}
}

@misc{RecoveryTemporalInformation,
  title = {Recovery of {{Temporal Information From Static Images}} of {{Handwriting}}},
  urldate = {2019-10-05},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=223211},
  file = {C:\Users\tarchibald\Zotero\storage\AQNPCILM\full-text.pdf}
}

@techreport{ReeveIngle,
  title = {A {{Scalable Handwritten Text Recognition System}}},
  author = {Ingle, R Reeve and Fujii, Yasuhisa and Deselaers, Thomas and Baccash, Jonathan and Popat, Ashok C and Reeve Ingle, R and Fujii, Yasuhisa and Deselaers, Thomas and Baccash, Jonathan and Popat, Ashok C},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.09150},
  eprint = {1904.09150},
  urldate = {2019-08-26},
  abstract = {Many studies on (Offline) Handwritten Text Recognition (HTR) systems have focused on building state-of-the-art models for line recognition on small corpora. However, adding HTR capability to a large scale multilingual OCR system poses new challenges. This paper addresses three problems in building such systems: data, efficiency, and integration. Firstly, one of the biggest challenges is obtaining sufficient amounts of high quality training data. We address the problem by using online handwriting data collected for a large scale production online handwriting recognition system. We describe our image data generation pipeline and study how online data can be used to build HTR models. We show that the data improve the models significantly under the condition where only a small number of real images is available, which is usually the case for HTR models. It enables us to support a new script at substantially lower cost. Secondly, we propose a line recognition model based on neural networks without recurrent connections. The model achieves a comparable accuracy with LSTM-based models while allowing for better parallelism in training and inference. Finally, we present a simple way to integrate HTR models into an OCR system. These constitute a solution to bring HTR capability into a large scale OCR system.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\GGZS7W2N\Reeve Ingle et al. - Unknown - A Scalable Handwritten Text Recognition System.pdf}
}

@inproceedings{renRecognitionOnlineHandwriting2019,
  title = {Recognition of {{Online Handwriting}} with {{Variability}} on {{Smart Devices}}},
  booktitle = {{{ICASSP}}, {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} - {{Proceedings}}},
  author = {Ren, Guohua and Ganapathy, Viswanath},
  year = {2019},
  month = may,
  volume = {2019-May},
  pages = {7605--7609},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15206149},
  doi = {10.1109/ICASSP.2019.8682706},
  urldate = {2020-07-24},
  abstract = {Online handwriting recognition enables a convenient user interface for smart devices. However, readability of the handwriting varies based on the device user and the writing surface. State of art handwriting recognition systems are not yet robust with respect to different variability in writing text which results from user movement, writing speed, available space to write on, sloppiness etc. In this work we employ hybrid deep neural network architectures based on Bidirectional Long Short Term Memory (BLSTM) and Connectionist Temporal Classification (CTC) networks to recognize user independent dynamic handwriting with variability. Evaluated on IAMOnDB database, we show that our proposed model achieves state of the art accuracy with minimal data preprocessing and recognizes text with variability. We further compress the model using knowledge distillation to deploy on resource constrained smart devices. Our novel strategy of training student model for CTC networks enjoys significant model size reduction with moderate performance degradation.},
  isbn = {978-1-4799-8131-1},
  keywords = {CTC,Handwriting recognition,Knowledge Distillation,LSTM,overlapped handwriting},
  file = {C:\Users\tarchibald\Zotero\storage\WQFCDFCR\full-text.pdf}
}

@article{renSurveyDeepActive2021,
  title = {A {{Survey}} of {{Deep Active Learning}}},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
  year = {2021},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {9},
  pages = {180:1--180:40},
  issn = {0360-0300},
  doi = {10.1145/3472291},
  urldate = {2023-08-29},
  abstract = {Active learning (AL) attempts to maximize a model's performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
  keywords = {active learning,deep active learning,Deep learning},
  file = {C:\Users\tarchibald\Zotero\storage\LYBV2MDG\Ren et al. - 2021 - A Survey of Deep Active Learning.pdf}
}

@article{Reul2018,
  title = {Improving {{OCR Accuracy}} on {{Early Printed Books}} by Combining {{Pretraining}}, {{Voting}}, and {{Active Learning}}},
  author = {Reul, Christian and Springmann, Uwe and Wick, Christoph and Puppe, Frank},
  year = {2018},
  month = feb,
  eprint = {1802.10038},
  urldate = {2019-09-23},
  abstract = {We combine three methods which significantly improve the OCR accuracy of OCR models trained on early printed books: (1) The pretraining method utilizes the information stored in already existing models trained on a variety of typesets (mixed models) instead of starting the training from scratch. (2) Performing cross fold training on a single set of ground truth data (line images and their transcriptions) with a single OCR engine (OCRopus) produces a committee whose members then vote for the best outcome by also taking the top-N alternatives and their intrinsic confidence values into account. (3) Following the principle of maximal disagreement we select additional training lines which the voters disagree most on, expecting them to offer the highest information gain for a subsequent training (active learning). Evaluations on six early printed books yielded the following results: On average the combination of pretraining and voting improved the character accuracy by 46\% when training five folds starting from the same mixed model. This number rose to 53\% when using different models for pretraining, underlining the importance of diverse voters. Incorporating active learning improved the obtained results by another 16\% on average (evaluated on three of the six books). Overall, the proposed methods lead to an average error rate of 2.5\% when training on only 60 lines. Using a substantial ground truth pool of 1,000 lines brought the error rate down even further to less than 1\% on average.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\Q86ZKFYJ\full-text.pdf}
}

@inproceedings{reulImprovingOCRAccuracy2018,
  title = {Improving {{OCR Accuracy}} on {{Early Printed Books}} by Utilizing {{Cross Fold Training}} and {{Voting}}},
  booktitle = {2018 13th {{IAPR International Workshop}} on {{Document Analysis Systems}} ({{DAS}})},
  author = {Reul, Christian and Springmann, Uwe and Wick, Christoph and Puppe, Frank},
  year = {2018},
  month = apr,
  eprint = {1711.09670},
  primaryclass = {cs},
  pages = {423--428},
  doi = {10.1109/DAS.2018.30},
  urldate = {2025-02-07},
  abstract = {In this paper we introduce a method that significantly reduces the character error rates for OCR text obtained from OCRopus models trained on early printed books. The method uses a combination of cross fold training and confidence based voting. After allocating the available ground truth in different subsets several training processes are performed, each resulting in a specific OCR model. The OCR text generated by these models then gets voted to determine the final output by taking the recognized characters, their alternatives, and the confidence values assigned to each character into consideration. Experiments on seven early printed books show that the proposed method outperforms the standard approach considerably by reducing the amount of errors by up to 50\% and more.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{Robert2014,
  title = {Machine {{Learning}}, a {{Probabilistic Perspective}}},
  author = {Robert, Christian},
  year = {2014},
  month = apr,
  journal = {CHANCE},
  volume = {27},
  number = {2},
  pages = {62--63},
  publisher = {Informa UK Limited},
  issn = {0933-2480},
  doi = {10.1080/09332480.2014.914768},
  abstract = {This books adopts the view that the best way to solve such problems is to use the tools of probability theory. Probability theory can be applied to any problem involving uncertainty. In machine learning, uncertainty comes in many forms: what is the best prediction about the future given some past data? what is the best model to explain some data? what measurement should I perform next? etc. The probabilistic approach to machine learning is closely related to the field of statistics, but differs slightly in terms of its emphasis and terminology3. We},
  file = {C:\Users\tarchibald\Zotero\storage\6NV86N8M\Machine Learning - A Probabilistic Perspective.pdf}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2023-08-17},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\4TQAWNRA\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\5T8XKD56\\1505.html}
}

@article{royHMMbasedIndicHandwritten2017,
  title = {{{HMM-based Indic Handwritten Word Recognition}} Using {{Zone Segmentation}}},
  author = {Roy, Partha Pratim and Bhunia, Ayan Kumar and Das, Ayan and Dey, Prasenjit and Pal, Umapada},
  year = {2017},
  month = aug,
  journal = {Pattern Recognition},
  volume = {60},
  eprint = {1708.00227},
  pages = {1057--1075},
  publisher = {Elsevier Ltd},
  doi = {10.1016/j.patcog.2016.04.012},
  urldate = {2020-07-25},
  abstract = {This paper presents a novel approach towards Indic handwritten word recognition using zone-wise information. Because of complex nature due to compound characters, modifiers, overlapping and touching, etc., character segmentation and recognition is a tedious job in Indic scripts (e.g. Devanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character segmentation in such scripts, HMM-based sequence modeling has been used earlier in holistic way. This paper proposes an efficient word recognition framework by segmenting the handwritten word images horizontally into three zones (upper, middle and lower) and recognize the corresponding zones. The main aim of this zone segmentation approach is to reduce the number of distinct component classes compared to the total number of classes in Indic scripts. As a result, use of this zone segmentation approach enhances the recognition performance of the system. The components in middle zone where characters are mostly touching are recognized using HMM. After the recognition of middle zone, HMM based Viterbi forced alignment is applied to mark the left and right boundaries of the characters. Next, the residue components, if any, in upper and lower zones in their respective boundary are combined to achieve the final word level recognition. Water reservoir feature has been integrated in this framework to improve the zone segmentation and character alignment defects while segmentation. A novel sliding window-based feature, called Pyramid Histogram of Oriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive experiment is performed on two Indic scripts namely, Bangla and Devanagari for the performance evaluation. From the experiment, it has been noted that proposed zone-wise recognition improves accuracy with respect to the traditional way of Indic word recognition.},
  archiveprefix = {arXiv},
  keywords = {Handwritten word recognition,Hidden Markov Model,Indian script recognition},
  file = {C:\Users\tarchibald\Zotero\storage\ER4II3YL\full-text.pdf}
}

@inproceedings{Sabir2018,
  title = {Implicit {{Language Model}} in {{LSTM}} for {{OCR}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Sabir, Ekraam and Rawls, Stephen and Natarajan, Prem},
  year = {2018},
  month = jan,
  volume = {7},
  pages = {27--31},
  publisher = {IEEE Computer Society},
  issn = {15205363},
  doi = {10.1109/ICDAR.2017.361},
  urldate = {2019-10-06},
  abstract = {Neural networks have become the technique of choice for OCR, but many aspects of how and why they deliver superior performance are still unknown. One key difference between current neural network techniques using LSTMs and the previous state-of-the-art HMM systems is that HMM systems have a strong independence assumption. In comparison LSTMs have no explicit constraints on the amount of context that can be considered during decoding. In this paper we show that they learn an implicit LM and attempt to characterize the strength of the LM in terms of equivalent n-gram context. We show that this implicitly learned language model provides a 2.4{\textbackslash}\% CER improvement on our synthetic test set when compared against a test set of random characters (i.e. not naturally occurring sequences), and that the LSTM learns to use up to 5 characters of context (which is roughly 88 frames in our configuration). We believe that this is the first ever attempt at characterizing the strength of the implicit LM in LSTM based OCR systems.},
  isbn = {978-1-5386-3586-5},
  keywords = {Language Model,LSTM,OCR},
  file = {C:\Users\tarchibald\Zotero\storage\3QFUQSAV\full-text.pdf}
}

@techreport{sacramentoDendriticCorticalMicrocircuits2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  author = {Sacramento, Jo{\~a}o and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {31},
  pages = {8721--8732},
  urldate = {2020-12-24},
  abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances-error backpropagation-appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  file = {C:\Users\tarchibald\Zotero\storage\JTFJRTX9\full-text.pdf}
}

@inproceedings{sadekarLSHDIBLargeScale2022,
  title = {{{LS-HDIB}}: {{A Large Scale Handwritten Document Image Binarization Dataset}}},
  shorttitle = {{{LS-HDIB}}},
  booktitle = {2022 26th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Sadekar, Kaustubh and Tiwari, Ashish and Singh, Prajwal and Raman, Shanmuganathan},
  year = {2022},
  month = aug,
  pages = {1678--1684},
  issn = {2831-7475},
  doi = {10.1109/ICPR56361.2022.9956447},
  abstract = {Handwritten document image binarization is challenging due to high variability in the written content and complex background attributes such as page style, paper quality, stains, shadow gradients, and non-uniform illumination. While the traditional thresholding methods do not effectively generalize on such challenging real-world scenarios, deep learning-based methods have performed relatively well when provided with sufficient training data. However, the existing datasets are limited in size and diversity. This work proposes LS-HDIB - a large-scale handwritten document image binarization dataset containing over a million document images that span numerous real-world scenarios. Additionally, we introduce a novel technique that uses a combination of adaptive thresholding and seamless cloning methods to create the dataset with accurate ground truths. Through an extensive quantitative and qualitative evaluation over eight different deep learning based models, we demonstrate the enhancement in the performance of these models when trained on the LS-HDIB dataset and tested on unseen images.},
  keywords = {Adaptation models,Cloning,Degradation,Image segmentation,Learning systems,Thresholding (Imaging),Training},
  file = {C:\Users\tarchibald\Zotero\storage\5X7ISBHR\Sadekar et al. - 2022 - LS-HDIB A Large Scale Handwritten Document Image .pdf}
}

@misc{saifullahAnalyzingPotentialActive2022,
  title = {Analyzing the {{Potential}} of {{Active Learning}} for {{Document Image Classification}}},
  author = {Saifullah},
  year = {2022},
  month = nov,
  urldate = {2023-08-29}
}

@misc{saifullahAnalyzingPotentialActive2022a,
  title = {Analyzing the {{Potential}} of {{Active Learning}} for {{Document Image Classification}}},
  author = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  year = {2022},
  month = nov,
  issn = {2693-5015},
  doi = {10.21203/rs.3.rs-2273654/v1},
  urldate = {2024-05-22},
  abstract = {Deep learning has been extensively researched in the field of document analysis and has shown excellent performance across a wide range of document-related tasks. As a result, a great deal of emphasis is now being placed on its practical deployment and integration into modern industrial document processing pipelines. It is well-known, however, that deep learning models are data-hungry and often require huge volumes of annotated data in order to achieve competitive performances. And since data annotation is a costly and labor-intensive process, it remains one of the major hurdles to their practical deployment. This study investigates the possibility of using active learning to reduce the costs of data annotation in the context of Document Image Classification, which is one of the core components of modern document processing pipelines. The results of this study demonstrate that by utilizing active learning (AL), deep document classification models can achieve competitive performances to the models trained on fully annotated datasets, and in some cases, even surpass them by annotating only 15\%-40\% of the total training dataset. Furthermore, this study demonstrates that modern AL strategies significantly outperform random querying, and in many cases achieve comparable performance to the models trained on fully annotated datasets even in the presence of practical deployment issues such as data imbalance, and annotation noise, and thus, offer tremendous benefits in real-world deployment of deep document classification models. The code to reproduce our experiments is publicly available at https://github.com/saifullah3396/doc\_al.}
}

@article{saifullahAnalyzingPotentialActive2023,
  title = {Analyzing the Potential of Active Learning for Document Image Classification},
  author = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  year = {2023},
  month = apr,
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  issn = {1433-2825},
  doi = {10.1007/s10032-023-00429-8},
  urldate = {2023-08-16},
  abstract = {Deep learning has been extensively researched in the field of document analysis and has shown excellent performance across a wide range of document-related tasks. As a result, a great deal of emphasis is now being placed on its practical deployment and integration into modern industrial document processing pipelines. It is well known, however, that deep learning models are data-hungry and often require huge volumes of annotated data in order to achieve competitive performances. And since data annotation is a costly and labor-intensive process, it remains one of the major hurdles to their practical deployment. This study investigates the possibility of using active learning to reduce the costs of data annotation in the context of document image classification, which is one of the core components of modern document processing pipelines. The results of this study demonstrate that by utilizing active learning (AL), deep document classification models can achieve competitive performances to the models trained on fully annotated datasets and, in some cases, even surpass them by annotating only 15--40\% of the total training dataset. Furthermore, this study demonstrates that modern AL strategies significantly outperform random querying, and in many cases achieve comparable performance to the models trained on fully annotated datasets even in the presence of practical deployment issues such as data imbalance, and annotation noise, and thus, offer tremendous benefits in real-world deployment of deep document classification models. The code to reproduce our experiments is publicly available at https://github.com/saifullah3396/doc\_al.},
  langid = {english},
  keywords = {Active learning,Deep active learning,Document analysis,Document image classification},
  annotation = {dfki.de},
  file = {C:\Users\tarchibald\Zotero\storage\429X6NB4\Saifullah et al. - 2023 - Analyzing the potential of active learning for doc.pdf}
}

@inproceedings{saifullahDocXplainNovelModelAgnostic2024,
  title = {{{DocXplain}}: {{A Novel Model-Agnostic Explainability Method}} for~{{Document Image Classification}}},
  shorttitle = {{{DocXplain}}},
  booktitle = {Document {{Analysis}} and {{Recognition}} - {{ICDAR}} 2024},
  author = {Saifullah, Saifullah and Agne, Stefan and Dengel, Andreas and Ahmed, Sheraz},
  editor = {Barney Smith, Elisa H. and Liwicki, Marcus and Peng, Liangrui},
  year = {2024},
  pages = {103--123},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-70546-5_7},
  abstract = {Deep learning (DL) has revolutionized the field of document image analysis, showcasing superhuman performance across a diverse set of tasks. However, the inherent black-box nature of deep learning models still presents a significant challenge to their safe and robust deployment in industry. Regrettably, while a plethora of research has been dedicated in recent years to the development of DL-powered document analysis systems, research addressing their transparency aspects has been relatively scarce. In this paper, we aim to bridge this research gap by introducing DocXplain, a novel model-agnostic explainability method specifically designed for generating high interpretability feature attribution maps for the task of document image classification. In particular, our approach involves independently segmenting the foreground and background features of the documents into different document elements and then ablating these elements to assign feature importance. We extensively evaluate our proposed approach in the context of document image classification, utilizing 4 different evaluation metrics, 2 widely recognized document benchmark datasets, and 10 state-of-the-art document image classification models. By conducting a thorough quantitative and qualitative analysis against 9 existing state-of-the-art attribution methods, we demonstrate the superiority of our approach in terms of both faithfulness and interpretability. To the best of the authors' knowledge, this work presents the first model-agnostic attribution-based explainability method specifically tailored for document images. We anticipate that our work will significantly contribute to advancing research on transparency, fairness, and robustness of document image classification models.},
  isbn = {978-3-031-70546-5},
  langid = {english},
  keywords = {Document Image Classification,Explainable AI,Explainable Document Classification,Model Interpretability,Model-Agnostic}
}

@article{sakoeDynamicProgrammingAlgorithm1978,
  title = {Dynamic {{Programming Algorithm Optimization}} for {{Spoken Word Recognition}}},
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  year = {1978},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {26},
  number = {1},
  pages = {43--49},
  issn = {00963518},
  doi = {10.1109/TASSP.1978.1163055},
  urldate = {2020-07-23},
  abstract = {This paper reports on an optimum dynamic programming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm. {\copyright} 1978 IEEE},
  file = {C:\Users\tarchibald\Zotero\storage\FIBZFKDV\full-text.pdf}
}

@misc{sarkarMitigatingHallucinationsVisionLanguage2025,
  title = {Mitigating {{Hallucinations}} in {{Vision-Language Models}} through {{Image-Guided Head Suppression}}},
  author = {Sarkar, Sreetama and Che, Yue and Gavin, Alex and Beerel, Peter A. and Kundu, Souvik},
  year = {2025},
  month = may,
  number = {arXiv:2505.16411},
  eprint = {2505.16411},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.16411},
  urldate = {2025-06-04},
  abstract = {Despite their remarkable progress in multimodal understanding tasks, large vision language models (LVLMs) often suffer from "hallucinations", generating texts misaligned with the visual context. Existing methods aimed at reducing hallucinations through inference time intervention incur a significant increase in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided head suppression strategy that can be seamlessly integrated during inference, without incurring any significant compute or latency overhead. We investigate whether hallucination in LVLMs can be linked to specific model components. Our analysis suggests that hallucinations can be attributed to a dynamic subset of attention heads in each layer. Leveraging this insight, for each text query token, we selectively suppress attention heads that exhibit low attention to image tokens, keeping the top-K attention heads intact. Extensive evaluations on visual question answering and image description tasks demonstrate the efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining F1, and improving throughput by 1.8x compared to existing alternatives. Code is available at https://github.com/YUECHE77/SPIN.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\KPZFWXJQ\\Sarkar et al. - 2025 - Mitigating Hallucinations in Vision-Language Model.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\FAWEQG4Q\\2505.html}
}

@article{schlagLinearTransformersAre2021,
  title = {Linear {{Transformers Are Secretly Fast Weight Memory Systems}}},
  author = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  year = {2021},
  month = feb,
  eprint = {2102.11174},
  urldate = {2021-02-28},
  abstract = {We show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early '90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\F8BYFKPE\full-text.pdf}
}

@techreport{Schuster1997,
  title = {Bidirectional {{Recurrent Neural Networks}}},
  author = {Schuster, Mike and Paliwal, Kuldip K},
  year = {1997},
  journal = {IEEE TRANSACTIONS ON SIGNAL PROCESSING},
  volume = {45},
  number = {11},
  urldate = {2019-10-10},
  abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported. Index Terms-Recurrent neural networks.},
  file = {C:\Users\tarchibald\Zotero\storage\U2VXDKD9\full-text.pdf}
}

@inproceedings{senerActiveLearningConvolutional2018,
  title = {Active {{Learning}} for {{Convolutional Neural Networks}}: {{A Core-Set Approach}}},
  shorttitle = {Active {{Learning}} for {{Convolutional Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sener, Ozan and Savarese, Silvio},
  year = {2018},
  month = feb,
  urldate = {2023-08-16},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs when applied in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\87ZDEAI4\Sener and Savarese - 2018 - Active Learning for Convolutional Neural Networks.pdf}
}

@inproceedings{seungQueryCommittee1992,
  title = {{Query by committee}},
  booktitle = {{Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory}},
  author = {Seung, H. S. and Opper, M. and Sompolinsky, H.},
  year = {1992},
  pages = {287--294},
  publisher = {Publ by ACM},
  doi = {10.1145/130385.130417},
  urldate = {2023-08-17},
  langid = {English (US)},
  file = {C:\Users\tarchibald\Zotero\storage\9GNDY3DV\Seung et al. - 1992 - Query by committee.pdf}
}

@inproceedings{shekharOPADOptimizedPolicyBased2022a,
  title = {{{OPAD}}: {{An Optimized Policy-Based Active Learning Framework}} for {{Document Content Analysis}}},
  shorttitle = {{{OPAD}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shekhar, Sumit and Guda, Bhanu Prakash Reddy and Chaubey, Ashutosh and Jindal, Ishan and Jain, Avneet},
  year = {2022},
  pages = {2826--2836},
  urldate = {2023-08-16},
  langid = {english},
  annotation = {Adobe},
  file = {C:\Users\tarchibald\Zotero\storage\QJN2NVKB\Shekhar et al. - 2022 - OPAD An Optimized Policy-Based Active Learning Fr.pdf}
}

@article{shenDeepActiveLearning2017,
  title = {Deep {{Active Learning}} for {{Named Entity Recognition}}},
  author = {Shen, Yanyao and Yun, Hyokun and Lipton, Zachary C. and Kronrod, Yakov and Anandkumar, Animashree},
  year = {2017},
  month = jul,
  journal = {Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP 2017 at the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017},
  eprint = {1707.05928},
  pages = {252--256},
  publisher = {Association for Computational Linguistics (ACL)},
  doi = {10.18653/v1/w17-2630},
  urldate = {2023-08-14},
  abstract = {Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25{\textbackslash}\% of the original training data.},
  archiveprefix = {arXiv},
  isbn = {9781945626623},
  file = {C:\Users\tarchibald\Zotero\storage\NZGGXN9Y\full-text.pdf}
}

@misc{shiResLoRAIdentityResidual2024,
  title = {{{ResLoRA}}: {{Identity Residual Mapping}} in {{Low-Rank Adaption}}},
  shorttitle = {{{ResLoRA}}},
  author = {Shi, Shuhua and Huang, Shaohan and Song, Minghui and Li, Zhoujun and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18039},
  eprint = {2402.18039},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-01},
  abstract = {As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at https://github.com/microsoft/LMOps/tree/main/reslora .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\Z6EA78ZD\\Shi et al. - 2024 - ResLoRA Identity Residual Mapping in Low-Rank Ada.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\H8KVD68C\\Shi et al. - 2024 - ResLoRA Identity Residual Mapping in Low-Rank Ada.html}
}

@article{shonenkovsbersochiStackMixBlotAugmentations2021,
  title = {{{StackMix}} and {{Blot Augmentations}} for {{Handwritten Text Recognition}}},
  author = {Shonenkov SBER Sochi, Alex AI and Federation, Russian and Max Novopoltsev SBER Sochi, OCRVru AI and Dimitrov SBER, Denis AI and MSU Moscow, Lomonosov},
  year = {2021},
  month = aug,
  eprint = {2108.11667},
  urldate = {2023-04-12},
  abstract = {This paper proposes a handwritten text recognition(HTR) system that outperforms current state-of-the-artmethods. The comparison was carried out on three of themost frequently used in HTR task datasets, namely Ben-tham, IAM, and Saint Gall. In addition, the results on tworecently presented datasets, Peter the Greats manuscriptsand HKR Dataset, are provided.The paper describes the architecture of the neural net-work and two ways of increasing the volume of train-ing data: augmentation that simulates strikethrough text(HandWritten Blots) and a new text generation method(StackMix), which proved to be very effective in HTR tasks.StackMix can also be applied to the standalone task of gen-erating handwritten text based on printed text.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\ZXZHNZ22\full-text.pdf}
}

@misc{shorinwaSurveyUncertaintyQuantification2024,
  title = {A {{Survey}} on {{Uncertainty Quantification}} of {{Large Language Models}}: {{Taxonomy}}, {{Open Research Challenges}}, and {{Future Directions}}},
  shorttitle = {A {{Survey}} on {{Uncertainty Quantification}} of {{Large Language Models}}},
  author = {Shorinwa, Ola and Mei, Zhiting and Lidard, Justin and Ren, Allen Z. and Majumdar, Anirudha},
  year = {2024},
  month = dec,
  number = {arXiv:2412.05563},
  eprint = {2412.05563},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05563},
  urldate = {2025-06-04},
  abstract = {The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence. Previous work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant research efforts devoted to quantifying the uncertainty of LLMs. This survey seeks to provide an extensive review of existing uncertainty quantification methods for LLMs, identifying their salient features, along with their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid understanding of the state of the art. Furthermore, we highlight applications of uncertainty quantification methods for LLMs, spanning chatbot and textual applications to embodied artificial intelligence applications in robotics. We conclude with open research challenges in uncertainty quantification of LLMs, seeking to motivate future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\EE9AIMSS\\Shorinwa et al. - 2024 - A Survey on Uncertainty Quantification of Large La.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\UBH62ZVF\\2412.html}
}

@techreport{Shrivastava,
  title = {Learning from {{Simulated}} and {{Unsupervised Images}} through {{Adversarial Training}}},
  author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  urldate = {2019-09-24},
  abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However , learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simula-tor. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations , avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  file = {C:\Users\tarchibald\Zotero\storage\FE3DNV23\full-text.pdf}
}

@article{Shrivastava2016,
  title = {Learning from {{Simulated}} and {{Unsupervised Images}} through {{Adversarial Training}}},
  author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  year = {2016},
  month = dec,
  journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume = {2017-Janua},
  eprint = {1612.07828},
  pages = {2242--2251},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2020-10-28},
  abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\VJC8TMI6\full-text.pdf}
}

@article{shrivastavaLearningSimulatedUnsupervised2016,
  title = {Learning from {{Simulated}} and {{Unsupervised Images}} through {{Adversarial Training}}},
  author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
  year = {2016},
  month = dec,
  journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume = {2017-January},
  eprint = {1612.07828},
  pages = {2242--2251},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2020-10-14},
  abstract = {With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\7CVLWNCV\full-text.pdf}
}

@inproceedings{siddiquiRethinkingSemanticSegmentation2019,
  title = {Rethinking {{Semantic Segmentation}} for {{Table Structure Recognition}} in {{Documents}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Siddiqui, Shoaib Ahmed and Khan, Pervaiz Iqbal and Dengel, Andreas and Ahmed, Sheraz},
  year = {2019},
  month = sep,
  pages = {1397--1402},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00225},
  abstract = {Based on the recent advancements in the domain of semantic segmentation, Fully-Convolutional Networks (FCN) have been successfully applied for the task of table structure recognition in the past. We analyze the efficacy of semantic segmentation networks for this purpose and simplify the problem by proposing prediction tiling based on the consistency assumption which holds for tabular structures. For an image of dimensions H {\texttimes} W, we predict a single column for the rows ({\^y}row {$\epsilon$} H) and a predict a single row for the columns ({\^y}row {$\epsilon$} W). We use a dual-headed architecture where initial feature maps (from the encoder-decoder model) are shared while the last two layers generate class specific (row/column) predictions. This allows us to generate predictions using a single model for both rows and columns simultaneously, where previous methods relied on two separate models for inference. With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13 image-based table structure recognition dataset with an average F-Measure of 92.39\% (91.90\% and 92.88\% F-Measure for rows and columns respectively). With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13. The obtained results advocate that constraining the problem space in the case of FCN by imposing valid constraints can lead to significant performance gains.},
  keywords = {Data mining,Decoding,Document Analysis Table Structure Recognition Table Understanding Fully-Convolutional Networks (FCN),Feature extraction,Image segmentation,Measurement,Semantics,Task analysis}
}

@article{siddiquiSelfSupervisedRepresentationLearning2021,
  title = {Self-{{Supervised Representation Learning}} for {{Document Image Classification}}},
  author = {Siddiqui, Shoaib Ahmed and Dengel, Andreas and Ahmed, Sheraz},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {164358--164367},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {21693536},
  doi = {10.1109/ACCESS.2021.3133200},
  urldate = {2022-08-26},
  abstract = {Supervised learning, despite being extremely effective, relies on expensive, time-consuming, and error-prone annotations. Self-supervised learning has recently emerged as a strong alternate to supervised learning in a range of different domains as collecting a large amount of unlabeled data can be achieved by simply crawling the internet. These self-supervised methods automatically discover features relevant to represent an input example by using self-defined proxy tasks. In this paper, we question the potential of commonly employed purely supervised training (starting either from ImageNet pretrained networks or pure random initialization) in contrast to self-supervised representations that can be learned directly using self-supervised representation learning methods on large document image datasets. For this purpose, we leverage a large-scale document image collection (RVL-CDIP) to train ResNet-50 image encoder using two different self-supervision methods (SimCLR and Barlow Twins). Employing a linear classifier on top of self-supervised embeddings from ResNet-50 results in an accuracy of 86.75\% as compared to 71.43\% from the corresponding ImageNet pretrained embeddings. Similarly, evaluating on Tobacco-3482 dataset using self-supervised embeddings from ResNet-50 yields an accuracy of 88.52\% in contrast to 74.16\% from the corresponding ImageNet pretrained embeddings. We show that in the case of limited labeled data, this wide gap in performance between self-supervised and fully supervised models persists even after fine-tuning pretrained models. However, a significant reduction in this gap is observed with an increasing amount of data including the case where the model is trained from scratch. Our results show that representations learned using self-supervised representation learning techniques are a viable option for document image classification, specifically in the context of limited labeled data, which is a usual restriction in industrial use cases.},
  keywords = {convolutional neural networks,deep learning,Document image classification,representation learning,self-supervised learning},
  file = {C:\Users\tarchibald\Zotero\storage\WP2IHK7D\full-text.pdf}
}

@article{singhFullPageHandwriting2021,
  title = {Full {{Page Handwriting Recognition}} via {{Image}} to {{Sequence Extraction}}},
  author = {Singh, Sumeet S. and Karayev, Sergey},
  year = {2021},
  month = mar,
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {12823 LNCS},
  eprint = {2103.06450v2},
  pages = {55--69},
  publisher = {{Springer Science and Business Media Deutschland GmbH}},
  doi = {10.1007/978-3-030-86334-0_4},
  urldate = {2022-03-10},
  abstract = {We present a Neural Network based Handwritten Text Recognition (HTR) model architecture that can be trained to recognize full pages of handwritten or printed text without image segmentation. Being based on Image to Sequence architecture, it can extract text present in an image and then sequence it correctly without imposing any constraints regarding orientation, layout and size of text and non-text. Further, it can also be trained to generate auxiliary markup related to formatting, layout and content. We use character level vocabulary, thereby enabling language and terminology of any subject. The model achieves a new state-of-art in paragraph level recognition on the IAM dataset. When evaluated on scans of real world handwritten free form test answers - beset with curved and slanted lines, drawings, tables, math, chemistry and other symbols - it performs better than all commercially available HTR cloud APIs. It is deployed in production as part of a commercial web application.},
  archiveprefix = {arXiv}
}

@article{singhUnpairedDocumentImage2023,
  title = {Unpaired {{Document Image Denoising}} for {{OCR}} Using {{BiLSTM}} Enhanced {{CycleGAN}}},
  author = {Singh, Katyani and Ray, Nilanjan and Tata, Ganesh and Oeveren, Eric Van},
  year = {2023},
  month = jun,
  doi = {10.21203/RS.3.RS-3074040/V1},
  urldate = {2023-08-15},
  abstract = {The recognition performance of optical character recognition (OCR) models can be sub-optimal when document images suffer from various degradations. Supervised deep learning methods for image enhancement can generate high-quality enhanced images. However, these methods demand the availability of corresponding clean images or ground truth text. Sometimes this requirement is difficult to fulfill for real-world noisy documents. For instance, it can be challenging to create paired noisy/clean training datasets or obtain ground truth text for noisy point-of-sale receipts and invoices. Unsu-pervised methods have been explored in recent years to enhance images in the absence of ground truth images or text. However, these methods focus on enhancing natural scene images. In the case of document images, preserving the readability of text in the enhanced images is of utmost importance for improved OCR performance. In this work, we propose a modified architecture to the CycleGAN model to improve its performance in enhancing document images with better text preservation. Inspired by the success of CNN-BiLSTM combination networks in text recognition models, we propose modifying the discriminator network in the CycleGAN model to a combined CNN-BiLSTM network for better feature extraction from document images during classification by the discriminator network. Results indicate that our proposed model not only leads to better preservation of text and improved OCR performance over the CycleGAN model but also achieves better performance than the classical unsupervised image pre-processing techniques like Sauvola and Otsu.},
  keywords = {Document enhancement,Generative adversarial networks,Generative adversarial networks Keywords: Document enhancement,Image processing,Unsupervised learning},
  file = {C:\Users\tarchibald\Zotero\storage\LJX2HJF6\2023 - Unpaired Document Image Denoising for OCR using Bi.pdf}
}

@inproceedings{sinhaCICAContentInjectedContrastive2024,
  title = {{{CICA}}: {{Content-Injected Contrastive Alignment}} for~{{Zero-Shot Document Image Classification}}},
  shorttitle = {{{CICA}}},
  booktitle = {Document {{Analysis}} and {{Recognition}} - {{ICDAR}} 2024},
  author = {Sinha, Sankalp and Khan, Muhammad Saif Ullah and Sheikh, Talha Uddin and Stricker, Didier and Afzal, Muhammad Zeshan},
  editor = {Barney Smith, Elisa H. and Liwicki, Marcus and Peng, Liangrui},
  year = {2024},
  pages = {124--141},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-70546-5_8},
  abstract = {Zero-shot learning has been extensively investigated in the broader field of visual recognition, attracting significant interest recently. However, the current work on zero-shot learning in document image classification remains scarce. The existing studies either focus exclusively on zero-shot inference, or their evaluation does not align with the established criteria of zero-shot evaluation in the visual recognition domain. We provide a comprehensive document image classification analysis in Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) settings to address this gap. Our methodology and evaluation align with the established practices of this domain. Additionally, we propose zero-shot splits for the RVL-CDIP dataset. Furthermore, we introduce CICA~(pronounced ``ki\$\${\textbackslash}cdot \$\${$\cdot$}ka''), a framework that enhances the zero-shot learning capabilities of CLIP. CICA~consists of a novel `content module' designed to leverage any generic document-related textual information. The discriminative features extracted by this module are aligned with CLIP's text and image features using a novel `coupled-contrastive' loss. Our module improves CLIP's ZSL top-1 accuracy by 6.7\% and GZSL harmonic mean by 24\% on the RVL-CDIP dataset. Our module is lightweight and adds only 3.3\% more parameters to CLIP. Our work sets the direction for future research in zero-shot document classification.},
  isbn = {978-3-031-70546-5},
  langid = {english},
  keywords = {Document Image Classification,Generalised Zero-Shot Learning,Multimodal Learning,Zero-Shot Learning}
}

@techreport{Sixt,
  title = {{{RENDERGAN}}: {{GENERATING REALISTIC LABELED DATA}}},
  author = {Sixt, Leon and Wild, Benjamin and Landgraf, Tim},
  urldate = {2019-09-24},
  abstract = {Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks. Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible. We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model. We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines.},
  file = {C:\Users\tarchibald\Zotero\storage\W5C3USRQ\full-text.pdf}
}

@misc{slossbergCalibrationScenetextRecognition2022,
  title = {On Calibration of Scene-Text Recognition Models},
  author = {Slossberg, Ron and Anschel, Oron and Markovitz, Amir and Litman, Ron and Aberdam, Aviad and Tsiper, Shahar and Mazor, Shai and Wu, Jon and Manmatha, R.},
  year = {2022},
  journal = {Amazon Science},
  urldate = {2025-03-14},
  abstract = {The topics of confidence and trust in modern scene-text recognition (STR) models have been rarely investigated in spite of their prevalent use within critical user-facing applications. We analyze confidence estimation for STR models and find that they tend towards overconfidence thus leading to{\dots}},
  howpublished = {https://www.amazon.science/publications/on-calibration-of-scene-text-recognition-models},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\JQ7RV9QY\Slossberg et al. - 2022 - On calibration of scene-text recognition models.pdf}
}

@article{sollnerExploringSelectiveUse2019,
  title = {Exploring the {{Selective Use}} of {{Ad Blockers}} and {{Testing Banner Appeals}} to {{Reduce Ad Blocking}}},
  author = {S{\"o}llner, Johanna and Dost, Florian},
  year = {2019},
  month = may,
  journal = {Journal of Advertising},
  volume = {48},
  number = {3},
  pages = {302--312},
  publisher = {Routledge},
  issn = {0091-3367},
  doi = {10.1080/00913367.2019.1613699},
  urldate = {2020-07-01},
  abstract = {Ad blocker proliferation threatens the revenue streams of many websites and raises fears about the viability of digital advertising as a whole. Although industry initiatives have increasingly aimed to mitigate ad blocking, surprisingly little research addresses the problem. It remains largely unknown which factors drive ad blocking or help unblocking or ``whitelisting'' of select websites. This article presents an exploratory survey study of 1,634 ad blocker users and uncovers the main factors driving ad blocking and conditions for unblocking select websites. The conditions suggest changes to online advertising in line with current industry initiatives, as well as opportunities for direct appeals to ad blocker users. A field experiment with 294,331 users tests whether banner appeals can reduce ad blocking. The results show that these appeals reduce ad blocking among 1\% of the ad blocker users, and among frequent visitors who encounter repeated banner appeals, this percentage increases to 2\%. However, repeated banners also lead to fewer website visits of users not conforming to the appeal, which suggests a trade-off between reducing ad blocking among some users and further increasing avoidance behaviors among others.},
  file = {C:\Users\tarchibald\Zotero\storage\AYYB4N7E\full-text.pdf}
}

@inproceedings{soperBARTPostCorrectionOCR2021,
  title = {{{BART}} for {{Post-Correction}} of {{OCR Newspaper Text}}},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Noisy User-generated Text}} ({{W-NUT}} 2021)},
  author = {Soper, Elizabeth and Fujimoto, Stanley and Yu, Yen-Yun},
  year = {2021},
  pages = {284--290},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.wnut-1.31},
  urldate = {2022-12-08},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\H29V5G9B\Soper et al. - 2021 - BART for Post-Correction of OCR Newspaper Text.pdf}
}

@inproceedings{soperBARTPostCorrectionOCR2021a,
  title = {{{BART}} for {{Post-Correction}} of {{OCR Newspaper Text}}},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Noisy User-generated Text}} ({{W-NUT}} 2021)},
  author = {Soper, Elizabeth and Fujimoto, Stanley and Yu, Yen-Yun},
  year = {2021},
  pages = {284--290},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.wnut-1.31},
  urldate = {2022-12-08},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\6EXGHQ8H\Soper et al. - 2021 - BART for Post-Correction of OCR Newspaper Text.pdf}
}

@inproceedings{soperBARTPostCorrectionOCR2021b,
  title = {{{BART}} for {{Post-Correction}} of {{OCR Newspaper Text}}},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Noisy User-generated Text}} ({{W-NUT}} 2021)},
  author = {Soper, Elizabeth and Fujimoto, Stanley and Yu, Yen-Yun},
  year = {2021},
  pages = {284--290},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.wnut-1.31},
  urldate = {2022-12-08},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\36ZLQMZ5\2021.wnut-1.31.pdf}
}

@article{soperBARTPostCorrectionOCR2021c,
  title = {{{BART}} for {{Post-Correction}} of {{OCR Newspaper Text Work Done}} at {{Ancestry}}.Com},
  author = {Soper, Elizabeth and Fujimoto, Stanley and Yu, Yen-Yun},
  year = {2021},
  pages = {284--290},
  urldate = {2022-12-07},
  abstract = {Optical character recognition (OCR) from newspaper page images is susceptible to noise due to degradation of old documents and variation in typesetting. In this report, we present a novel approach to OCR post-correction. We cast error correction as a translation task, and fine-tune BART, a transformer-based sequence-to-sequence language model pretrained to denoise corrupted text. We are the first to use sentence-level transformer models for OCR post-correction, and our best model achieves a 29.4\% improvement in character accuracy over the original noisy OCR text. Our results demonstrate the utility of pre-trained language models for dealing with noisy text.},
  file = {C:\Users\tarchibald\Zotero\storage\8C7A4NBY\full-text.pdf}
}

@article{souibguiDEGANConditionalGenerative2022,
  title = {{{DE-GAN}}: {{A Conditional Generative Adversarial Network}} for {{Document Enhancement}}},
  shorttitle = {{{DE-GAN}}},
  author = {Souibgui, Mohamed Ali and Kessentini, Yousri},
  year = {2022},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {3},
  pages = {1180--1191},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3022406},
  abstract = {Documents often exhibit various forms of degradation, which make it hard to be read and substantially deteriorate the performance of an OCR system. In this paper, we propose an effective end-to-end framework named document enhancement generative adversarial networks (DE-GAN) that uses the conditional GANs (cGANs) to restore severely degraded document images. To the best of our knowledge, this practice has not been studied within the context of generative adversarial deep networks. We demonstrate that, in different tasks (document clean up, binarization, deblurring and watermark removal), DE-GAN can produce an enhanced version of the degraded document with a high quality. In addition, our approach provides consistent improvements compared to state-of-the-art methods over the widely used DIBCO 2013, DIBCO 2017, and H-DIBCO 2018 datasets, proving its ability to restore a degraded document image to its ideal condition. The obtained results on a wide variety of degradation reveal the flexibility of the proposed model to be exploited in other document enhancement problems.},
  keywords = {deep learning,degraded document binarization,Document analysis,document enhancement,generative adversarial networks,Generative adversarial networks,Image restoration,Machine learning,Text analysis,watermark removal},
  file = {C:\Users\tarchibald\Zotero\storage\I52R9KQ2\Souibgui and Kessentini - 2022 - DE-GAN A Conditional Generative Adversarial Netwo.pdf}
}

@article{souibguiDocEnTrEndtoEndDocument2022,
  title = {{{DocEnTr}}: {{An End-to-End Document Image Enhancement Transformer}}},
  author = {Souibgui, Mohamed Ali and Biswas, Sanket and Jemni, Sana Khamekhem and Kessentini, Yousri and Fornes, Alicia and Llados, Josep and Pal, Umapada},
  year = {2022},
  journal = {Proceedings - International Conference on Pattern Recognition},
  volume = {2022-August},
  eprint = {2201.10252},
  pages = {1699--1705},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10514651},
  doi = {10.1109/ICPR56361.2022.9956101},
  urldate = {2023-08-15},
  abstract = {Document images can be affected by many degradation scenarios, which cause recognition and processing difficulties. In this age of digitization, it is important to denoise them for proper usage. To address this challenge, we present a new encoder-decoder architecture based on vision transformers to enhance both machine-printed and handwritten document images, in an end-to-end fashion. The encoder operates directly on the pixel patches with their positional information without the use of any convolutional layers, while the decoder reconstructs a clean image from the encoded patches. Conducted experiments show a superiority of the proposed model compared to the state-of-the-art methods on several DIBCO benchmarks. Code and models will be publicly available at: https://github.com/dali92002/DocEnTR.},
  archiveprefix = {arXiv},
  isbn = {9781665490627}
}

@inproceedings{souibguiDocEnTrEndtoEndDocument2022a,
  title = {{{DocEnTr}}: {{An End-to-End Document Image Enhancement Transformer}}},
  shorttitle = {{{DocEnTr}}},
  booktitle = {2022 26th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Souibgui, Mohamed Ali and Biswas, Sanket and Jemni, Sana Khamekhem and Kessentini, Yousri and Forn{\'e}s, Alicia and Llad{\'o}s, Josep and Pal, Umapada},
  year = {2022},
  month = aug,
  pages = {1699--1705},
  issn = {2831-7475},
  doi = {10.1109/ICPR56361.2022.9956101},
  abstract = {Document images can be affected by many degradation scenarios, which cause recognition and processing difficulties. In this age of digitization, it is important to denoise them for proper usage. To address this challenge, we present a new encoder-decoder architecture based on vision transformers to enhance both machine-printed and handwritten document images, in an end-to-end fashion. The encoder operates directly on the pixel patches with their positional information without the use of any convolutional layers, while the decoder reconstructs a clean image from the encoded patches. Conducted experiments show a superiority of the proposed model compared to the state-of-the-art methods on several DIBCO benchmarks. Code and models will be publicly available at: https://github.com/dali92002/DocEnTR.},
  keywords = {Benchmark testing,Degradation,Head,Magnetic heads,Optical character recognition,Self-supervised learning,Transformers},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\4DI87JWE\\Souibgui et al. - 2022 - DocEnTr An End-to-End Document Image Enhancement .pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\PJUHTVXT\\9956101.html}
}

@techreport{Sriram,
  title = {Cold {{Fusion}}: {{Training Seq2Seq Models Together}} with {{Language Models}}},
  author = {Sriram, Anuroop and Jun, Heewoo and Satheesh, Sanjeev and Coates, Adam},
  eprint = {1708.06426v1},
  urldate = {2019-08-28},
  abstract = {Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training , and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10\% of the labeled training data.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\MDKSH7XN\full-text.pdf}
}

@misc{StatisticalNonparametricMethodology,
  title = {A Statistical, Nonparametric Methodology for Document Degradation Model Validation {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-02-01},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/888707}
}

@incollection{steinbachChallengesClusteringHigh2004,
  title = {The {{Challenges}} of {{Clustering High Dimensional Data}}},
  booktitle = {New {{Directions}} in {{Statistical Physics}}: {{Econophysics}}, {{Bioinformatics}}, and {{Pattern Recognition}}},
  author = {Steinbach, Michael and Ert{\"o}z, Levent and Kumar, Vipin},
  editor = {Wille, Luc T.},
  year = {2004},
  pages = {273--309},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-08968-2_16},
  urldate = {2024-05-22},
  abstract = {Cluster analysis divides data into groups (clusters) for the purposes of summarization or improved understanding. For example, cluster analysis has been used to group related documents for browsing, to find genes and proteins that have similar functionality, or as a means of data compression. While clustering has a long history and a large number of clustering techniques have been developed in statistics, pattern recognition, data mining, and other fields, significant challenges still remain. In this chapter we provide a short introduction to cluster analysis, and then focus on the challenge of clustering high dimensional data. We present a brief overview of several recent techniques, including a more detailed description of recent work of our own which uses a concept-based clustering approach.},
  isbn = {978-3-662-08968-2},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\SLHINKH5\Steinbach et al. - 2004 - The Challenges of Clustering High Dimensional Data.pdf}
}

@inproceedings{stewartDocumentImagePage2017,
  title = {Document {{Image Page Segmentation}} and {{Character Recognition}} as {{Semantic Segmentation}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Historical Document Imaging}} and {{Processing}}},
  author = {Stewart, Seth and Barrett, Bill},
  year = {2017},
  month = nov,
  series = {{{HIP}} '17},
  pages = {101--106},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3151509.3151518},
  urldate = {2024-01-31},
  abstract = {Convolutional Neural Networks (CNNs) have produced excellent results in natural scene semantic pixel labeling tasks. We examine the application of this idea to document processing, using fully supervised Deep CNN semantic segmentation to separate content layers from historical document images containing diverse content types, including handwriting, machine print, form lines, and stamps. For efficiency, we employ a downsampling-upsampling network to make dense pixel predictions. CNNs achieve high generalization accuracy on document images with interleaved, overlapping strokes, even when trained on a solitary pixel-labeled document image. We also show a proof-of-concept extension of the semantic segmentation task to handwritten cursive character recognition, enabling a new "segmentation-free" approach to handwriting transcription.},
  isbn = {978-1-4503-5390-8}
}

@article{Sueiras2018,
  title = {Offline Continuous Handwriting Recognition Using Sequence to Sequence Neural Networks},
  author = {Sueiras, Jorge and Ruiz, Victoria and Sanchez, Angel and Velez, Jose F.},
  year = {2018},
  month = may,
  journal = {Neurocomputing},
  volume = {289},
  pages = {119--128},
  publisher = {Elsevier B.V.},
  issn = {18728286},
  doi = {10.1016/j.neucom.2018.02.008},
  abstract = {This paper proposes the use of a new neural network architecture that combines a deep convolutional neural network with an encoder--decoder, called sequence to sequence, to solve the problem of recognizing isolated handwritten words. The proposed architecture aims to identify the characters and contextualize them with their neighbors to recognize any given word. Our model proposes a novel way to extract relevant visual features from a word image. It combines the use of a horizontal sliding window, to extract image patches, and the application of the LeNet-5 convolutional architecture to identify the characters. Extracted features are modeled using a sequence-to-sequence architecture to encode the visual characteristics and then to decode the sequence of characters in the handwritten text image. We test the proposed model on two handwritten databases (IAM and RIMES) under several experiments to determine the optimal parameterization of the model. Competitive results above those presented in the current state-of-the-art, on handwriting models, are achieved. Without using any language model and with closed dictionary, we obtain a word error rate in the test set of 12.7\% in IAM and 6.6\% in RIMES.},
  keywords = {Artificial intelligence,Convolutional Neural Networks,Handwriting recognition,Recurrent Neural Networks,Sequence to sequence networks}
}

@article{Sumi2019,
  title = {Modality {{Conversion}} of {{Handwritten Patterns}} by {{Cross Variational Autoencoders}}},
  author = {Sumi, Taichi and Iwana, Brian Kenji and Hayashi, Hideaki and Uchida, Seiichi},
  year = {2019},
  month = jun,
  journal = {arXiv preprint arXiv:1906.06142},
  eprint = {1906.06142},
  urldate = {2019-10-10},
  abstract = {This research attempts to construct a network that can convert online and offline handwritten characters to each other. The proposed network consists of two Variational Auto-Encoders (VAEs) with a shared latent space. The VAEs are trained to generate online and offline handwritten Latin characters simultaneously. In this way, we create a cross-modal VAE (Cross-VAE). During training, the proposed Cross-VAE is trained to minimize the reconstruction loss of the two modalities, the distribution loss of the two VAEs, and a novel third loss called the space sharing loss. This third, space sharing loss is used to encourage the modalities to share the same latent space by calculating the distance between the latent variables. Through the proposed method mutual conversion of online and offline handwritten characters is possible. In this paper, we demonstrate the performance of the Cross-VAE through qualitative and quantitative analysis.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\F2T6TH88\Sumi et al. - 2019 - Modality Conversion of Handwritten Patterns by Cross Variational Autoencoders.pdf}
}

@article{Sun2021,
  title = {Visual {{Parser}}: {{Representing Part-whole Hierarchies}} with {{Transformers}}},
  author = {Sun, Shuyang and Yue, Xiaoyu and Bai, Song and Torr, Philip},
  year = {2021},
  month = jul,
  eprint = {2107.05790},
  doi = {10.48550/arxiv.2107.05790},
  urldate = {2022-03-22},
  abstract = {Human vision is able to capture the part-whole hierarchical information from the entire scene. This paper presents the Visual Parser (ViP) that explicitly constructs such a hierarchy with transformers. ViP divides visual representations into two levels, the part level and the whole level. Information of each part represents a combination of several independent vectors within the whole. To model the representations of the two levels, we first encode the information from the whole into part vectors through an attention mechanism, then decode the global information within the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can gradually refine the features on both levels. Experimental results demonstrate that ViP can achieve very competitive performance on three major tasks e.g. classification, detection and instance segmentation. In particular, it can surpass the previous state-of-the-art CNN backbones by a large margin on object detection. The tiny model of the ViP family with \$7.2{\textbackslash}times\$ fewer parameters and \$10.9{\textbackslash}times\$ fewer FLOPS can perform comparably with the largest model ResNeXt-101-64\${\textbackslash}times\$4d of ResNe(X)t family. Visualization results also demonstrate that the learnt parts are highly informative of the predicting class, making ViP more explainable than previous fundamental architectures. Code is available at https://github.com/kevin-ssy/ViP.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\9TH26VIB\Sun et al. - 2021 - Visual Parser Representing Part-whole Hierarchies with Transformers.pdf}
}

@article{tancikFourierFeaturesLet,
  title = {Fourier {{Features Let Networks Learn High Frequency Functions}} in {{Low Dimensional Domains}}},
  author = {Tancik, Matthew and Srinivasan, Pratul P and Mildenhall, Ben and {Fridovich-Keil}, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T and Ng, Ren},
  eprint = {2006.10739v1},
  urldate = {2021-10-21},
  abstract = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\PK8GUGVQ\full-text.pdf}
}

@inproceedings{tangUnifyingVisionText2023,
  title = {Unifying {{Vision}}, {{Text}}, and {{Layout}} for {{Universal Document Processing}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  year = {2023},
  pages = {19254--19264},
  urldate = {2024-04-15},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\GGTJHH43\Tang et al. - 2023 - Unifying Vision, Text, and Layout for Universal Do.pdf}
}

@article{tataDocumentImageCleaning2023,
  title = {Document {{Image Cleaning}} Using {{Budget-Aware Black-Box Approximation}}},
  author = {Tata, Ganesh and Singh, Katyani and Van Oeveren, Eric and Ray, Nilanjan},
  year = {2023},
  month = jun,
  eprint = {2306.13236},
  urldate = {2023-08-15},
  abstract = {Recent work has shown that by approximating the behaviour of a non-differentiable black-box function using a neural network, the black-box can be integrated into a differentiable training pipeline for end-to-end training. This methodology is termed "differentiable bypass,'' and a successful application of this method involves training a document preprocessor to improve the performance of a black-box OCR engine. However, a good approximation of an OCR engine requires querying it for all samples throughout the training process, which can be computationally and financially expensive. Several zeroth-order optimization (ZO) algorithms have been proposed in black-box attack literature to find adversarial examples for a black-box model by computing its gradient in a query-efficient manner. However, the query complexity and convergence rate of such algorithms makes them infeasible for our problem. In this work, we propose two sample selection algorithms to train an OCR preprocessor with less than 10\% of the original system's OCR engine queries, resulting in more than 60\% reduction of the total training time without significant loss of accuracy. We also show an improvement of 4\% in the word-level accuracy of a commercial OCR engine with only 2.5\% of the total queries and a 32x reduction in monetary cost. Further, we propose a simple ranking technique to prune 30\% of the document images from the training dataset without affecting the system's performance.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\QTPR8UD5\full-text.pdf}
}

@article{tayEfficientTransformersSurvey,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  author = {Tay, Yi and Research, Google and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  eprint = {2009.06732v3},
  urldate = {2022-03-15},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed-Reformer, Linformer, Performer, Longformer, to name a few-which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arXiv},
  keywords = {Atten-tion Models,Deep Learning,Natural Language Processing,Neural Networks,Transformer Models},
  file = {C:\Users\tarchibald\Zotero\storage\KVU6HCBE\full-text.pdf}
}

@article{tayEfficientTransformersSurvey2020,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  author = {Tay, Yi and Research, Google and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2020},
  month = sep,
  eprint = {2009.06732},
  doi = {10.48550/arxiv.2009.06732},
  urldate = {2022-03-18},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arXiv},
  keywords = {Atten-tion Models,Deep Learning,Natural Language Processing,Neural Networks,Transformer Models},
  file = {C:\Users\tarchibald\Zotero\storage\PA8KU4RY\full-text.pdf}
}

@misc{teamGeminiFamilyHighly2025,
  title = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle = {Gemini},
  author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and G{\"u}ra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, {\'A}goston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Ana{\"i}s and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and {Barth-Maron}, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, I{\~n}aki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adri{\`a} Puigdom{\`e}nech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, S{\'e}bastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozi{\'n}ska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Gim{\'e}nez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and {Castro-Ros}, Alex and van den Driessche, George and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lu{\v c}i{\'c}, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Rapha{\"e}l Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sj{\"o}sund, Lars Lowe and Cevey, S{\'e}bastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, L{\'e}onard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adri{\`a} and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, V{\'i}ctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and {\"U}nl{\"u}, {\c C}a{\u g}lar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and {Gu-Lemberg}, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and {Cobon-Kerr}, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Raki{\'c}evi{\'c}, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and Cao, Nicola De and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and K{\k e}pa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and {\"A}lgmyr, Anton and Lottaz, Timoth{\'e}e and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and von Dincklage, Daniel and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, Fran{\c c}ois-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and B{\"o}lle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah {\'O} and Pereira, S{\'e}bastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and {Safranek-Shrader}, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccol{\`o} Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and {\"A}hdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Vij, Roopali and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bra{\v z}inskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, H{\'e}ctor Fern{\'a}ndez and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and K{\"a}rrman, Ante and Nowak, Pawe{\l} and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and {Ben-David}, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Casta{\~n}o, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybi{\'n}ski, Miko{\l}aj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, R{\'e}mi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and tan Girgin, Ser and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and {Lee-Thorp}, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and {Owusu-Afriyie}, Denese and Du, Cosmo and Thornton, Chloe and {Pont-Tuset}, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and H{\'e}liou, Am{\'e}lie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and {Santamaria-Fernandez}, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and {Holtmann-Rice}, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and P{\~o}der, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and {Choquette-Choo}, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivi{\`e}re, Morgane and Walton, Alanna and Crepy, Cl{\'e}ment and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and {Latorre-Chimoto}, Eri and {Klimczak-Pluci{\'n}ska}, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M, Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and List{\'i}k, V{\'i}t and Carlen, Mathias and van de Kerkhof, Jan and Pikus, Marcin and Zaher, Krunoslav and M{\"u}ller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol and {\{\{Gemini Team\} and others\}}},
  year = {2025},
  month = may,
  number = {arXiv:2312.11805},
  eprint = {2312.11805},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11805},
  urldate = {2025-06-01},
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\V5WMDKQH\\Team et al. - 2025 - Gemini A Family of Highly Capable Multimodal Mode.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\JH8ZVLBV\\2312.html}
}

@inproceedings{tensmeyerDocumentImageBinarization2017,
  title = {Document {{Image Binarization}} with {{Fully Convolutional Neural Networks}}},
  booktitle = {2017 14th {{IAPR International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Tensmeyer, Chris and Martinez, Tony},
  year = {2017},
  month = nov,
  volume = {01},
  pages = {99--104},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2017.25},
  abstract = {Binarization of degraded historical manuscript images is an important pre-processing step for many document processing tasks. We formulate binarization as a pixel classification learning task and apply a novel Fully Convolutional Network (FCN) architecture that operates at multiple image scales, including full resolution. The FCN is trained to optimize a continuous version of the Pseudo F-measure metric and an ensemble of FCNs outperform the competition winners on 4 of 7 DIBCO competitions. This same binarization technique can also be applied to different domains such as Palm Leaf Manuscripts with good performance. We analyze the performance of the proposed model w.r.t. the architectural hyperparameters, size and diversity of training data, and the input features chosen.},
  keywords = {Binarization,Computer architecture,Convolution,Convolutional Neural Networks,Deep Learning,Frequency modulation,Historical Document Analysis,Kernel,Measurement,Preprocessing,Task analysis,Training},
  file = {C:\Users\tarchibald\Zotero\storage\PU96CHU4\Tensmeyer and Martinez - 2017 - Document Image Binarization with Fully Convolution.pdf}
}

@article{tensmeyerGeneratingRealisticBinarization2019,
  title = {Generating Realistic Binarization Data with Generative Adversarial Networks},
  author = {Tensmeyer, Chris and Brodie, Mike and Saunders, Daniel and Martinez, Tony},
  year = {2019},
  month = sep,
  journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},
  pages = {172--177},
  publisher = {IEEE Computer Society},
  issn = {15205363},
  doi = {10.1109/ICDAR.2019.00036},
  urldate = {2023-08-15},
  abstract = {One of the limitations for using Deep Learning models to solve binarization tasks is that there is a lack of large quantities of labeled data available to train such models. Efforts to create synthetic data for binarization mostly rely on heuristic image processing techniques and generally lack realism. In this work, we propose a method to produce realistic synthetic data using an adversarially trained image translation model. We extend the popular CycleGAN model to be conditioned on the ground truth binarization mask as it translates images from the domain of synthetic images to the domain of real images. For evaluation, we train deep networks on synthetic datasets produced in different ways and measure their performance on the DIBCO datasets. Compared to not pretraining, we reduce error by 13\% on average, and compared to pretraining on unrealistic data, we reduce error by 6\%. Visually, we show that DGT-CycleGAN model produces more realistic synthetic data than other models.},
  isbn = {9781728128610},
  keywords = {Binarization,Deep Learning,Generative Adversarial Networks,Synthetic Data}
}

@inproceedings{tensmeyerGeneratingRealisticBinarization2019a,
  title = {Generating {{Realistic Binarization Data}} with {{Generative Adversarial Networks}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Tensmeyer, Chris and Brodie, Mike and Saunders, Daniel and Martinez, Tony},
  year = {2019},
  month = sep,
  pages = {172--177},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00036},
  abstract = {One of the limitations for using Deep Learning models to solve binarization tasks is that there is a lack of large quantities of labeled data available to train such models. Efforts to create synthetic data for binarization mostly rely on heuristic image processing techniques and generally lack realism. In this work, we propose a method to produce realistic synthetic data using an adversarially trained image translation model. We extend the popular CycleGAN model to be conditioned on the ground truth binarization mask as it translates images from the domain of synthetic images to the domain of real images. For evaluation, we train deep networks on synthetic datasets produced in different ways and measure their performance on the DIBCO datasets. Compared to not pretraining, we reduce error by 13\% on average, and compared to pretraining on unrealistic data, we reduce error by 6\%. Visually, we show that DGT-CycleGAN model produces more realistic synthetic data than other models.},
  keywords = {Binarization,Data models,Deep learning,Deep Learning,Degradation,Generative Adversarial Networks,Ink,Noise measurement,Synthetic Data,Task analysis,Training},
  file = {C:\Users\tarchibald\Zotero\storage\VRVYV7LG\8978087.html}
}

@article{tensmeyerHistoricalDocumentImage2020,
  title = {Historical {{Document Image Binarization}}: {{A Review}}},
  shorttitle = {Historical {{Document Image Binarization}}},
  author = {Tensmeyer, Chris and Martinez, Tony},
  year = {2020},
  month = may,
  journal = {SN Computer Science},
  volume = {1},
  number = {3},
  pages = {173},
  issn = {2661-8907},
  doi = {10.1007/s42979-020-00176-1},
  urldate = {2023-08-16},
  abstract = {This review provides a comprehensive view of the field of historical document image binarization with a focus on the contributions made in the last decade. After the introduction of a standard benchmark dataset with the 2009 Document Image Binarization Contest, research in the field accelerated. Besides the standard methods for image thresholding, preprocessing, and post-processing, we review the literature on methods such as statistical models, pixel classification with learning algorithms, and parameter tuning. In addition to reviewing binarization algorithms, we discuss the available public datasets and evaluation metrics, including those that require pixel-level ground truth and those that do not. We conclude with recommendations for future work.},
  langid = {english},
  keywords = {Binarization,Document analysis,Historical documents,Image processing},
  file = {C:\Users\tarchibald\Zotero\storage\W9NJL7BX\Tensmeyer and Martinez - 2020 - Historical Document Image Binarization A Review.pdf}
}

@misc{theaugraphyprojectAugraphyAugmentationPipeline2023,
  title = {Augraphy: An Augmentation Pipeline for Rendering Synthetic Paper Printing, Faxing, Scanning and Copy Machine Processes},
  shorttitle = {Augraphy},
  author = {{The Augraphy Project}},
  year = {2023},
  month = aug,
  urldate = {2023-08-22},
  abstract = {Augmentation pipeline for rendering synthetic paper printing, faxing, scanning and copy machine processes},
  copyright = {MIT}
}

@inproceedings{tianJustAskCalibration2023,
  title = {Just {{Ask}} for {{Calibration}}: {{Strategies}} for {{Eliciting Calibrated Confidence Scores}} from {{Language Models Fine-Tuned}} with {{Human Feedback}}},
  shorttitle = {Just {{Ask}} for {{Calibration}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {5433--5442},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.330},
  urldate = {2025-07-07},
  abstract = {A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50\%.},
  file = {C:\Users\tarchibald\Zotero\storage\KB3N4JLU\Tian et al. - 2023 - Just Ask for Calibration Strategies for Eliciting.pdf}
}

@misc{tianJustAskCalibration2023preprint,
  title = {Just {{Ask}} for {{Calibration}}: {{Strategies}} for {{Eliciting Calibrated Confidence Scores}} from {{Language Models Fine-Tuned}} with {{Human Feedback}}},
  shorttitle = {Just {{Ask}} for {{Calibration}}},
  author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14975},
  eprint = {2305.14975},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14975},
  urldate = {2025-05-31},
  abstract = {A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\tarchibald\Zotero\storage\65HI26DR\Tian et al. - 2023 - Just Ask for Calibration Strategies for Eliciting.pdf}
}

@inproceedings{touvronThreeThingsEveryone2022,
  title = {Three {{Things Everyone Should Know About Vision Transformers}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Touvron, Hugo and Cord, Matthieu and {El-Nouby}, Alaaeldin and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  pages = {497--515},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-20053-3_29},
  abstract = {After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.},
  isbn = {978-3-031-20053-3},
  langid = {english}
}

@misc{touvronThreeThingsEveryone2022preprint,
  title = {Three Things Everyone Should Know about {{Vision Transformers}}},
  author = {Touvron, Hugo and Cord, Matthieu and {El-Nouby}, Alaaeldin and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  year = {2022},
  month = mar,
  number = {arXiv:2203.09795},
  eprint = {2203.09795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.09795},
  urldate = {2025-06-05},
  abstract = {After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\NIHR9N2U\\Touvron et al. - 2022 - Three things everyone should know about Vision Tra.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\YME2EFTX\\2203.html}
}

@article{touvronTrainingDataefficientImage2020,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2020},
  month = dec,
  journal = {arXiv},
  eprint = {2012.12877},
  publisher = {arXiv},
  urldate = {2021-03-03},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\N7SSNXDY\full-text.pdf}
}

@article{trockmanPatchesAreAll2022,
  title = {Patches {{Are All You Need}}?},
  author = {Trockman, Asher and Kolter, J. Zico},
  year = {2022},
  month = jan,
  eprint = {2201.09792},
  urldate = {2023-04-12},
  abstract = {Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.},
  archiveprefix = {arXiv}
}

@article{trockmanUnderstandingCovarianceStructure2022,
  title = {Understanding the {{Covariance Structure}} of {{Convolutional Filters}}},
  author = {Trockman, Asher and Willmott, Devin and Kolter, J. Zico},
  year = {2022},
  month = oct,
  eprint = {2210.03651},
  urldate = {2023-04-12},
  abstract = {Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\PF5ZUL6J\full-text.pdf}
}

@techreport{UCRLJC128715PREPRINTGlobal1997,
  title = {{{UCRL-JC-128715 PREPRINT Global Warming}} and {{Ice Ages}}: {{I}}. {{Prospects}} for {{Physics-Based Modulation}} of {{Global Change}}},
  year = {1997},
  urldate = {2019-09-16},
  abstract = {L a w r e n c e L i v e r m o r e N a t i o n a l L a b o r a t o r y},
  file = {C:\Users\tarchibald\Zotero\storage\2YN9AXQC\full-text.pdf}
}

@misc{UnpairedDocumentImage2023,
  title = {Unpaired {{Document Image Denoising}} for {{OCR}} Using {{BiLSTM}} Enhanced {{CycleGAN}}},
  year = {2023},
  month = jun,
  doi = {10.21203/rs.3.rs-3074040/v1},
  urldate = {2023-08-16},
  abstract = {The recognition performance of optical character recognition (OCR) models can be sub-optimal when document images suffer from various degradations. Supervised deep learning methods for image enhancement can generate high-quality enhanced images. However, these methods demand the availability of c...},
  howpublished = {https://www.researchsquare.com},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\WTER849J\2023 - Unpaired Document Image Denoising for OCR using Bi.pdf}
}

@misc{usamaAnalysingRobustnessVisionLanguageModels2025,
  title = {Analysing the {{Robustness}} of {{Vision-Language-Models}} to {{Common Corruptions}}},
  author = {Usama, Muhammad and Asim, Syeda Aishah and Ali, Syed Bilal and Wasim, Syed Talal and Mansoor, Umair Bin},
  year = {2025},
  month = apr,
  number = {arXiv:2504.13690},
  eprint = {2504.13690},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.13690},
  urldate = {2025-06-04},
  abstract = {Vision-language models (VLMs) have demonstrated impressive capabilities in understanding and reasoning about visual and textual content. However, their robustness to common image corruptions remains under-explored. In this work, we present the first comprehensive analysis of VLM robustness across 19 corruption types from the ImageNet-C benchmark, spanning four categories: noise, blur, weather, and digital distortions. We introduce two new benchmarks, TextVQA-C and GQA-C, to systematically evaluate how corruptions affect scene text understanding and object-based reasoning, respectively. Our analysis reveals that transformer-based VLMs exhibit distinct vulnerability patterns across tasks: text recognition deteriorates most severely under blur and snow corruptions, while object reasoning shows higher sensitivity to corruptions such as frost and impulse noise. We connect these observations to the frequency-domain characteristics of different corruptions, revealing how transformers' inherent bias toward low-frequency processing explains their differential robustness patterns. Our findings provide valuable insights for developing more corruption-robust vision-language models for real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\6YG97C4H\\Usama et al. - 2025 - Analysing the Robustness of Vision-Language-Models.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\QLLBJPGU\\2504.html}
}

@article{vafaieHandwrittenPrintedText2022,
  title = {Handwritten and {{Printed Text Identification}} in {{Historical Archival Documents}}},
  author = {Vafaie, Mahsa and Bruns, Oleksandra and Pilz, Nastasja and Waitelonis, J{\"o}rg and Sack, Harald and Bruns, Oleksandra and Pilz, Nastasja and Waitelonis, J{\"o}rg and Sack, Harald},
  year = {2022},
  month = jun,
  journal = {Archiving Conference},
  volume = {19},
  pages = {15--20},
  publisher = {{Society for Imaging Science and Technology}},
  issn = {2161-8798},
  doi = {10.2352/issn.2168-3204.2022.19.1.4},
  urldate = {2023-08-18},
  abstract = {Abstract Historical archival records present many challenges for OCR systems to correctly encode their content, due to visual complexity, e.g. mixed printed text and handwritten annotations, paper degradation, and faded ink. This paper addresses the problem of automatic identification and separation of handwritten and printed text in historical archival documents, including the creation of an artificial pixel-level annotated dataset and the presentation of a new FCN-based model trained on historical data. Initial test results indicate 18\% IoU performance improvement on recognition of printed pixels and 10\% IoU performance improvement on recognition of handwritten pixels in synthesised data when compared to the state-of-the-art trained on modern documents. Furthermore, an extrinsic OCR-based evaluation on the printed layer extracted from real historical documents shows 26\% performance increase.},
  langid = {english},
  annotation = {https://github.com/ISE-FIZKarlsruhe/Wiedergutmachung},
  file = {C:\Users\tarchibald\Zotero\storage\KAMKZDQR\Vafaie et al. - 2022 - Handwritten and Printed Text Identification in His.pdf}
}

@misc{VOTINGBASEDOCRSYSTEM,
  title = {{{VOTING-BASED OCR SYSTEM}}},
  journal = {ResearchGate},
  urldate = {2025-03-14},
  abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
  howpublished = {https://www.researchgate.net/publication/326016983\_VOTING-BASED\_OCR\_SYSTEM},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\S83S5QGW\326016983_VOTING-BASED_OCR_SYSTEM.html}
}

@article{wangImageForeignLanguage2022,
  title = {Image as a {{Foreign Language}}: {{BEiT Pretraining}} for {{All Vision}} and {{Vision-Language Tasks}}},
  author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and Wei, Furu},
  year = {2022},
  month = aug,
  eprint = {2208.10442},
  urldate = {2023-04-12},
  abstract = {A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\2D735K5G\full-text.pdf}
}

@article{wangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity ({{SSIM}})},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {C:\Users\tarchibald\Zotero\storage\AXCKI8RG\Wang et al. - 2004 - Image quality assessment from error visibility to.pdf}
}

@article{wangRegCLRSelfSupervisedFramework2022,
  title = {{{RegCLR}}: {{A Self-Supervised Framework}} for {{Tabular Representation Learning}} in the {{Wild}}},
  author = {Wang, Weiyao and Kim, Byung-Hak and Ganapathi, Varun},
  year = {2022},
  month = nov,
  eprint = {2211.01165},
  doi = {10.48550/arxiv.2211.01165},
  urldate = {2022-11-17},
  abstract = {Recent advances in self-supervised learning (SSL) using large models to learn visual representations from natural images are rapidly closing the gap between the results produced by fully supervised learning and those produced by SSL on downstream vision tasks. Inspired by this advancement and primarily motivated by the emergence of tabular and structured document image applications, we investigate which self-supervised pretraining objectives, architectures, and fine-tuning strategies are most effective. To address these questions, we introduce RegCLR, a new self-supervised framework that combines contrastive and regularized methods and is compatible with the standard Vision Transformer architecture. Then, RegCLR is instantiated by integrating masked autoencoders as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches. Several real-world table recognition scenarios (e.g., extracting tables from document images), ranging from standard Word and Latex documents to even more challenging electronic health records (EHR) computer screen images, have been shown to benefit greatly from the representations learned from this new framework, with detection average-precision (AP) improving relatively by 4.8\% for Table, 11.8\% for Column, and 11.1\% for GUI objects over a previous fully supervised baseline on real-world EHR screen images.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\GW798JZJ\full-text.pdf}
}

@misc{wangScalingLawsPatchification2025,
  title = {Scaling {{Laws}} in {{Patchification}}: {{An Image Is Worth}} 50,176 {{Tokens And More}}},
  shorttitle = {Scaling {{Laws}} in {{Patchification}}},
  author = {Wang, Feng and Yu, Yaodong and Wei, Guoyizhe and Shao, Wei and Zhou, Yuyin and Yuille, Alan and Xie, Cihang},
  year = {2025},
  month = feb,
  number = {arXiv:2502.03738},
  eprint = {2502.03738},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.03738},
  urldate = {2025-05-05},
  abstract = {Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6\% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch\_Scaling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{wangSelfConsistencyImprovesChain2022,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V. and Chi, Ed H. and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2022},
  month = sep,
  urldate = {2025-06-03},
  abstract = {Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\S5HC5IGR\Wang et al. - 2022 - Self-Consistency Improves Chain of Thought Reasoni.pdf}
}

@techreport{Wasserman,
  title = {A {{Concise Course}} in {{Statistical Inference}}},
  author = {Wasserman, Larry and Berlin, Springer and New, Heidelberg and Barcelona, York and Kong, Hong and Milan, London and Tokyo, Paris},
  file = {C:\Users\tarchibald\Zotero\storage\74AX4ITM\All of Statistics.pdf}
}

@article{weiConvolutionalPoseMachines2016,
  title = {Convolutional {{Pose Machines}}},
  author = {Wei, Shih En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume = {2016-December},
  eprint = {1602.00134},
  pages = {4724--4732},
  publisher = {IEEE Computer Society},
  issn = {10636919},
  doi = {10.48550/arxiv.1602.00134},
  urldate = {2022-03-18},
  abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
  archiveprefix = {arXiv},
  isbn = {9781467388504},
  file = {C:\Users\tarchibald\Zotero\storage\ARFKS7MM\full-text.pdf}
}

@article{wickRescoringSequencetoSequenceModels2021,
  title = {Rescoring {{Sequence-to-Sequence Models}} for {{Text Line Recognition}} with {{CTC-Prefixes}}},
  author = {Wick, Christoph and Z{\"o}llner, Jochen and Gr{\"u}ning, Tobias},
  year = {2021},
  month = oct,
  eprint = {2110.05909},
  doi = {10.48550/arxiv.2110.05909},
  urldate = {2022-03-17},
  abstract = {In contrast to Connectionist Temporal Classification (CTC) approaches, Sequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer from errors such as skipped or repeated words which often occur at the end of a sequence. In this paper, to combine the best of both approaches, we propose to use the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths that are invalid according to the CTC confidence matrix are penalised. Our network architecture is composed of a Convolutional Neural Network (CNN) as visual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder, and a decoder which is a Transformer with inserted mutual attention layers. The CTC confidences are computed on the encoder while the Transformer is only used for character-wise S2S decoding. We evaluate this setup on three HTR data sets: IAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate (CER) of 2.95\% when pretraining our model on synthetic data and including a character-based language model for contemporary English. Compared to other state-of-the-art approaches, our model requires about 10-20 times less parameters. Access our shared implementations via this link to GitHub: https://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.},
  archiveprefix = {arXiv},
  keywords = {CTC,Doc-ument Analysis,Handwritten Text Recognition,Sequence-To-Sequence,Text Line Recognition},
  file = {C:\Users\tarchibald\Zotero\storage\C98KJBC8\full-text.pdf}
}

@article{wickTransformerHandwrittenText2021,
  title = {Transformer for {{Handwritten Text Recognition Using Bidirectional Post-decoding}}},
  author = {Wick, Christoph and Z{\"o}llner, Jochen and Gr{\"u}ning, Tobias},
  year = {2021},
  month = sep,
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {12823 LNCS},
  pages = {112--126},
  publisher = {Springer, Cham},
  issn = {16113349},
  doi = {10.1007/978-3-030-86334-0_8},
  urldate = {2022-03-17},
  abstract = {Most recently, Transformers -- which are recurrent-free neural network architectures -- achieved tremendous performances on various Natural Language Processing (NLP) tasks. Since Transformers represent a traditional Sequence-To-Sequence (S2S)-approach they can be used for several different tasks such as Handwritten Text Recognition (HTR). In this paper, we propose a bidirectional Transformer architecture for line-based HTR that is composed of a Convolutional Neural Network (CNN) for feature extraction and a Transformer-based encoder/decoder, whereby the decoding is performed in reading-order direction and reversed. A voter combines the two predicted sequences to obtain a single result. Our network performed worse compared to a traditional Connectionist Temporal Classification (CTC) approach on the IAM-dataset but reduced the state-of-the-art of Transformers-based approaches by about 25\% without using additional data. On a significantly larger dataset, the proposed Transformer significantly outperformed our reference model by about 26\%. In an error analysis, we show that the Transformer is able to learn a strong language model which explains why a larger training dataset is required to outperform traditional approaches and discuss why Transformers should be used with caution for HTR due to several shortcomings such as repetitions in the text.},
  isbn = {9783030863333},
  keywords = {Bidirectional,Handwritten Text Recognition,Transformer}
}

@misc{Wiedergutmachung2022,
  title = {Wiedergutmachung},
  year = {2022},
  month = may,
  urldate = {2023-08-23},
  abstract = {a dataset for printed and handwritten text separation},
  howpublished = {Information Service Engineering}
}

@article{wieluchStrokeCoderPathBasedImage2020,
  title = {{{StrokeCoder}}: {{Path-Based Image Generation}} from {{Single Examples}} Using {{Transformers}}},
  author = {Wieluch, Sabine and Schwenker, Friedhelm},
  year = {2020},
  month = mar,
  eprint = {2003.11958},
  urldate = {2020-06-11},
  abstract = {This paper demonstrates how a Transformer Neural Network can be used to learn a Generative Model from a single path-based example image. We further show how a data set can be generated from the example image and how the model can be used to generate a large set of deviated images, which still represent the original image's style and concept.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\VVIZR237\full-text.pdf}
}

@inproceedings{Wigington2018,
  title = {Data {{Augmentation}} for {{Recognition}} of {{Handwritten Words}} and {{Lines Using}} a {{CNN-LSTM Network}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Document Analysis}} and {{Recognition}}, {{ICDAR}}},
  author = {Wigington, Curtis and Stewart, Seth and Davis, Brian and Barrett, Bill and Price, Brian and Cohen, Scott},
  year = {2018},
  month = jan,
  volume = {1},
  pages = {639--645},
  publisher = {IEEE Computer Society},
  issn = {15205363},
  doi = {10.1109/ICDAR.2017.110},
  urldate = {2019-08-28},
  abstract = {{\copyright} 2017 IEEE. We introduce two data augmentation and normalization techniques, which, used with a CNN-LSTM, significantly reduce Word Error Rate (WER) and Character Error Rate (CER) beyond best-reported results on handwriting recognition tasks. (1) We apply a novel profile normalization technique to both word and line images. (2) We augment existing text images using random perturbations on a regular grid. We apply our normalization and augmentation to both training and test images. Our approach achieves low WER and CER over hundreds of authors, multiple languages and a variety of collections written centuries apart. Image augmentation in this manner achieves state-of-The-art recognition accuracy on several popular handwritten word benchmarks.},
  isbn = {978-1-5386-3586-5},
  keywords = {CNN,Data Augmentation,Deep Learning,Elastic Distortion,Handwriting Recognition,LSTM},
  file = {C:\Users\tarchibald\Zotero\storage\NRDZAMI3\full-text.pdf}
}

@techreport{wigingtonStartFollowRead,
  title = {Start, {{Follow}}, {{Read}}: {{End-to-End Full-Page Handwriting Recognition}}},
  author = {Wigington, Curtis and Tensmeyer, Chris and Davis, Brian and Barrett, William and Price, Brian and Cohen, Scott},
  urldate = {2021-05-26},
  abstract = {Despite decades of research, offline handwriting recognition (HWR) of degraded historical documents remains a challenging problem, which if solved could greatly improve the searchability of online cultural heritage archives. HWR models are often limited by the accuracy of the preceding steps of text detection and segmentation. Motivated by this, we present a deep learning model that jointly learns text detection, segmentation, and recognition using mostly images without detection or segmentation annotations. Our Start, Follow, Read (SFR) model is composed of a Region Proposal Network to find the start position of text lines, a novel line follower network that incrementally follows and preprocesses lines of (perhaps curved) text into dewarped images suitable for recognition by a CNN-LSTM network. SFR exceeds the performance of the winner of the ICDAR2017 handwriting recognition competition, even when not using the provided competition region annotations.},
  keywords = {Document Analysis,Handwriting Recognition,Historical Document Processing,Text Detection,Text Line Segmentation},
  file = {C:\Users\tarchibald\Zotero\storage\TZP3JFF2\full-text.pdf}
}

@inproceedings{Wrobel2019,
  title = {A Greedy Algorithm for Extraction of Handwritten Strokes},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Wr{\'o}bel, Micha{\l} and Starczewski, Janusz T. and Nieszporek, Katarzyna and Opie{\l}ka, Piotr and Ka{\'z}mierczak, Andrzej},
  year = {2019},
  volume = {11509 LNAI},
  pages = {464--473},
  publisher = {Springer Verlag},
  issn = {16113349},
  doi = {10.1007/978-3-030-20915-5_42},
  urldate = {2019-10-10},
  isbn = {978-3-030-20914-8},
  file = {C:\Users\tarchibald\Zotero\storage\2SU6CDBS\full-text.pdf}
}

@inproceedings{wrobelGreedyAlgorithmExtraction2019,
  title = {A Greedy Algorithm for Extraction of Handwritten Strokes},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Wr{\'o}bel, Micha{\l} and Starczewski, Janusz T. and Nieszporek, Katarzyna and Opie{\l}ka, Piotr and Ka{\'z}mierczak, Andrzej},
  year = {2019},
  volume = {11509 LNAI},
  pages = {464--473},
  publisher = {Springer Verlag},
  issn = {16113349},
  doi = {10.1007/978-3-030-20915-5_42},
  urldate = {2020-01-15},
  abstract = {This paper presents an algorithm for the extraction of handwritten strokes from a binary image. Each stroke is represented by a family of discs covering the area of the handwritten word. These discs are selected and connected to each other using heuristic (especially greedy) techniques.},
  isbn = {978-3-030-20914-8},
  file = {C:\Users\tarchibald\Zotero\storage\IPZ9BVBK\full-text.pdf}
}

@incollection{xieDewarpingDocumentImage2020,
  title = {Dewarping {{Document Image}} by {{Displacement Flow Estimation}} with {{Fully Convolutional Network}}},
  author = {Xie, Guo-Wang and Yin, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin},
  year = {2020},
  month = jul,
  pages = {131--144},
  publisher = {Springer, Cham},
  doi = {10.1007/978-3-030-57058-3_10},
  urldate = {2020-09-10},
  abstract = {As camera-based documents are increasingly used, the rec-tification of distorted document images becomes a need to improve the recognition performance. In this paper, we propose a novel framework for both rectifying distorted document image and removing background finely, by estimating pixel-wise displacements using a fully convolutional network (FCN). The document image is rectified by transformation according to the displacements of pixels. The FCN is trained by regressing displacements of synthesized distorted documents, and to control the smoothness of displacements, we propose a Local Smooth Constraint (LSC) in regularization. Our approach is easy to implement and consumes moderate computing resource. Experiments proved that our approach can dewarp document images effectively under various geometric distortions, and has achieved the state-of-the-art performance in terms of local details and overall effect.},
  keywords = {Dewarping document image,Fully convolutional network,Local Smooth Constraint,Pixel-wise displacement},
  file = {C:\Users\tarchibald\Zotero\storage\EMPNLCPZ\full-text.pdf}
}

@inproceedings{xiongCanLLMsExpress2023,
  title = {Can {{LLMs Express Their Uncertainty}}? {{An Empirical Evaluation}} of {{Confidence Elicitation}} in {{LLMs}}},
  shorttitle = {Can {{LLMs Express Their Uncertainty}}?},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  year = {2023},
  month = oct,
  urldate = {2025-05-31},
  abstract = {Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on *white-box access* to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of *black-box* approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: *prompting* strategies for eliciting verbalized confidence, *sampling* methods for generating multiple responses, and *aggregation* techniques for computing consistency. We then benchmark these methods on two key tasks---confidence calibration and failure prediction---across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be *overconfident*, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve, yet still far from ideal performance. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs. The code is publicly available at https://github.com/MiaoXiong2320/llm-uncertainty.},
  langid = {english},
  file = {C:\Users\tarchibald\Zotero\storage\ABQBUXZF\Xiong et al. - 2023 - Can LLMs Express Their Uncertainty An Empirical E.pdf}
}

@article{xuDeepLearningFreeHand2020,
  title = {Deep {{Learning}} for {{Free-Hand Sketch}}: {{A Survey}} and {{A Toolbox}}},
  author = {Xu, Peng and Hospedales, Timothy M. and Yin, Qiyue and Song, Yi-Zhe and Xiang, Tao and Wang, Liang},
  year = {2020},
  month = jan,
  eprint = {2001.02600},
  urldate = {2020-06-11},
  abstract = {Free-hand sketches are highly illustrative, and have been widely used by humans to depict objects or stories from ancient times to the present. The recent prevalence of touchscreen devices has made sketch creation a much easier task than ever and consequently made sketch-oriented applications increasingly popular. The progress of deep learning has immensely benefited free-hand sketch research and applications. This paper presents a comprehensive survey of the deep learning techniques oriented at free-hand sketch data, and the applications that they enable. The main contents of this survey include: (i) A discussion of the intrinsic traits and unique challenges of free-hand sketch, to highlight the essential differences between sketch data and other data modalities, e.g., natural photos. (ii) A review of the developments of free-hand sketch research in the deep learning era, by surveying existing datasets, research topics, and the state-of-the-art methods through a detailed taxonomy and experimental evaluation. (iii) Promotion of future work via a discussion of bottlenecks, open problems, and potential research directions for the community. Finally, to support future sketch research and applications, we contribute TorchSketch -- the first sketch-oriented open-source deep learning library, which is built on PyTorch and available at https://github.com/PengBoXiangShang/torchsketch/.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\9AXRFDPE\full-text.pdf}
}

@article{xuLargeLanguageModels2024,
  title = {Large Language Models for Generative Information Extraction: A Survey},
  shorttitle = {Large Language Models for Generative Information Extraction},
  author = {Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  year = {2024},
  month = nov,
  journal = {Frontiers of Computer Science},
  volume = {18},
  number = {6},
  pages = {186357},
  issn = {2095-2236},
  doi = {10.1007/s11704-024-40555-y},
  urldate = {2025-07-07},
  abstract = {Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).},
  langid = {english},
  keywords = {Computational Linguistics,information extraction,Information Model,Language Processing,large language models,Literature mining,Machine Translation,Natural Language Processing (NLP),review},
  file = {C:\Users\tarchibald\Zotero\storage\CSBZBVLR\Xu et al. - 2024 - Large language models for generative information e.pdf}
}

@misc{xuLargeLanguageModels2024preprint,
  title = {Large {{Language Models}} for {{Generative Information Extraction}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Generative Information Extraction}}},
  author = {Xu, Derong and Chen, Wei and Peng, Wenjun and Zhang, Chao and Xu, Tong and Zhao, Xiangyu and Wu, Xian and Zheng, Yefeng and Wang, Yang and Chen, Enhong},
  year = {2024},
  month = oct,
  number = {arXiv:2312.17617},
  eprint = {2312.17617},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.17617},
  urldate = {2025-05-31},
  abstract = {Information extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub ({\textbackslash}href\{https://github.com/quqxui/Awesome-LLM4IE-Papers\}\{LLM4IE repository\})},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\49VV4RJF\\Xu et al. - 2024 - Large Language Models for Generative Information E.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\4XDTFXXW\\2312.html}
}

@inproceedings{xuLayoutLMPretrainingText2020,
  title = {{{LayoutLM}}: {{Pre-training}} of {{Text}} and {{Layout}} for {{Document Image Understanding}}},
  shorttitle = {{{LayoutLM}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  year = {2020},
  month = aug,
  eprint = {1912.13318},
  primaryclass = {cs},
  pages = {1192--1200},
  doi = {10.1145/3394486.3403172},
  urldate = {2025-05-31},
  abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the {\textbackslash}textbf\{LayoutLM\} to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlm\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\tarchibald\Zotero\storage\U6L7LV9R\Xu et al. - 2020 - LayoutLM Pre-training of Text and Layout for Docu.pdf}
}

@misc{yangDawnLMMsPreliminary2023,
  title = {The {{Dawn}} of {{LMMs}}: {{Preliminary Explorations}} with {{GPT-4V}}(Ision)},
  shorttitle = {The {{Dawn}} of {{LMMs}}},
  author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  year = {2023},
  month = oct,
  number = {arXiv:2309.17421},
  eprint = {2309.17421},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.17421},
  urldate = {2025-05-31},
  abstract = {Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\tarchibald\Zotero\storage\FWQRR2XD\Yang et al. - 2023 - The Dawn of LMMs Preliminary Explorations with GP.pdf}
}

@inproceedings{yangLearningExtractSemantic2017,
  title = {Learning to {{Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Lee Giles, C.},
  year = {2017},
  pages = {5315--5324},
  urldate = {2023-08-16},
  file = {C:\Users\tarchibald\Zotero\storage\PCDEQDW4\Yang et al. - 2017 - Learning to Extract Semantic Structure From Docume.pdf}
}

@article{youngBenchmarkingMultipleLarge2025,
  title = {Benchmarking {{Multiple Large Language Models}} for {{Automated Clinical Trial Data Extraction}} in {{Aging Research}}},
  author = {Young, Richard J. and Matthews, Alice M. and Poston, Brach},
  year = {2025},
  month = may,
  journal = {Algorithms},
  volume = {18},
  number = {5},
  pages = {296},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  doi = {10.3390/a18050296},
  urldate = {2025-06-03},
  abstract = {Large-language models (LLMs) show promise for automating evidence synthesis, yet head-to-head evaluations remain scarce. We benchmarked five state-of-the-art LLMs---openai/o1-mini, x-ai/grok-2-1212, meta-llama/Llama-3.3-70B-Instruct, google/Gemini-Flash-1.5-8B, and deepseek/DeepSeek-R1-70B-Distill---on extracting protocol details from transcranial direct-current stimulation (tDCS) trials enrolling older adults. A multi-LLM ensemble pipeline ingested ClinicalTrials.gov records, applied a structured JSON schema, and generated comparable outputs from unstructured text. The pipeline retrieved 83 aging-related tDCS trials---roughly double the yield of a conventional keyword search. Across models, agreement was almost perfect for the binary field brain stimulation used (Fleiss {$\kappa$} {$\approx$} 0.92) and substantial for the categorical primary target ({$\kappa$} {$\approx$} 0.71). Numeric parameters such as stimulation intensity and session duration showed excellent consistency when explicitly reported (ICC 0.95--0.96); secondary targets and free-text duration phrases remained challenging ({$\kappa$} {$\approx$} 0.61; ICC {$\approx$} 0.35). An ensemble consensus (majority vote or averaging) resolved most disagreements and delivered near-perfect reliability on core stimulation attributes ({$\kappa$} = 0.94). These results demonstrate that multi-LLM ensembles can markedly expand trial coverage and reach expert-level accuracy on well-defined fields while still requiring human oversight for nuanced or sparsely reported details. The benchmark and open-source workflow set a solid baseline for future advances in prompt engineering, model specialization, and ensemble strategies aimed at fully automated evidence synthesis in neurostimulation research involving aging populations. Overall, the five-model multi-LLM ensemble doubled the number of eligible aging-related tDCS trials retrieved versus keyword searching and achieved near-perfect agreement on core stimulation parameters ({$\kappa$} {$\approx$} 0.94), demonstrating expert-level extraction accuracy.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {aging research,API integration,automated data extraction,clinical trial data extraction,evidence synthesis,large language models (LLMs),multi-agent systems,protocol analysis,systematic review methodology,transcranial direct current stimulation (tDCS)},
  file = {C:\Users\tarchibald\Zotero\storage\HRVG82W5\Young et al. - 2025 - Benchmarking Multiple Large Language Models for Au.pdf}
}

@article{yuAssessingPhrasalRepresentation2020,
  title = {Assessing {{Phrasal Representation}} and {{Composition}} in {{Transformers}}},
  author = {Yu, Lang and Ettinger, Allyson},
  year = {2020},
  month = oct,
  eprint = {2010.03763},
  pages = {4896--4907},
  publisher = {Association for Computational Linguistics (ACL)},
  urldate = {2021-10-13},
  abstract = {Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\3N9B4MYG\full-text.pdf}
}

@article{yuEfficientLanguageModeling2022,
  title = {Efficient {{Language Modeling}} with {{Sparse}} All-{{MLP}}},
  author = {Yu, Ping and Artetxe, Mikel and Ott, Myle and Shleifer, Sam and Gong, Hongyu and Stoyanov, Ves and Li, Xian},
  year = {2022},
  month = mar,
  eprint = {2203.06850},
  doi = {10.48550/arxiv.2203.06850},
  urldate = {2022-03-16},
  abstract = {All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2\${\textbackslash}times\$ improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\T94VHNV9\full-text.pdf}
}

@article{yuEfficientLanguageModeling2022a,
  title = {Efficient {{Language Modeling}} with {{Sparse}} All-{{MLP}}},
  author = {Yu, Ping and Artetxe, Mikel and Ott, Myle and Shleifer, Sam and Gong, Hongyu and Stoyanov, Ves and Li, Xian},
  year = {2022},
  month = mar,
  eprint = {2203.06850},
  doi = {10.48550/arxiv.2203.06850},
  urldate = {2022-03-18},
  abstract = {All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2\${\textbackslash}times\$ improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\ZDEAVMUL\full-text.pdf}
}

@misc{yuICDAR2023Competition2023,
  title = {{{ICDAR}} 2023 {{Competition}} on {{Reading}} the {{Seal Title}}},
  author = {Yu, Wenwen and Liu, Mingyu and Chen, Mingrui and Lu, Ning and Wen, Yinlong and Liu, Yuliang and Karatzas, Dimosthenis and Bai, Xiang},
  year = {2023},
  month = jun,
  number = {arXiv:2304.11966},
  eprint = {2304.11966},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11966},
  urldate = {2023-08-21},
  abstract = {Reading seal title text is a challenging task due to the variable shapes of seals, curved text, background noise, and overlapped text. However, this important element is commonly found in official and financial scenarios, and has not received the attention it deserves in the field of OCR technology. To promote research in this area, we organized ICDAR 2023 competition on reading the seal title (ReST), which included two tasks: seal title text detection (Task 1) and end-to-end seal title recognition (Task 2). We constructed a dataset of 10,000 real seal data, covering the most common classes of seals, and labeled all seal title texts with text polygons and text contents. The competition opened on 30th December, 2022 and closed on 20th March, 2023. The competition attracted 53 participants from academia and industry including 28 submissions for Task 1 and 25 submissions for Task 2, which demonstrated significant interest in this challenging task. In this report, we present an overview of the competition, including the organization, challenges, and results. We describe the dataset and tasks, and summarize the submissions and evaluation results. The results show that significant progress has been made in the field of seal title text reading, and we hope that this competition will inspire further research and development in this important area of OCR technology.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\QGFJEFJJ\\Yu et al. - 2023 - ICDAR 2023 Competition on Reading the Seal Title.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\I9ZQPM6I\\2304.html}
}

@article{zamirFeedbackNetworks2016,
  title = {Feedback {{Networks}}},
  author = {Zamir, Amir R. and Wu, Te-Lin and Sun, Lin and Shen, William and Malik, Jitendra and Savarese, Silvio},
  year = {2016},
  month = dec,
  journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume = {2017-January},
  eprint = {1612.09508},
  pages = {1808--1817},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2021-02-27},
  abstract = {Currently, the most successful learning models in computer vision are based on learning successive representations followed by a decision layer. This is usually actualized through feedforward multilayer neural networks, e.g. ConvNets, where each layer forms one of such successive representations. However, an alternative that can achieve the same goal is a feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration's output. We establish that a feedback based approach has several fundamental advantages over feedforward: it enables making early predictions at the query time, its output naturally conforms to a hierarchical structure in the label space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning. We observe that feedback networks develop a considerably different representation compared to feedforward counterparts, in line with the aforementioned advantages. We put forth a general feedback based learning architecture with the endpoint results on par or better than existing feedforward networks with the addition of the above advantages. We also investigate several mechanisms in feedback architectures (e.g. skip connections in time) and design choices (e.g. feedback length). We hope this study offers new perspectives in quest for more natural and practical learning models.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\4RFGEPVI\full-text.pdf}
}

@article{zamora-martinezNeuralNetworkLanguage2014,
  title = {Neural Network Language Models for Off-Line Handwriting Recognition},
  author = {{Zamora-Mart{\'i}nez}, F. and Frinken, V. and {Espa{\~n}a-Boquera}, S. and {Castro-Bleda}, M. J. and Fischer, A. and Bunke, H.},
  year = {2014},
  month = apr,
  journal = {Pattern Recognition},
  volume = {47},
  number = {4},
  pages = {1642--1652},
  publisher = {Pergamon},
  issn = {00313203},
  doi = {10.1016/j.patcog.2013.10.020},
  urldate = {2021-05-26},
  abstract = {Unconstrained off-line continuous handwritten text recognition is a very challenging task which has been recently addressed by different promising techniques. This work presents our latest contribution to this task, integrating neural network language models in the decoding process of three state-of-the-art systems: one based on bidirectional recurrent neural networks, another based on hybrid hidden Markov models and, finally, a combination of both. Experimental results obtained on the IAM off-line database demonstrate that consistent word error rate reductions can be achieved with neural network language models when compared with statistical N-gram language models on the three tested systems. The best word error rate, 16.1\%, reported with ROVER combination of systems using neural network language models significantly outperforms current benchmark results for the IAM database. {\copyright} 2013 Elsevier Ltd.},
  keywords = {Bidirectional long short-term memory neural networks (BLSTM),Handwritten text recognition (HTR),Hybrid HMM/ANN models,Language models (LMs),Neural network language model (NN LM),Neural networks (NNs),ROVER combination},
  file = {C:\Users\tarchibald\Zotero\storage\PI3IV2MV\full-text.pdf}
}

@inproceedings{zbontarBarlowTwinsSelfSupervised2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  year = {2021},
  month = jul,
  pages = {12310--12320},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-23},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  langid = {english},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\C6ALCVTS\\Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\HME6MQKD\\Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf}
}

@article{zellersRecognitionCognitionVisual2018,
  title = {From {{Recognition}} to {{Cognition}}: {{Visual Commonsense Reasoning}}},
  author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  year = {2018},
  eprint = {1811.10830},
  abstract = {Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90\% accuracy), state-of-the-art vision models struggle ({\textasciitilde}45\%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines ({\textasciitilde}65\%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\BUX36SWJ\1096939380.pdf}
}

@misc{Zhang2019,
  title = {Drawing {{Order Recovery}} Based on Deep Learning},
  author = {Zhang, Rui and Chen, Jinlong and Yang, Minghao},
  year = {2019},
  month = jul,
  pages = {129--133},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/icaci.2019.8778533},
  urldate = {2019-10-05},
  howpublished = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=8778533},
  isbn = {9781538677322},
  file = {C:\Users\tarchibald\Zotero\storage\4YNVLMZX\Unknown - Unknown - Drawing Order Recovery based on deep learning.pdf}
}

@article{zhangApvitViTAdaptive2025,
  title = {Apvit: {{ViT}} with Adaptive Patches for Scene Text Recognition},
  shorttitle = {Apvit},
  author = {Zhang, Ning and Li, Ce and Wang, Zongshun and Ma, Jialin and Feng, Zhiqiang},
  year = {2025},
  month = mar,
  journal = {Discover Applied Sciences},
  volume = {7},
  number = {4},
  pages = {245},
  issn = {3004-9261},
  doi = {10.1007/s42452-025-06570-9},
  urldate = {2025-05-14},
  abstract = {Scene texts in nature exhibit varied colors, which serve as a significant distinguishing feature that effectively suppresses background interference. In this study, color clustering is utilized as a prior guide to group patches, enhancing their spatial relationships. Additionally, patch sizes are adaptively adjusted during training to balance speed and accuracy, while unimportant tokens and blocks in the model are pruned. We propose APViT, which modifies the ViTs model for scene text recognition requirements. It consists of three components: Sparse Patches Selection (SPS), ViT-STR, and Token Code (TC). First, SPS segments images into appropriate patches and clusters similar ones to explore diverse local patches adaptively. Second, we enhance the ViTs model specifically for scene text recognition as ViT-STR. Finally, TC prunes non-essential parts of the network based on self-attention mechanisms to accelerate performance. Consequently, our proposed APViT model outperforms state-of-the-art methods across several datasets, demonstrating its effectiveness.},
  langid = {english},
  keywords = {Adaptive patches,Automated Pattern Recognition,Biometrics,Computer Vision,Image Processing,Object Recognition,Prune,Scene text recognition,Visual system,ViTs}
}

@misc{zhangConsensusEntropyHarnessing2025,
  title = {Consensus {{Entropy}}: {{Harnessing Multi-VLM Agreement}} for {{Self-Verifying}} and {{Self-Improving OCR}}},
  shorttitle = {Consensus {{Entropy}}},
  author = {Zhang, Yulong and Liang, Tianyi and Huang, Xinyue and Cui, Erfei and Guo, Xu and Chu, Pei and Li, Chenhui and Zhang, Ru and Wang, Wenhai and Liu, Gongshen},
  year = {2025},
  month = apr,
  number = {arXiv:2504.11101},
  eprint = {2504.11101},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.11101},
  urldate = {2025-05-26},
  abstract = {The Optical Character Recognition (OCR) task is important for evaluating Vision-Language Models (VLMs) and providing high-quality data sources for LLM training data. While state-of-the-art VLMs show improved average OCR accuracy, they still struggle with sample-level quality degradation and lack reliable automatic detection of low-quality outputs. We introduce Consensus Entropy (CE), a training-free post-inference method that quantifies OCR uncertainty by aggregating outputs from multiple VLMs. Our approach exploits a key insight: correct VLM OCR predictions converge in output space while errors diverge. We develop a lightweight multi-model framework that effectively identifies problematic samples, selects the best outputs and combines model strengths. Experiments across multiple OCR benchmarks and VLMs demonstrate that CE outperforms VLM-as-judge approaches and single-model baselines at the same cost and achieves state-of-the-art results across multiple metrics. For instance, our solution demonstrates: achieving 15.2\% higher F1 scores than VLM-as-judge methods in quality verification, delivering 6.0\% accuracy gains on mathematical calculation tasks, and requiring rephrasing only 7.3\% of inputs while maintaining overall performance. Notably, the entire process requires neither training nor supervision while maintaining plug-and-play functionality throughout.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia}
}

@article{zhangEXTENDINGTROCRTEXT,
  title = {{{EXTENDING TROCR FOR TEXT LOCALIZATION-FREE OCR OF FULL-PAGE SCANNED RECEIPT IMAGES}}},
  author = {Zhang, Hongkuan and Whittaker, Edward and Kitagishi, Ikuo and Best, {\dag} K K and Research, Path},
  eprint = {2212.05525v2},
  urldate = {2023-04-10},
  abstract = {Digitization of scanned receipts aims to extract text from receipt images and save it into structured documents. This is usually split into two sub-tasks: text localization and optical character recognition (OCR). Most existing OCR models only focus on the cropped text instance images, which require the bounding box information provided by a text region detection model. Introducing an additional detector to identify the text instance images in advance is inefficient, however instance-level OCR models have very low accuracy when processing the whole image for the document-level OCR, such as receipt images containing multiple text lines arranged in various layouts. To this end, we propose a localization-free document-level OCR model for transcribing all the characters in a receipt image into an ordered sequence end-to-end. Specifically , we finetune the pretrained Transformer-based instance-level model TrOCR with randomly cropped image chunks, and gradually increase the image chunk size to generalize the recognition ability from instance images to full-page images. In our experiments on the SROIE receipt OCR dataset, the model finetuned with our strategy achieved 64.4 F1-score and a 22.8\% character error rates (CER) on the word-level and character-level metrics, respectively, which outperforms the baseline results with 48.5 F1-score and 50.6\% CER. The best model, which splits the full image into 15 equally sized chunks, gives 87.8 F1-score and 4.98\% CER with minimal additional pre or post-processing of the output. Moreover, the characters in the generated document-level sequences are arranged in the reading order, which is practical for real-world applications.},
  archiveprefix = {arXiv},
  keywords = {End-to-End Receipt OCR,Index Terms-Receipt Digitization,Localization-Free OCR},
  file = {C:\Users\tarchibald\Zotero\storage\P82A5SHK\full-text.pdf}
}

@misc{zhangSurveyTestTimeScaling2025,
  title = {A {{Survey}} on {{Test-Time Scaling}} in {{Large Language Models}}: {{What}}, {{How}}, {{Where}}, and {{How Well}}?},
  shorttitle = {A {{Survey}} on {{Test-Time Scaling}} in {{Large Language Models}}},
  author = {Zhang, Qiyuan and Lyu, Fuyuan and Sun, Zexu and Wang, Lei and Zhang, Weixu and Hua, Wenyue and Wu, Haolun and Guo, Zhihan and Wang, Yufei and Muennighoff, Niklas and King, Irwin and Liu, Xue and Ma, Chen},
  year = {2025},
  month = may,
  number = {arXiv:2503.24235},
  eprint = {2503.24235},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.24235},
  urldate = {2025-06-03},
  abstract = {As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q\&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\P643BYP9\\Zhang et al. - 2025 - A Survey on Test-Time Scaling in Large Language Mo.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\MJGEBCAL\\2503.html}
}

@inproceedings{Zhao2018,
  title = {Pen {{Tip Motion Prediction}} for {{Handwriting Drawing Order Recovery}} Using {{Deep Neural Network}}},
  booktitle = {Proceedings - {{International Conference}} on {{Pattern Recognition}}},
  author = {Zhao, Bocheng and Yang, Minghao and Tao, Jianhua},
  year = {2018},
  month = nov,
  volume = {2018-Augus},
  pages = {704--709},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10514651},
  doi = {10.1109/ICPR.2018.8546086},
  urldate = {2019-10-05},
  abstract = {{\copyright} 2018 IEEE. Pen Tip Motion Prediction (PTMP) is the key step for Chinese handwriting order recovery (DOR), which is a challenge topic in the past few decades. We proposed a novel algorithm framework using Convolutional Neural Network (CNN) to predict pen tip movement for human handwriting pictures. The network is a regression CNN model, whose inputs are a series of part-drawn handwriting images and output is a vector that represents the probability of next stroke point position. The predicted output vector is utilized by an iteration framework to generate pen movement sequences. Experiments on public Chinese and English online handwriting database have indicated that the proposed model performs competitively in multi-writer handwriting PTMP and DOR tasks. Furthermore, the experiment demonstrated that characters belong to different languages shares some common writing patterns and the proposed method could learn these laws effectively.},
  isbn = {978-1-5386-3788-3},
  keywords = {convolutional neural network,Drawing order,image to sequence model,markov decision process},
  file = {C:\Users\tarchibald\Zotero\storage\39XS8P74\full-text.pdf}
}

@inproceedings{zhongPubLayNetLargestDataset2019,
  title = {{{PubLayNet}}: {{Largest Dataset Ever}} for {{Document Layout Analysis}}},
  shorttitle = {{{PubLayNet}}},
  booktitle = {2019 {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Zhong, Xu and Tang, Jianbin and Jimeno Yepes, Antonio},
  year = {2019},
  month = sep,
  pages = {1015--1022},
  issn = {2379-2140},
  doi = {10.1109/ICDAR.2019.00166},
  abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
  keywords = {Australia,automatic annotation document layout deep learning transfer learning,Layout,Proteins,Retina,Text analysis,Veins,XML},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\7IYTENH8\\Zhong et al. - 2019 - PubLayNet Largest Dataset Ever for Document Layou.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\Z96SRD8P\\8977963.html}
}

@misc{zhouAnalyzingMitigatingObject2024,
  title = {Analyzing and {{Mitigating Object Hallucination}} in {{Large Vision-Language Models}}},
  author = {Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  year = {2024},
  month = mar,
  number = {arXiv:2310.00754},
  eprint = {2310.00754},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.00754},
  urldate = {2025-06-04},
  abstract = {Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23\% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\tarchibald\\Zotero\\storage\\SK4KQVMV\\Zhou et al. - 2024 - Analyzing and Mitigating Object Hallucination in L.pdf;C\:\\Users\\tarchibald\\Zotero\\storage\\9UHNH93T\\2310.html}
}

@article{zhouNoisyMachinesUnderstanding2020,
  title = {Noisy {{Machines}}: {{Understanding Noisy Neural Networks}} and {{Enhancing Robustness}} to {{Analog Hardware Errors Using Distillation}}},
  author = {Zhou, Chuteng and Kadambi, Prad and Mattina, Matthew and Whatmough, Paul N.},
  year = {2020},
  month = jan,
  eprint = {2001.04974},
  urldate = {2021-10-21},
  abstract = {The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. In particular, analog computing hardware has been heavily motivated specifically for accelerating neural networks, based on either electronic, optical or photonic devices, which may well achieve lower power consumption than conventional digital electronics. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, we advance the understanding of noisy neural networks. We outline how a noisy neural network has reduced learning capacity as a result of loss of mutual information between its input and output. To combat this, we propose using knowledge distillation combined with noise injection during training to achieve more noise robust networks, which is demonstrated experimentally across different networks and datasets, including ImageNet. Our method achieves models with as much as two times greater noise tolerance compared with the previous best attempts, which is a significant step towards making analog hardware practical for deep learning.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\29QCCJD3\full-text.pdf}
}

@article{zhouReviewDocumentImage2023,
  title = {A {{Review}} of {{Document Image Enhancement Based}} on {{Document Degradation Problem}}},
  author = {Zhou, Yanxi and Zuo, Shikai and Yang, Zhengxian and He, Jinlong and Shi, Jianwen and Zhang, Rui},
  year = {2023},
  month = jan,
  journal = {Applied Sciences},
  volume = {13},
  number = {13},
  pages = {7855},
  issn = {2076-3417},
  doi = {10.3390/app13137855},
  urldate = {2024-02-01},
  abstract = {Document image enhancement methods are often used to improve the accuracy and efficiency of automated document analysis and recognition tasks such as character recognition. These document images could be degraded or damaged for various reasons including aging, fading handwriting, poor lighting conditions, watermarks, etc. In recent years, with the improvement of computer performance and the continuous development of deep learning, many methods have been proposed to enhance the quality of these document images. In this paper, we review six tasks of document degradation, namely, background texture, page smudging, fading, poor lighting conditions, watermarking, and blurring. We summarize the main models for each degradation problem as well as recent work, such as the binarization model that can be used to deal with the degradation of background textures, lettering smudges. When facing the problem of fading, a model for stroke connectivity can be used, while the other three degradation problems are mostly deep learning models. We discuss the current limitations and challenges of each degradation task and introduce the common public datasets and metrics. We identify several promising research directions and opportunities for future research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural network,deep learning,degradation,document image analysis and recognition,document image enhancement},
  file = {C:\Users\tarchibald\Zotero\storage\9E2ZN4IP\Zhou et al. - 2023 - A Review of Document Image Enhancement Based on Do.pdf}
}

@article{zhuUnpairedImagetoImageTranslation2017a,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  month = mar,
  journal = {Proceedings of the IEEE International Conference on Computer Vision},
  volume = {2017-October},
  eprint = {1703.10593},
  pages = {2242--2251},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  urldate = {2020-10-28},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archiveprefix = {arXiv},
  file = {C:\Users\tarchibald\Zotero\storage\EHBLZSND\full-text.pdf}
}
