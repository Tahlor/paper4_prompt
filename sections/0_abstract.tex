\begin{abstract}
Large language and multimodal models are increasingly used for document transcription and structured data extraction, but their confidence signals remain opaque and unreliable. This paper introduces a review-budget evaluation framework that quantifies the tradeoff between human review effort and post-review accuracy, offering a more operational metric than raw accuracy or character error rate alone. We systematically evaluate multiple uncertainty heuristics—logit-derived entropy, model self-reported confidence, and ensemble agreement—across modern reasoning-oriented models. Our experiments reveal that (1) logit confidence is poorly calibrated for reasoning models, (2) self-reported confidence from Gemini 2.0 outperforms logit-based scores, (3) ordinal confidence scales (1–10) yield more discriminative ranking than normalized 0–1 scales, and (4) prompt granularity—literal versus contextual or character-level transcription—strongly affects calibration. We further demonstrate that prompt and model ensembles achieve superior review-budget tradeoffs, suggesting that diversity at inference time is a practical route to uncertainty quantification. Together, these findings provide the first systematic characterization of confidence reliability in modern LLM transcription systems and establish a foundation for human-in-loop evaluation under real deployment constraints.
\end{abstract}