The challenge of uncertainty quantification (UQ) in Large Language Models (LLMs) is a broad and active field of research, as models that can signal their own uncertainty are critical for trustworthy deployment~\cite{liuUncertaintyQuantificationConfidence2025, shorinwaSurveyUncertaintyQuantification2024}. This research is fragmented, prompting efforts to create unified benchmarks like LM-Polygraph to systematically evaluate UQ techniques on text-only tasks~\cite{vashurinBenchmarkingUncertaintyQuantification2025}.

Traditionally, confidence is derived from the model's internal state, specifically the token probabilities (TPU) or logits from its final output layer~\cite{maEstimatingLLMUncertainty2025}. However, these raw probabilities are often miscalibrated, leading to overconfidence. A significant body of work on text-only LLMs focuses on \emph{calibration}â€”post-processing these probabilities (e.g., with temperature scaling) or fine-tuning models specifically to produce calibrated outputs~\cite{kapoorCalibrationTuningTeachingLarge2024}. This challenge extends to Multimodal Large Language Models (MLLMs), which also exhibit significant miscalibration on general visual question answering (VQA) tasks~\cite{chenUnveilingUncertaintyDeep2024, kostumovUncertaintyAwareEvaluationVisionLanguage2024}. While this logprob-based approach serves as a ``gold standard,'' its application to the fine-grained, text-centric task of document transcription remains less explored.

\subsection{Confidence Proxies for Black-Box API Models}

A primary obstacle for practitioners is that most state-of-the-art models are accessible only via black-box APIs, which do not expose logits. This necessitates a shift to \emph{black-box confidence proxies} derived from the model's generated output. Our work systematically evaluates three main families of such proxies, whose literature is largely confined to text-only applications.

First is \textbf{verbalized confidence}, where the model is prompted to express its own uncertainty. Seminal work on text-only QA tasks demonstrated that models can be taught to generate calibrated confidence statements (e.g., ``I am 90\% confident'')~\cite{linTeachingModelsExpress2022} or to estimate the probability that their answer is correct~\cite{kadavathLanguageModelsMostly2022}. However, this method is contested: some find verbalized confidence to be \emph{better} calibrated than logprobs~\cite{tianJustAskCalibration2023}, while others find models are chronically \emph{overconfident} in their self-assessments on text-only reasoning tasks~\cite{xiongCanLLMsExpress2023, pawitanConfidenceReasoningLarge2025}. This line of inquiry has been almost exclusively conducted on text-only benchmarks.

Second is \textbf{justification-based confidence}, which infers uncertainty from the model's reasoning process. For text-only tasks, recent work supports this, showing that the length of a reasoning trace~\cite{devicTraceLengthSimple2025} or the mere act of reasoning~\cite{podolakReadYourOwn2025} helps surface internal confidence signals. However, this may come at a ``reasoning tax,'' where models trained for text-based reasoning become less aware of their own knowledge boundaries~\cite{zengThinkingOutLoud2025}. These signals have not been systematically evaluated for MLLM transcription, where the ``reasoning'' pertains to visual evidence.

Third is \textbf{ensemble-based confidence}, which uses disagreement among multiple outputs as a proxy for uncertainty. This is a classic technique, originating in speech recognition with methods like ROVER~\cite{fiscusPostprocessingSystemYield1997}. In modern LLMs, this has re-emerged as ``self-consistency'' for arithmetic reasoning tasks~\cite{wangSelfConsistencyImprovesChain2022}. This concept of using ``response agreement'' on text-only tasks is now a widely-used black-box method for improving both accuracy and calibration~\cite{liThinkTwiceAssure2024, xiaInfluencesLLMCalibration2025}.

While these proxy families are well-studied for text-only LLMs, how they compare against each other in a unified, black-box \emph{document transcription} setting remains an open question.

\subsection{Evaluation Beyond Scalar Metrics: Accuracy-Effort Tradeoffs}

Our work diverges from traditional evaluation by focusing on the operational tradeoff between accuracy and human review effort. This move is motivated by the growing critique of static benchmarks, which are increasingly susceptible to training data contamination~\cite{sainzNLPEvaluationTrouble2023} and fail to capture the complex, real-world behaviors of modern models~\cite{mcintoshInadequaciesLargeLanguage2025}.

Recent literature argues for new evaluation paradigms centered on the \emph{cost} of inference. This includes developing ``budget-aware'' evaluations that plot accuracy against compute~\cite{wangReasoningTokenEconomies2024}, studying ``inference scaling laws'' that trade model size for compute budget~\cite{wuInferenceScalingLaws2024}, and designing ``cost-aware routing'' to select the cheapest model for a given query~\cite{shirkavandCostAwareContrastiveRouting2025}. While some frame this as a purely ``economic evaluation''~\cite{zellingerEconomicEvaluationLLMs2025}, the core idea is to map the Pareto frontier of accuracy and inference cost.

Our ``review-budget'' framework adopts this principle, applying it not to inference-compute, but to the \emph{human-in-the-loop (HITL) effort}. The mechanism for implementing such a framework is often ``selective classification,'' or teaching a model to ``know when to abstain''~\cite{hengKnowWhenAbstain2025} or ``learn to refuse''~\cite{caoLearnRefuseMaking2024}. This directly models the HITL workflow, where the goal is to optimize the final system accuracy given a fixed budget for manual review~\cite{bagdouriMinimizingAnnotationCost2013}. Conformal prediction (CP) is a formal method for achieving this~\cite{kumarConformalPredictionLarge2023, kuwaharaDocumentSummarizationConformal2025}, though most CP methods rely on logprobs, motivating the need for black-box proxy comparisons~\cite{suAPIEnoughConformal2024}.

\subsection{Application to Document AI and Transcription}

We situate our investigation within the domain of document transcription and extraction~\cite{alkendiAdvancementsChallengesHandwritten2024, dingSurveyMLLMbasedVisually2025}. MLLMs are increasingly benchmarked as replacements for traditional Handwritten Text Recognition (HTR) and OCR systems~\cite{crosillaBenchmarkingLargeLanguage2025, greifMultimodalLLMsOCR2025}.

This domain has long utilized ensemble and voting methods to improve accuracy. Classic work in OCR used consensus sequence voting~\cite{loprestiUsingConsensusSequence1997} and HMM-based ensembles~\cite{bertolamiHiddenMarkovModelbased2008} to reduce error rates. This trend continues today, with MLLM-based pipelines using ensembles~\cite{archibaldImprovingMLLMHistorical2025}, voting~\cite{abdellatifLMVRPALargeModel2024, youngBenchmarkingMultipleLarge2025}, and ``consensus entropy''~\cite{zhangConsensusEntropyHarnessing2025} to improve transcription and extraction.

Critically, confidence is a known challenge in this specific domain. Scene-text recognition models are known to be poorly calibrated~\cite{slossbergCalibrationScenetextRecognition2022}, and MLLMs often ``hallucinate'' text on degraded documents~\cite{heSeeingBelievingMitigating2025}. This has led to specific work on using OCR confidence scores for error detection~\cite{hemmerConfidenceAwareDocumentOCR2024} and even attempting to fine-tune MLLMs to explicitly ``admit uncertainty'' when transcribing~\cite{TeachingLLMsAdmit2025}. While these works identify the problem, they typically test a \emph{single} confidence method (e.g., ensemble variance~\cite{archibaldImprovingMLLMHistorical2025} or fine-tuning for refusal~\cite{TeachingLLMsAdmit2025}). A systematic comparison of the different families of black-box proxies (verbalized, justification, and ensemble) under a unified review-budget framework for document transcription has, to our knowledge, not yet been performed.
