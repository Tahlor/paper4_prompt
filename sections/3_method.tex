\section{Methodology}
\label{sec:method}

We introduce a selective risk framework for evaluating confidence signals in document transcription, define a rigorous taxonomy of error modalities, and describe our experimental setup involving diverse prompts and visual degradations.

\subsection{Selective Risk Framework}
\label{subsec:selective_risk}

Traditional evaluation metrics for transcription, such as Character Error Rate (CER), report a single scalar value representing the average performance of a system. However, in real-world deployments, the goal is often to maximize automation (coverage) while maintaining a strict upper bound on error (risk).

We formalize this using a \textbf{Selective Risk Framework}. Let $f(x)$ be a model that outputs a prediction $\hat{y}$ and a confidence score $c$. For a chosen confidence threshold $t$, the system accepts the prediction if $c > t$ and rejects it (sending it for human review) otherwise.

We define \textbf{Coverage} $\phi(t)$ as the probability of acceptance:
\begin{equation}
    \phi(t) = P(c > t)
\end{equation}

We define \textbf{Selective Risk} $R(t)$ as the expected error of the accepted predictions:
\begin{equation}
    R(t) = \mathbb{E}[\mathcal{L}(y, \hat{y}) \mid c > t]
\end{equation}
where $\mathcal{L}$ is a loss function (e.g., 0-1 loss for exact match, or normalized Levenshtein distance).

We visualize the system's performance using a \textbf{Risk-Coverage Curve}, plotting $R(t)$ against $\phi(t)$. A superior confidence signal yields a lower risk for any given level of coverage.

\paragraph{Metrics} We evaluate confidence signals using:
\begin{itemize}
    \item \textbf{AUC-RC (Area Under the Risk-Coverage Curve)}: Quantifies the overall efficiency of the confidence signal.
    \item \textbf{AUROC}: Measures the ability to rank correct predictions higher than incorrect ones.
    \item \textbf{ECE (Expected Calibration Error)}: Measures the alignment between predicted confidence and empirical accuracy.
\end{itemize}

\subsection{Error Taxonomy}
\label{subsec:error_taxonomy}

To characterize \emph{why} models fail and how prompts influence these failures, we introduce a taxonomy of error modalities:

\begin{itemize}
    \item \textbf{Literalness}: The tendency to transcribe visual homoglyphs (e.g., `1' vs `l', `0' vs `O') that are visually faithful but semantically incorrect. Measured by the ratio of homoglyph errors to total errors.
    \item \textbf{Semantic Error}: Errors where the transcription is visually distinct but semantically related (e.g., ``January'' vs ``Jan''). Measured using embedding distance.
    \item \textbf{Drift}: Errors caused by loss of alignment with the visual line, often resulting in repeating or skipping lines. Measured by sequence alignment drift.
    \item \textbf{Instability}: The rate of insertion and deletion errors relative to the sequence length, indicating hallucination or omission.
    \item \textbf{Missed Detection}: The rate of False Negatives, where a field present in the ground truth is returned as ``N/A'' or empty.
\end{itemize}

\subsection{Confidence Signal Families}
\label{subsec:confidence_families}

We systematically evaluate three families of confidence proxies:

\subsubsection{Logprob-Based Confidence ($C_{logprob}$)}
For models exposing token-level log-probabilities, we compute sequence confidence $C_{logprob}$ as the length-normalized sum of log-probabilities:
\begin{equation}
    C_{logprob}(\hat{y}) = \frac{1}{L} \sum_{j=1}^L \log P(t_j | x, t_{<j})
\end{equation}

\subsubsection{Verbalized (Self-Reported) Confidence}
We prompt the model to explicitly estimate its own uncertainty. We investigate scaling strategies (Float [0,1] vs Integer [1-10]) and ``Justification-Based'' prompting, where the model explains its reasoning before assigning a score.

\subsubsection{Ensemble-Based Confidence}
We utilize ensemble variance as a proxy for uncertainty. For a set of predictions from different prompts or model variants, we define confidence based on pairwise agreement (normalized edit distance).

\subsection{Learned Confidence Combination (GBT)}
\label{subsec:gbt}

We train a Gradient Boosted Tree (GBT) classifier to predict correctness using features from two ``cheap'' model runs. Features include $C_{logprob}$ (if available), self-reported confidence, document legibility scores, and pairwise edit distance. The GBT output probability serves as the final confidence score for selective routing.

\subsection{Experimental Setup}
\label{subsec:setup}

\paragraph{Models} We evaluate Gemini 2.0 Flash, Gemini 2.5 Flash, and GPT-5 Mini Low.

\paragraph{Prompt Variants} We employ a diverse set of prompts designed to elicit different behaviors (Table~\ref{tab:prompt_taxonomy}).

\begin{table}[h]
    \centering
    \begin{tabular}{l|p{0.7\linewidth}}
        \textbf{Variant} & \textbf{Description} \\
        \hline
        Literal Interpretation & Strict character-level transcription; no context used. (Level 1) \\
        Balanced Context & Mix of visual transcription and contextual inference. (Level 5) \\
        Heavy Context & Prioritizes plausible values over messy visuals. (Level 10) \\
        Character-Level & Forces output tokenization (e.g., ``M, a, r, y'') to break BPE biases. \\
        Validity/Rough Draft & Chain-of-Thought prompting: Draft $\to$ Critical Review $\to$ Final JSON. \\
        Confidence Score & Explicitly requests a confidence score (0.0-1.0) per field. \\
    \end{tabular}
    \caption{Taxonomy of prompt variants used to generate diverse predictions and confidence signals.}
    \label{tab:prompt_taxonomy}
\end{table}

\paragraph{Visual Degradations} We apply Gridwarp, Gaussian Noise, and Gaussian Blur at varying intensities to test robustness.
